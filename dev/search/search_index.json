{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Spark-Expectations","text":"<p>Taking inspiration from DLT - data quality expectations: Spark-Expectations is built, so that the data quality rules can  run using decorator pattern while the spark job is in flight and Additionally, the framework able to perform data  quality checks when the data is at rest.</p>"},{"location":"#concept","title":"Concept","text":"<p>Most of the data quality tools do the data quality checks or data validation on a table at rest and provide metrics in  different forms. <code>While the existing tools are good to do profiling and provide metrics, below are the problems that we  commonly see</code> </p> <ul> <li>The existing tools do not perform any action or remove the malformed data in the original table </li> <li>Most existing frameworks do not offer the capability to perform both row and column level data quality checks  within a single tool.</li> <li>User have to manually check the provided metrics, and it becomes cumbersome to find the records which doesn't meet  the data quality standards</li> <li>Downstream users have to consume the same data with error, or they have to do additional computation to remove the  records that doesn't meet the standards</li> <li>Another process is required as a corrective action to rectify the errors in the data and lot of planning is usually  required for this acitivity</li> </ul> <p><code>Spark-Expectations solves all of the above problems by following the below principles</code></p> <ul> <li>Spark Expectations provides the ability to run both individual row-based and overall aggregated data quality rules  on both the source and validated data sets. In case a rules fails, the row-level error is recorded in the <code>_error</code> table  and a summarized report of all failed aggregated data quality rules is compiled in the <code>_stats</code> table</li> <li>All the records which fail one or more data quality rules, are by default quarantined in an <code>_error</code> table along with  the metadata on rules that failed, job information etc. This helps analysts or products to look at the error data easily  and work with the teams required to correct the data and reprocess it easily</li> <li>Aggregated Metrics are provided on the job level along with necessary metadata so that recalculation or compute is  avoided</li> <li>The data that doesn't meet the data quality contract or the standards is not written into the final table unless or otherwise specified. </li> <li>By default, frameworks have the capability to send notifications only upon failure, but they have the ability to  send notifications at the start, as well as upon completion</li> </ul> <p>There is a field in the rules table called action_if_failed, which determines what needs to be done if a rule fails</p> <ul> <li>Let's consider a hypothetical scenario, where we have 100 columns and with 200 row level data quality rules, 10 aggregation data quality rules and 5 query data quality rules  computed against. When the dq job is run, there are 10 rules that failed on a particular row and 4 aggregation rules fails- what determines if that row should end up in  final table or not? Below are the heirarchy of checks that happens?</li> <li>Among the row level 10 rules failed, if there is atleast one rule which has an action_if_failed as fail -    then the job will be failed </li> <li>Among the 10 row level rules failed, if there is no rule that has an action_if_failed as fail, but atleast    has one rule with action_if_failed as drop - then the record/row will be dropped</li> <li>Among the 10 row level rules failed, if no rule neither has fail nor drop as an action_if_failed - then    the record will be end up in the final table. Note that, this record would also exist in the <code>_error</code> table</li> <li>The aggregation and query dq rules have a setting called <code>action_if_failed</code> with two options: <code>fail</code> or <code>ignore</code>. If any of   the 10 aggregation rules and 5 query dq rules which failed has an action_if_failed_as_fail, then the metadata summary will be    recorded in the <code>_stats</code> table and the job will be considered a failure. However, if none of the failed rules    has an action_if_failed_as_fail, then summary of the aggregated rules' metadata will still be collected in the    <code>_stats</code> table for failed aggregated and  query dq rules.</li> </ul> <p>Please find the spark-expectations flow and feature diagrams here</p>"},{"location":"examples/","title":"Initialization_Examples","text":""},{"location":"examples/#configurations","title":"Configurations","text":"<p>In order to establish the global configuration parameter for DQ Spark Expectations, you must define and complete the required fields within a variable. This involves creating a variable and ensuring that all the necessary information is provided in the appropriate fields.</p> <pre><code>from spark_expectations.config.user_config import Constants as user_config\nse_global_spark_Conf = {\nuser_config.se_notifications_enable_email: False,  # (1)!\nuser_config.se_notifications_email_smtp_host: \"mailhost.com\",  # (2)!\nuser_config.se_notifications_email_smtp_port: 25,  # (3)!\nuser_config.se_notifications_email_from: \"&lt;sender_email_id&gt;\",  # (4)!\nuser_config.se_notifications_email_to_other_mail_id: \"&lt;receiver_email_id's&gt;\",  # (5)!\nuser_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",  # (6)!\nuser_config.se_notifications_enable_slack: True,  # (7)!\nuser_config.se_notifications_slack_webhook_url: \"&lt;slack-webhook-url&gt;\",  # (8)!\nuser_config.se_notifications_on_start: True,  # (9)!\nuser_config.se_notifications_on_completion: True,  # (10)!\nuser_config.se_notifications_on_fail: True,  # (11)!\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,  # (12)!\nuser_config.se_notifications_on_error_drop_threshold: 15,  # (13)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_notifications_enable_email</code> parameter, which controls whether notifications are sent via email, is set to false by default</li> <li>The <code>user_config.se_notifications_email_smtp_host</code> parameter is set to \"mailhost.com\" by default and is used to specify the email SMTP domain host</li> <li>The <code>user_config.se_notifications_email_smtp_port</code> parameter, which accepts a port number, is set to \"25\" by default</li> <li>The <code>user_config.se_notifications_email_from</code> parameter is used to specify the email ID that will trigger the email notification</li> <li>The <code>user_configse_notifications_email_to_other_mail_id</code> parameter accepts a list of recipient email IDs</li> <li>The <code>user_config.se_notifications_email_subject</code> parameter captures the subject line of the email</li> <li>The <code>user_config.se_notifications_enable_slack</code> parameter, which controls whether notifications are sent via slack, is set to false by default</li> <li>The <code>user_config/se_notifications_slack_webhook_url</code> parameter accepts the webhook URL of a Slack channel for sending notifications</li> <li>When <code>user_config.se_notifications_on_start</code> parameter set to <code>True</code> enables notification on start of the spark-expectations, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_completion</code> parameter set to <code>True</code> enables notification on completion of spark-expectations framework, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_fail</code> parameter set to <code>True</code> enables notification on failure of spark-expectations data qulaity framework, variable by default set to <code>True</code></li> <li>When <code>user_config.se_notifications_on_error_drop_exceeds_threshold_breach</code> parameter set to <code>True</code> enables notification when error threshold reaches above the configured value</li> <li>The <code>user_config.se_notifications_on_error_drop_threshold</code> parameter captures error drop threshold value</li> </ol>"},{"location":"examples/#spark-expectations-initialization","title":"Spark Expectations Initialization","text":"<p>For all the below examples the below import and SparkExpectations class instantiation is mandatory</p> <p>When store for sensitive details is Databricks secret scope,construct config dictionary for authentication of kafka and  avoid duplicate construction every time your project is initialized, you can create a dictionary with the following keys and their appropriate values.  This dictionary can be placed in the init.py file of your project or declared as a global variable. <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.dbx_workspace_url  : \"https://workspace.cloud.databricks.com\", # (3)!\nuser_config.dbx_secret_scope: \"sole_common_prod\", # (4)!\nuser_config.dbx_kafka_server_url: \"se_streaming_server_url_secret_key\", # (5)!\nuser_config.dbx_secret_token_url: \"se_streaming_auth_secret_token_url_key\", # (6)!\nuser_config.dbx_secret_app_name: \"se_streaming_auth_secret_appid_key\", # (7)!\nuser_config.dbx_secret_token: \"se_streaming_auth_secret_token_key\", # (8)!\nuser_config.dbx_topic_name: \"se_streaming_topic_name\", # (9)!\n}\n</code></pre></p> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cererus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.dbx_workspace_url</code> used to pass databricks workspace in the format <code>https://&lt;workspace_name&gt;.cloud.databricks.com</code></li> <li>The <code>user_config.dbx_secret_scope</code> captures name of the secret scope</li> <li>The <code>user_config.dbx_kafka_server_url</code> captures secret key for the kafka url</li> <li>The <code>user_config.dbx_secret_token_url</code> captures secret key for the kafka authentication app url</li> <li>The <code>user_config.dbx_secret_app_name</code> captures secret key for the kafka authentication app name</li> <li>The <code>user_config.dbx_secret_token</code> captures secret key for the kafka authentication app secret token</li> <li>The <code>user_config.dbx_topic_name</code> captures secret key for the kafka topic name</li> </ol> <p>Similarly when sensitive store is cerberus: </p> <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.cbs_url  : \"https://&lt;url&gt;.cerberus.com\", # (3)!\nuser_config.cbs_sdb_path: \"cerberus_sdb_path\", # (4)!\nuser_config.cbs_kafka_server_url: \"se_streaming_server_url_secret_sdb_path\", # (5)!\nuser_config.cbs_secret_token_url: \"se_streaming_auth_secret_token_url_sdb_apth\", # (6)!\nuser_config.cbs_secret_app_name: \"se_streaming_auth_secret_appid_sdb_path\", # (7)!\nuser_config.cbs_secret_token: \"se_streaming_auth_secret_token_sdb_path\", # (8)!\nuser_config.cbs_topic_name: \"se_streaming_topic_name_sdb_path\", # (9)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cererus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.cbs_url</code> used to pass cerberus url</li> <li>The <code>user_config.cbs_sdb_path</code> captures cerberus secure data store path</li> <li>The <code>user_config.cbs_kafka_server_url</code> captures path where kafka url stored in the cerberus sdb</li> <li>The <code>user_config.cbs_secret_token_url</code> captures path where kafka authentication app stored in the cerberus sdb</li> <li>The <code>user_config.cbs_secret_app_name</code> captures path where kafka authentication app name stored in the cerberus sdb</li> <li>The <code>user_config.cbs_secret_token</code> captures path where kafka authentication app name secret token stored in the cerberus sdb</li> <li>The <code>user_config.cbs_topic_name</code>  captures path where kafka topic name stored in the cerberus sdb</li> </ol> <pre><code>from spark_expectations.core.expectations import SparkExpectations\n# product_id should match with the \"product_id\" in the rules table\nse: SparkExpectations = SparkExpectations(product_id=\"your-products-id\", stats_streaming_options=stats_streaming_config_dict)  # (1)!\n</code></pre> <ol> <li>Instantiate <code>SparkExpectations</code> class which has all the required functions for running data quality rules</li> </ol>"},{"location":"examples/#example-1","title":"Example 1","text":"<pre><code>from spark_expectations.config.user_config import *  # (7)!\n@se.with_expectations(  # (6)!\nse.reader.get_rules_from_table(  # (5)!\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",  # (1)!\ntable_name=\"pilot_nonpub.dq_employee.employee\",  # (2)!\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"  # (3)!\n),\nwrite_to_table=True,  # (4)!\nwrite_to_temp_table=True,  # (8)!\nrow_dq=True,  # (9)!\nagg_dq={  # (10)!\nuser_config.se_agg_dq: True,  # (11)!\nuser_config.se_source_agg_dq: True,  # (12)!\nuser_config.se_final_agg_dq: True,  # (13)!\n},\nquery_dq={  # (14)!\nuser_config.se_query_dq: True,  # (15)!\nuser_config.se_source_query_dq: True,  # (16)!\nuser_config.se_final_query_dq: True,  # (17)!\nuser_config.se_target_table_view: \"order\",  # (18)!\n},\nspark_conf=se_global_spark_Conf,  # (19)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")  # (20)!\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")  # (20)!\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")  # (20)!\nreturn _df_order  # (21)!\n</code></pre> <ol> <li>Provide the full table name of the table which contains the rules</li> <li>Provide the table name using which the <code>_error</code> table will be created, which contains all the failed records.     Note if you are also wanting to write the data using <code>write_df</code>, then the table_name provided to both the functions     should be same</li> <li>Provide the full table name where the stats will be written into</li> <li>Use this argument to write the final data into the table. By default, it is False.    This is optional, if you just want to run the data quality checks.    A good example will be a staging table or temporary view.</li> <li>This functions reads the rules from the table and return them as a dict, which is an input to the <code>with_expectations</code> function</li> <li>This is the decorator that helps us run the data quality rules. After running the rules the results will be written into <code>_stats</code> table and <code>error</code> table</li> <li>import necessary configurable variables from <code>user_config</code> package for the specific functionality to configure in spark-expectations</li> <li>Use this argument to write the input dataframe into the temp table, so that it breaks the spark plan and might speed     up the job in cases of complex dataframe lineage</li> <li>The argument row_dq is optional and enables the conducting of row-based data quality checks. By default, this     argument is set to True, however, if desired, these checks can be skipped by setting the argument to False.</li> <li>The <code>agg_dq</code> argument is a dictionary that is used to gather different settings and options for the purpose of configuring the <code>agg_dq</code></li> <li>The argument <code>se_agg_dq</code> is utilized to activate the aggregate data quality check, and its default setting is True.</li> <li>The <code>se_source_agg_dq</code> argument is optional and enables the conducting of aggregate-based data quality checks on the      source data. By default, this argument is set to True, and this option depends on the <code>agg_dq</code> value.      If desired, these checks can be skipped by setting the source_agg_dq argument to False.</li> <li>This optional argument <code>se_final_agg_dq</code> allows to perform agg-based data quality checks on final data, with the      default setting being <code>True</code>, which depended on <code>row_agg</code> and <code>agg_dq</code>. skip these checks by setting argument to <code>False</code></li> <li>The <code>query_dq</code> argument is a dictionary that is used to gather different settings and options for the purpose of configuring the <code>query_dq</code></li> <li>The argument <code>se_query_dq</code> is utilized to activate the aggregate data quality check, and its default setting is True. </li> <li>The <code>se_source_query_dq</code> argument is optional and enables the conducting of query-based data quality checks on the      source data. By default, this argument is set to True, and this option depends on the <code>agg_dq</code> value.      If desired, these checks can be skipped by setting the source_agg_dq argument to False.</li> <li>This optional argument <code>se_final_query_dq</code> allows to perform query_based data quality checks on final data, with the      default setting being <code>True</code>, which depended on <code>row_agg</code> and <code>agg_dq</code>. skip these checks by setting argument to <code>False</code></li> <li>The parameter <code>se_target_table_view</code> can be provided with the name of a view that represents the target validated dataset for implementation of <code>query_dq</code> on the clean dataset from <code>row_dq</code></li> <li>The <code>spark_conf</code> parameter is utilized to gather all the configurations that are associated with notifications</li> <li>View registration can be utilized when implementing <code>query_dq</code> expectations.</li> <li>Returning a dataframe is mandatory for the <code>spark_expectations</code> to work, if we do not return a dataframe - then an exceptionm will be raised</li> </ol>"},{"location":"examples/#example-2","title":"Example 2","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\")\n),\nrow_dq=True  # (2)!\n)\ndef build_new() -&gt; DataFrame:\n_df: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/employee.csv\"))\n)\nreturn df \n</code></pre> <ol> <li>Conduct only row-based data quality checks while skipping the aggregate data quality checks</li> <li>Disabled the aggregate data quality checks</li> </ol>"},{"location":"examples/#example-3","title":"Example 3","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=False,  # (2)!\nagg_dq={\nuser_config.se_agg_dq: True, \nuser_config.se_source_agg_dq: True, \nuser_config.se_final_agg_dq: False, \n}\n)\ndef build_new() -&gt; DataFrame:\n_df: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/employee.csv\"))\n)\nreturn df \n</code></pre> <ol> <li>Perform only aggregate-based data quality checks while avoiding both row-based data quality checks and aggregate data    quality checks on the validated dataset, since row validation has not taken place</li> <li>Disabled the row data quality checks</li> </ol>"},{"location":"examples/#example-4","title":"Example 4","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=True, \nquery_dq={  # (2)!\nuser_config.se_query_dq: True,\nuser_config.se_source_query_dq: True,\nuser_config.se_final_query_dq: True,\nuser_config.se_target_table_view: \"order\",\n},\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\") \n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\")) \n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")\nreturn _df_order\n</code></pre> <ol> <li>Conduct row-based and query-based data quality checks only on the source and target dataset, while skipping the aggregate     data quality checks on the validated dataset</li> <li>Enabled the query data quality checks</li> </ol>"},{"location":"examples/#example-5","title":"Example 5","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=True, \nagg_dq={ # (10)!\nuser_config.user_configse_agg_dq: True, \nuser_config.se_source_agg_dq: True,\nuser_config.se_final_agg_dq: False, # (2)!\n}, \n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>Conduct row-based and aggregate-based data quality checks only on the source dataset, while skipping the aggregate     data quality checks on the validated dataset</li> <li>Disabled the final aggregate data quality quality checks</li> </ol>"},{"location":"examples/#example-6","title":"Example 6","text":"<pre><code>import os\n@se.with_expectations(\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nspark_conf=se_global_spark_Conf, # (2)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>There are four types of notifications: notification_on_start, notification_on_completion, notification_on_fail and notification_on_error_threshold_breach.     Enable notifications for all four stages by setting the values to <code>True</code></li> <li>To provide the absolute file path for a configuration variable that holds information regarding notifications, use the    decalared global variable, <code>se_global_spark_Conf</code></li> </ol>"},{"location":"examples/#example-7","title":"Example 7","text":"<pre><code>@se.with_expectations( # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=False, \nagg_dq={ \nuser_config.se_agg_dq: False,\nuser_config.se_source_agg_dq: False, \nuser_config.se_final_agg_dq: True,\n},  \nquery_dq={ \nuser_config.se_query_dq: False,\nuser_config.se_source_query_dq: True,\nuser_config.se_final_query_dq: True,\nuser_config.se_target_table_view: \"order\", \n},\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>Below combination of <code>row_dq, agg_dq, source_agg_dq, final_agg_dq, query_dq, source_query_dq and final_query_dq</code> skips the data quality checks because     source_agg_dq depends on agg_dq and final_agg_dq depends on row_dq and agg_dq</li> </ol>"},{"location":"examples/#example-8","title":"Example 8","text":"<pre><code>@se.with_expectations( # (1)!\nse.reader.get_rules_from_table(\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",\ntarget_table_name=\"pilot_nonpub.customer_order\",\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\",\nactions_if_failed=[\"drop\", \"ignore\"]  # (1)!\n)\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>By default <code>action_if_failed</code> contains [\"fail\", \"drop\", \"ignore\"], but if we want to run only rules which has a     particular action then we can pass them as list shown in the example</li> </ol>"},{"location":"examples/#example-9","title":"Example 9","text":"<pre><code>@se.with_expectations( # (1)!\nse.reader.get_rules_from_table(\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",\ntarget_table_name=\"pilot_nonpub.customer_order\",\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\",\nactions_if_failed=[\"drop\", \"ignore\"]  # (1)!\n),\nrow_dq=True,  # (2)!\nagg_dq={ \nuser_config.se_agg_dq: True, \nuser_config.se_source_agg_dq: True,\nuser_config.se_final_agg_dq: True, \n},  \nquery_dq={ \nuser_config.se_query_dq: True,\nuser_config.se_source_query_dq: True,\nuser_config.se_final_query_dq: True,\nuser_config.se_target_table_view: \"order\", \n}\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\") \n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\")) \n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\") \nreturn _df_order\n</code></pre> <ol> <li>The default options for the action_if_failed field are [\"fail\", \"drop\", or \"ignore\"], but you can specify which of     these actions to run by providing a list of the desired actions in the example when selecting which data quality rules     set to apply</li> <li>Data quality rules will only be applied if they have [\"drop\" or \"ignore\"] specified in the action_if_failed field</li> </ol>"},{"location":"examples/#example-10","title":"Example 10","text":"<pre><code>@se.with_expectations(\nse.reader.get_rules_from_table(\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",\ntarget_table_name=\"pilot_nonpub.customer_order\",\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nspark_conf={\"spark.files.maxPartitionBytes\": \"134217728\"},  # (1)!\noptions={\"mode\": \"overwrite\", \"partitionBy\": \"order_month\", \n\"overwriteSchema\": \"true\"},  # (2)!\noptions_error_table={\"partition_by\": \"id\"}  # (3)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order\n</code></pre> <ol> <li>Provide the optional <code>spark_conf</code> if needed, this is used while writing the data into the <code>final</code> and <code>error</code> table along with notification related configurations</li> <li>Provide the optional <code>options</code> if needed, this is used while writing the data into the <code>final</code> table</li> <li>Provide the optional <code>options_error_table</code> if needed, this is used while writing the data into the <code>error</code> table</li> </ol>"},{"location":"api/actions/","title":"Actions","text":""},{"location":"api/actions/#spark_expectations.utils.actions-classes","title":"Classes","text":""},{"location":"api/actions/#spark_expectations.utils.actions.SparkExpectationsActions","title":"<code>spark_expectations.utils.actions.SparkExpectationsActions</code>","text":"<p>This class implements/supports applying data quality rules on given dataframe and performing required action</p>"},{"location":"api/actions/#spark_expectations.utils.actions.SparkExpectationsActions-functions","title":"Functions","text":""},{"location":"api/actions/#spark_expectations.utils.actions.SparkExpectationsActions.action_on_rules","title":"<code>action_on_rules(_context: SparkExpectationsContext, _df_dq: DataFrame, _table_name: str, _input_count: int, _error_count: int = 0, _output_count: int = 0, _rule_type: Optional[str] = None, _row_dq_flag: bool = False, _source_agg_dq_flag: bool = False, _final_agg_dq_flag: bool = False, _source_query_dq_flag: bool = False, _final_query_dq_flag: bool = False) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>This function takes necessary action set by the user on the rules and returns the dataframe with results</p> <p>Parameters:</p> Name Type Description Default <code>_context</code> <code>SparkExpectationsContext</code> <p>Provide SparkExpectationsContext</p> required <code>_df_dq</code> <code>DataFrame</code> <p>Input dataframe on which data quality rules need to be applied</p> required <code>table_name</code> <p>error table name</p> required <code>_input_count</code> <code>int</code> <p>input dataset count</p> required <code>_error_count</code> <code>int</code> <p>error count in the dataset</p> <code>0</code> <code>_output_count</code> <code>int</code> <p>output count in the dataset</p> <code>0</code> <code>_rule_type</code> <code>Optional[str]</code> <p>type of rule expectations</p> <code>None</code> <code>_row_dq_flag</code> <code>bool</code> <p>Mark it as True when dq running for row level expectations</p> <code>False</code> <code>_source_agg_dq_flag</code> <code>bool</code> <p>Mark it as True when dq running for agg level expectations on source dataframe</p> <code>False</code> <code>_final_agg_dq_flag</code> <code>bool</code> <p>Mark it as True when dq running for agg level expectations on final dataframe</p> <code>False</code> <code>_source_query_dq_flag</code> <code>bool</code> <p>Mark it as True when dq running for query level expectations on source dataframe</p> <code>False</code> <code>_final_query_dq_flag</code> <code>bool</code> <p>Mark it as True when dq running for query level expectations on final dataframe</p> <code>False</code> <code>_action_on</code> <p>perform action on different stages in dq</p> required <code>_source_agg_dq_result</code> <p>source aggregated data quality result</p> required <code>_final_agg_dq_result</code> <p>final aggregated data quality result</p> required <code>_source_query_dq_result</code> <p>source query based data quality result</p> required <code>_final_query_dq_result</code> <p>final query based data quality result</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Returns a dataframe after dropping the error from the dataset</p> Source code in <code>spark_expectations/utils/actions.py</code> <pre><code>@staticmethod\ndef action_on_rules(\n_context: SparkExpectationsContext,\n_df_dq: DataFrame,\n_table_name: str,\n_input_count: int,\n_error_count: int = 0,\n_output_count: int = 0,\n_rule_type: Optional[str] = None,\n_row_dq_flag: bool = False,\n_source_agg_dq_flag: bool = False,\n_final_agg_dq_flag: bool = False,\n_source_query_dq_flag: bool = False,\n_final_query_dq_flag: bool = False,\n) -&gt; DataFrame:\n\"\"\"\n    This function takes necessary action set by the user on the rules and returns the dataframe with results\n    Args:\n        _context: Provide SparkExpectationsContext\n        _df_dq: Input dataframe on which data quality rules need to be applied\n        table_name: error table name\n        _input_count: input dataset count\n        _error_count: error count in the dataset\n        _output_count: output count in the dataset\n        _rule_type: type of rule expectations\n        _row_dq_flag: Mark it as True when dq running for row level expectations\n        _source_agg_dq_flag: Mark it as True when dq running for agg level expectations on source dataframe\n        _final_agg_dq_flag: Mark it as True when dq running for agg level expectations on final dataframe\n        _source_query_dq_flag: Mark it as True when dq running for query level expectations on source dataframe\n        _final_query_dq_flag: Mark it as True when dq running for query level expectations on final dataframe\n        _action_on: perform action on different stages in dq\n        _source_agg_dq_result: source aggregated data quality result\n        _final_agg_dq_result: final aggregated data quality result\n        _source_query_dq_result: source query based data quality result\n        _final_query_dq_result: final query based data quality result\n    Returns:\n            DataFrame: Returns a dataframe after dropping the error from the dataset\n    \"\"\"\ntry:\n_df_dq = _df_dq.withColumn(\n\"action_if_failed\", get_actions_list(col(f\"meta_{_rule_type}_results\"))\n).drop(f\"meta_{_rule_type}_results\")\nif (\nnot _df_dq.filter(\narray_contains(_df_dq.action_if_failed, \"fail\")\n).count()\n&gt; 0\n):\n_df_dq = _df_dq.filter(~array_contains(_df_dq.action_if_failed, \"drop\"))\nelse:\nif _row_dq_flag:\n_context.set_row_dq_status(\"Failed\")\nelif _source_agg_dq_flag:\n_context.set_source_agg_dq_status(\"Failed\")\nelif _final_agg_dq_flag:\n_context.set_final_agg_dq_status(\"Failed\")\nelif _source_query_dq_flag:\n_context.set_source_query_dq_status(\"Failed\")\nelif _final_query_dq_flag:\n_context.set_final_query_dq_status(\"Failed\")\nraise SparkExpectOrFailException(\nf\"Job failed, as there is a data quality issue at {_rule_type} \"\nf\"expectations and the action_if_failed \"\n\"suggested to fail\"\n)\nreturn _df_dq.drop(_df_dq.action_if_failed)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occured while taking action on given rules {e}\"\n)\n</code></pre>"},{"location":"api/actions/#spark_expectations.utils.actions.SparkExpectationsActions.create_agg_dq_results","title":"<code>create_agg_dq_results(_context: SparkExpectationsContext, _df: DataFrame, _rule_type_name: str) -&gt; List[Dict[str, str]]</code>","text":"<p>This function helps to collect the aggregation results in to the list  Args:      _context: SparkContext object      _df: dataframe which contains agg data quality results      _rule_type_name: which determines the type of the rule</p> <p>Returns:         List with dict of agg rules property name and value</p> Source code in <code>spark_expectations/utils/actions.py</code> <pre><code>def create_agg_dq_results(\nself, _context: SparkExpectationsContext, _df: DataFrame, _rule_type_name: str\n) -&gt; List[Dict[str, str]]:\n\"\"\"\n    This function helps to collect the aggregation results in to the list\n     Args:\n         _context: SparkContext object\n         _df: dataframe which contains agg data quality results\n         _rule_type_name: which determines the type of the rule\n     Returns:\n            List with dict of agg rules property name and value\n    \"\"\"\ntry:\nreturn (\n_df.first()[f\"meta_{_rule_type_name}_results\"]\nif _df\nand f\"meta_{_rule_type_name}_results\" in _df.columns\nand len(_df.first()[f\"meta_{_rule_type_name}_results\"]) &gt; 0\nelse None\n)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while running create agg dq results {e}\"\n)\n</code></pre>"},{"location":"api/actions/#spark_expectations.utils.actions.SparkExpectationsActions.create_rules_map","title":"<code>create_rules_map(_rule_map: Dict[str, str]) -&gt; Any</code>  <code>staticmethod</code>","text":"<p>This function helps to extract the selected rules properties and returns array of dict with key and value</p> <p>Parameters:</p> Name Type Description Default <code>_rule_map</code> <code>Dict[str, str]</code> <p>dict with rules properties</p> required Source code in <code>spark_expectations/utils/actions.py</code> <pre><code>@staticmethod\ndef create_rules_map(_rule_map: Dict[str, str]) -&gt; Any:\n\"\"\"\n    This function helps to extract the selected rules properties and returns array of dict with key and value\n    Args:\n        _rule_map: dict with rules properties\n    Returns: Array of tuple with expectations rule properties\n    \"\"\"\nreturn array(\n[\nstruct(lit(elem), lit(_rule_map.get(elem)))\nfor elem in [\n\"rule_type\",\n\"rule\",\n\"action_if_failed\",\n\"tag\",\n\"description\",\n]\n]\n)\n</code></pre>"},{"location":"api/actions/#spark_expectations.utils.actions.SparkExpectationsActions.get_rule_is_active","title":"<code>get_rule_is_active(_context: SparkExpectationsContext, rule: dict, _rule_type_name: str, _source_dq_enabled: bool = False, _target_dq_enabled: bool = False) -&gt; bool</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>rule</code> <code>dict</code> required <code>_rule_tye_name</code> required <code>_source_dq_enabled</code> <code>bool</code> <code>False</code> <code>_target_dq_enabled</code> <code>bool</code> <code>False</code> Source code in <code>spark_expectations/utils/actions.py</code> <pre><code>@staticmethod\ndef get_rule_is_active(\n_context: SparkExpectationsContext,\nrule: dict,\n_rule_type_name: str,\n_source_dq_enabled: bool = False,\n_target_dq_enabled: bool = False,\n) -&gt; bool:\n\"\"\"\n    Args:\n        rule:\n        _rule_tye_name:\n        _source_dq_enabled:\n        _target_dq_enabled:\n    Returns:\n    \"\"\"\n_is_active: bool = False\nif (\n_rule_type_name\nin [\n_context.get_query_dq_rule_type_name,\n_context.get_agg_dq_rule_type_name,\n]\nand _source_dq_enabled is True\n):\n_is_active = rule[\"enable_for_source_dq_validation\"]\nelif (\n_rule_type_name\nin [\n_context.get_query_dq_rule_type_name,\n_context.get_agg_dq_rule_type_name,\n]\nand _target_dq_enabled is True\n):\n_is_active = rule[\"enable_for_target_dq_validation\"]\nreturn _is_active\n</code></pre>"},{"location":"api/actions/#spark_expectations.utils.actions.SparkExpectationsActions.run_dq_rules","title":"<code>run_dq_rules(_context: SparkExpectationsContext, df: DataFrame, expectations: Dict[str, List[dict]], rule_type: str, _source_dq_enabled: bool = False, _target_dq_enabled: bool = False) -&gt; DataFrame</code>  <code>staticmethod</code>","text":"<p>This function builds the expressions for the data quality rules and returns the dataframe with results</p> <p>Parameters:</p> Name Type Description Default <code>_context</code> <code>SparkExpectationsContext</code> <p>Provide SparkExpectationsContext</p> required <code>df</code> <code>DataFrame</code> <p>Input dataframe on which data quality rules need to be applied</p> required <code>expectations</code> <code>Dict[str, List[dict]]</code> <p>Provide the dict which has all the rules</p> required <code>rule_type</code> <code>str</code> <p>identifier for the type of rule to be applied in processing</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Returns a dataframe with all the rules run the input dataframe</p> Source code in <code>spark_expectations/utils/actions.py</code> <pre><code>@staticmethod\ndef run_dq_rules(\n_context: SparkExpectationsContext,\ndf: DataFrame,\nexpectations: Dict[str, List[dict]],\nrule_type: str,\n_source_dq_enabled: bool = False,\n_target_dq_enabled: bool = False,\n) -&gt; DataFrame:\n\"\"\"\n    This function builds the expressions for the data quality rules and returns the dataframe with results\n    Args:\n        _context: Provide SparkExpectationsContext\n        df: Input dataframe on which data quality rules need to be applied\n        expectations: Provide the dict which has all the rules\n        rule_type: identifier for the type of rule to be applied in processing\n    Returns:\n        DataFrame: Returns a dataframe with all the rules run the input dataframe\n    \"\"\"\ntry:\ncondition_expressions = []\nif len(expectations) &lt;= 0:\nraise SparkExpectationsMiscException(\"no rules found to process\")\nif (\nf\"{rule_type}_rules\" not in expectations\nor len(expectations[f\"{rule_type}_rules\"]) &lt;= 0\n):\nraise SparkExpectationsMiscException(\nf\"zero expectations to process for {rule_type}_rules from the `dq_rules` table, \"\nf\"please configure rules or avoid this error by setting {rule_type} to False\"\n)\nfor rule in expectations[f\"{rule_type}_rules\"]:\n_rule_is_active = SparkExpectationsActions.get_rule_is_active(\n_context,\nrule,\nrule_type,\n_source_dq_enabled=_source_dq_enabled,\n_target_dq_enabled=_target_dq_enabled,\n)\nif _rule_is_active or rule_type == _context.get_row_dq_rule_type_name:\ncolumn = f\"{rule_type}_{rule['rule']}\"\ncondition_expressions.append(\nwhen(expr(rule[\"expectation\"]), create_map())\n.otherwise(\nmap_from_entries(\nSparkExpectationsActions.create_rules_map(rule)\n)\n)\n.alias(column)\n)\nif len(condition_expressions) &gt; 0:\nif rule_type in [\n_context.get_query_dq_rule_type_name,\n_context.get_agg_dq_rule_type_name,\n]:\ndf = (\ndf\nif rule_type == _context.get_agg_dq_rule_type_name\nelse _context.get_supported_df_query_dq\n)\ndf = df.select(*condition_expressions)\ndf = df.withColumn(\nf\"meta_{rule_type}_results\", array(*list(df.columns))\n)\ndf = df.withColumn(\nf\"meta_{rule_type}_results\",\nremove_empty_maps(df[f\"meta_{rule_type}_results\"]),\n).drop(\n*[\n_col\nfor _col in df.columns\nif _col != f\"meta_{rule_type}_results\"\n]\n)\n_context.print_dataframe_with_debugger(df)\nelif rule_type == _context.get_row_dq_rule_type_name:\ndf = df.select(col(\"*\"), *condition_expressions)\nelse:\nraise SparkExpectationsMiscException(\nf\"zero active expectations to process for {rule_type}_rules from the `dq_rules` table, \"\nf\"at {f'final_{rule_type}' if _source_dq_enabled else f'final_{rule_type}' }\"\nf\", please configure rules or avoid this error by setting final_{rule_type} to False\"\n)\nreturn df\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while running expectations {e}\"\n)\n</code></pre>"},{"location":"api/actions/#spark_expectations.utils.actions-functions","title":"Functions","text":""},{"location":"api/base_notification_plugin/","title":"Base_Notification_plugin","text":""},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification-attributes","title":"Attributes","text":""},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification.SPARK_EXPECTATIONS_NOTIFICATION_PLUGIN","title":"<code>spark_expectations.notifications.plugins.base_notification.SPARK_EXPECTATIONS_NOTIFICATION_PLUGIN = 'spark_expectations_notification_plugins'</code>  <code>module-attribute</code>","text":""},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification.notification_plugin_spec","title":"<code>spark_expectations.notifications.plugins.base_notification.notification_plugin_spec = pluggy.HookspecMarker(SPARK_EXPECTATIONS_NOTIFICATION_PLUGIN)</code>  <code>module-attribute</code>","text":""},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification.spark_expectations_notification_impl","title":"<code>spark_expectations.notifications.plugins.base_notification.spark_expectations_notification_impl = pluggy.HookimplMarker(SPARK_EXPECTATIONS_NOTIFICATION_PLUGIN)</code>  <code>module-attribute</code>","text":""},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification-classes","title":"Classes","text":""},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification.SparkExpectationsNotification","title":"<code>spark_expectations.notifications.plugins.base_notification.SparkExpectationsNotification</code>","text":"<p>This is base class for notifications plugin</p>"},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification.SparkExpectationsNotification-functions","title":"Functions","text":""},{"location":"api/base_notification_plugin/#spark_expectations.notifications.plugins.base_notification.SparkExpectationsNotification.send_notification","title":"<code>send_notification(_context: SparkExpectationsContext, _config_args: Dict[Union[str], Union[str, bool]]) -&gt; None</code>","text":"<p>function consist signature to notification, which will be implemented in the child class</p> <p>Parameters:</p> Name Type Description Default <code>_context</code> <code>SparkExpectationsContext</code> <p>object of SparkExpectationsContext</p> required <code>_config_args</code> <code>Dict[Union[str], Union[str, bool]]</code> <p>dict which contains required parameter to send notification</p> required Source code in <code>spark_expectations/notifications/plugins/base_notification.py</code> <pre><code>@notification_plugin_spec\ndef send_notification(\nself,\n_context: SparkExpectationsContext,\n_config_args: Dict[Union[str], Union[str, bool]],\n) -&gt; None:\n\"\"\"\n    function consist signature to notification, which will be implemented in the child class\n    Args:\n        _context:object of SparkExpectationsContext\n        _config_args: dict which contains required parameter to send notification\n    Returns: None\n    \"\"\"\npass\n</code></pre>"},{"location":"api/base_setup/","title":"Base_Setup","text":""},{"location":"api/base_setup/#spark_expectations.examples.base_setup-attributes","title":"Attributes","text":""},{"location":"api/base_setup/#spark_expectations.examples.base_setup.spark","title":"<code>spark_expectations.examples.base_setup.spark = get_spark_session()</code>  <code>module-attribute</code>","text":""},{"location":"api/base_setup/#spark_expectations.examples.base_setup-functions","title":"Functions","text":""},{"location":"api/base_setup/#spark_expectations.examples.base_setup.main","title":"<code>spark_expectations.examples.base_setup.main() -&gt; None</code>","text":"Source code in <code>spark_expectations/examples/base_setup.py</code> <pre><code>def main() -&gt; None:\nos.environ[\"DQ_SPARK_EXPECTATIONS_CERBERUS_TOKEN\"] = \"\"\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\nprint(\"Creating the necessary infrastructure for the tests to run locally!\")\n# run kafka locally in docker\nprint(\"create or run if exist docker container\")\nos.system(f\"sh {current_dir}/docker_scripts/docker_kafka_start_script.sh\")\n# create database\nos.system(\"rm -rf /tmp/hive/warehouse/dq_spark_local.db\")\nspark.sql(\"create database if not exists dq_spark_local\")\nspark.sql(\"use dq_spark_local\")\n# create project_rules_table\nspark.sql(\"drop table if exists dq_rules\")\nos.system(\"rm -rf /tmp/hive/warehouse/dq_spark_local.db/dq_rules\")\nspark.sql(\n\"\"\"\n    create table dq_rules (\n    product_id STRING,\n    table_name STRING,\n    rule_type STRING,\n    rule STRING,\n    column_name STRING,\n    expectation STRING,\n    action_if_failed STRING,\n    tag STRING,\n    description STRING,\n    enable_for_source_dq_validation BOOLEAN, \n    enable_for_target_dq_validation BOOLEAN,\n    is_active BOOLEAN,\n    enable_error_drop_alert BOOLEAN,\n    error_drop_threshold INT\n    )\n    USING delta\n    \"\"\"\n)\nspark.sql(\n\"ALTER TABLE dq_rules ADD CONSTRAINT rule_type_action CHECK (rule_type in ('row_dq', 'agg_dq', 'query_dq'));\"\n)\nspark.sql(\n\"ALTER TABLE dq_rules ADD CONSTRAINT action CHECK ((rule_type = 'row_dq' and action_if_failed IN ('ignore', 'drop', 'fail')) or \"\n\"(rule_type = 'agg_dq' and action_if_failed in ('ignore', 'fail')) or (rule_type = 'query_dq' and action_if_failed in ('ignore', 'fail')));\"\n)\n# create project_dq_stats_table\n# spark.sql(\"drop table if exists dq_stats\")\n# os.system(\"rm -rf /tmp/hive/warehouse/dq_spark_local.db/dq_stats\")\n# spark.sql(\n#     \"\"\"\n# create table dq_stats (\n# product_id STRING,\n# table_name STRING,\n# input_count LONG,\n# error_count LONG,\n# output_count LONG,\n# output_percentage FLOAT,\n# success_percentage FLOAT,\n# error_percentage FLOAT,\n# source_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,\n# final_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,\n# source_query_dq_results array&lt;map&lt;string, string&gt;&gt;,\n# final_query_dq_results array&lt;map&lt;string, string&gt;&gt;,\n# row_dq_res_summary array&lt;map&lt;string, string&gt;&gt;,\n# dq_status map&lt;string, string&gt;,\n# dq_run_time map&lt;string, float&gt;,\n# dq_rules map&lt;string, map&lt;string,int&gt;&gt;,\n# meta_dq_run_id STRING,\n# meta_dq_run_date DATE,\n# meta_dq_run_datetime TIMESTAMP\n# )\n# USING delta\n# \"\"\"\n# )\nspark.sql(\n\"\"\"\n    insert into table dq_rules values\n    (\"your_product\", \"dq_spark_local.customer_order\",  \"row_dq\", \"customer_id_is_not_null\", \"customer_id\", \"customer_id is not null\",\"drop\", \"validity\", \"customer_id ishould not be null\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"sales_greater_than_zero\", \"sales\", \"sales &gt; 0\", \"drop\", \"accuracy\", \"sales value should be greater than zero\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"discount_threshold\", \"discount\", \"discount*100 &lt; 60\",\"drop\", \"validity\", \"discount should be less than 40\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"ship_mode_in_set\", \"ship_mode\", \"lower(trim(ship_mode)) in('second class', 'standard class', 'standard class')\", \"drop\", \"validity\", \"ship_mode mode belongs in the sets\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"profit_threshold\", \"profit\", \"profit&gt;0\", \"drop\", \"validity\", \"profit threshold should be greater tahn 0\", true, true, true, true, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"sum_of_sales\", \"sales\", \"sum(sales)&gt;10000\", \"ignore\", \"validity\", \"regex format validation for quantity\",  true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"sum_of_quantity\", \"quantity\", \"sum(sales)&gt;10000\", \"ignore\", \"validity\", \"regex format validation for quantity\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"distinct_of_ship_mode\", \"ship_mode\", \"count(distinct ship_mode)&lt;=3\", \"ignore\", \"validity\", \"regex format validation for quantity\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"row_count\", \"*\", \"count(*)&gt;=10000\", \"ignore\", \"validity\", \"regex format validation for quantity\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"query_dq\", \"product_missing_count_threshold\", \"*\", \"((select count(distinct product_id) from product) - (select count(distinct product_id) from order))&gt;(select count(distinct product_id) from product)*0.2\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"query_dq\", \"product_category\", \"*\", \"(select count(distinct category) from product) &lt; 5\", \"ignore\", \"validity\", \"distinct product category\", true, true, true, false, 0)\n    ,(\"your_product\", \"dq_spark_local.customer_order\", \"query_dq\", \"row_count_in_order\", \"*\", \"(select count(*) from order)&lt;10000\", \"ignore\", \"accuracy\", \"count of the row in order dataset\", true, true, true, false, 0)\n     \"\"\"\n)\n# , (\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"referential_integrity_customer_id\", \"customer_id\",\n#    \"customer_id in(select distinct customer_id from customer)\", true, true, \"drop\", true, \"validity\",\n#    \"referential integrity for cuatomer_id\")\n# , (\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"referential_integrity_product_id\", \"product_id\",\n#    \"select count(*) from (select distinct product_id as ref_product from product) where product_id=ref_product &gt; 1\",\n#    true, true, \"drop\", true, \"validity\", \"referntial integrity for product_id\")\n# , (\n# \"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"regex_format_sales\", \"sales\", \"sales rlike '[1-9]+.[1-9]+'\",\n# true, true, \"drop\", true, \"validity\", \"regex format validation for sales\")\n# , (\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"regex_format_quantity\", \"quantity\",\n#    \"quantity rlike '[1-9]+.[1-9]+'\", true, true, \"drop\", true, \"validity\", \"regex format validation for quantity\")\n# , (\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"date_format_order_date\", \"order_date\",\n#    \"order_date rlike '([1-3][1-9]|[0-1])/([1-2]|[1-9])/20[0-2][0-9]''\", true, true, \"drop\", true, \"validity\",\n#    \"regex format validation for quantity\")\n# , (\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"regex_format_order_id\", \"order_id\",\n#    \"order_id rlike '(US|CA)-20[0-2][0-9]-*''\", true, true, \"drop\", true, \"validity\",\n#    \"regex format validation for quantity\")\n# , (\"your_product\", \"dq_spark_local.employee_new\", \"query_dq\", \"\", \"*\",\n#    \"(select count(*) from dq_spark_local_employee_new)!=(select count(*) from dq_spark_local_employee_new)\", true,\n#    false, \"ignore\", false, \"validity\", \"canary check to comapre the two table count\")\n# , (\"your_product\", \"dq_spark_local.employee_new\", \"query_dq\", \"department_salary_threshold\", \"department\",\n#    \"(select count(*) from (select department from dq_spark_local_employee_new group by department having sum(bonus)&gt;1000))&lt;1\",\n#    true, false, \"ignore\", true, \"validity\", \"each sub-department threshold\")\n# , (\n# \"your_product\", \"dq_spark_local.employee_new\", \"query_dq\", \"count_of_exit_date_nulls_threshold\", \"exit_date\", \"\", true,\n# true, \"ignore\", false, \"validity\", \"exit_date null threshold\")\n# , (\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"complete_duplicate\", \"*\",\n#    \"count(*) over(partition by customer_id,product_id,order_id,order_date,ship_date,ship_mode,sales,quantity,discount,profit order by 1)\",\n#    true, true, \"drop\", true, \"validity\", \"complete duplicate record\")\n# , (\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"primary_key_check\", \"*\",\n#    \"count(*) over(partition by customer_id, order_id order by 1)\", true, true, \"drop\", true, \"validity\",\n#    \"primary key check\")\n# ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"order_date_format_check\", \"order_date\", \"to_date(order_date, 'dd/MM/yyyy')\", true, true,\"drop\" ,true, \"validity\", \"Age of the employee should be less than 65\")\nspark.sql(\"select * from dq_rules\").show(truncate=False)\n# DROP the data tables and error tables\nspark.sql(\"drop table if exists dq_spark_local.customer_order\")\nos.system(\n\"rm -rf /tmp/hive/warehouse/dq_spark_local.db/dq_spark_local.customer_order\"\n)\nspark.sql(\"drop table if exists dq_spark_local.customer_order_error\")\nos.system(\n\"rm -rf /tmp/hive/warehouse/dq_spark_local.db/dq_spark_local.customer_order_error\"\n)\nprint(\"Local infrastructure setup is done\")\n</code></pre>"},{"location":"api/base_sink_plugin/","title":"Base_Writer_Plugin","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer-attributes","title":"Attributes","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer.SPARK_EXPECTATIONS_WRITER_PLUGIN","title":"<code>spark_expectations.sinks.plugins.base_writer.SPARK_EXPECTATIONS_WRITER_PLUGIN = 'spark_expectations_writer_plugins'</code>  <code>module-attribute</code>","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer.spark_expectations_writer_impl","title":"<code>spark_expectations.sinks.plugins.base_writer.spark_expectations_writer_impl = pluggy.HookimplMarker(SPARK_EXPECTATIONS_WRITER_PLUGIN)</code>  <code>module-attribute</code>","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer.writer_plugin_spec","title":"<code>spark_expectations.sinks.plugins.base_writer.writer_plugin_spec = pluggy.HookspecMarker(SPARK_EXPECTATIONS_WRITER_PLUGIN)</code>  <code>module-attribute</code>","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer-classes","title":"Classes","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer.SparkExpectationsSinkWriter","title":"<code>spark_expectations.sinks.plugins.base_writer.SparkExpectationsSinkWriter</code>","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer.SparkExpectationsSinkWriter-functions","title":"Functions","text":""},{"location":"api/base_sink_plugin/#spark_expectations.sinks.plugins.base_writer.SparkExpectationsSinkWriter.writer","title":"<code>writer(_write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]) -&gt; None</code>","text":"<p>function consist signature to write data into delta/nsp, which will be implemented in the child class</p> <p>Parameters:</p> Name Type Description Default <code>_write_args</code> <code>Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]</code> required Source code in <code>spark_expectations/sinks/plugins/base_writer.py</code> <pre><code>@writer_plugin_spec\ndef writer(\nself, _write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]\n) -&gt; None:\n\"\"\"\n    function consist signature to write data into delta/nsp, which will be implemented in the child class\n    Args:\n        _write_args:\n    Returns:\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/","title":"Context","text":""},{"location":"api/context/#spark_expectations.core.context-classes","title":"Classes","text":""},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext","title":"<code>spark_expectations.core.context.SparkExpectationsContext</code>  <code>dataclass</code>","text":"<p>This class provides the context for SparkExpectations</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext-attributes","title":"Attributes","text":""},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_agg_dq_rule_type_name","title":"<code>get_agg_dq_rule_type_name: str</code>  <code>property</code>","text":"<p>This function is used to get aggregation data quality rule type name</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _agg_dq_rule_type_name\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_cerberus_cred_path","title":"<code>get_cerberus_cred_path: str</code>  <code>property</code>","text":"<p>This functions implemented to return cerberus credentials path</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_cerberus_token","title":"<code>get_cerberus_token: str</code>  <code>property</code>","text":"<p>This functions implemented to return cerberus token</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_cerberus_url","title":"<code>get_cerberus_url: str</code>  <code>property</code>","text":"<p>This functions implemented to return cerberus url</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_client_id","title":"<code>get_client_id: Optional[str]</code>  <code>property</code>","text":"<p>This function helps in getting key / path for client id</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>client id key / path in Optional[str]</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_config_file_path","title":"<code>get_config_file_path: str</code>  <code>property</code>","text":"<p>This function returns config file abs path</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _config_file_path(str)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_debugger_mode","title":"<code>get_debugger_mode: bool</code>  <code>property</code>","text":"<p>This function returns a debugger</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>return debugger</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_dq_run_status","title":"<code>get_dq_run_status: str</code>  <code>property</code>","text":"<p>This function is used to get data quality pipeline status</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _dq_status\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_dq_run_time","title":"<code>get_dq_run_time: float</code>  <code>property</code>","text":"<p>This function implements time diff for dq run</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>time in float</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_dq_stats_table_name","title":"<code>get_dq_stats_table_name: str</code>  <code>property</code>","text":"<p>Get dq_stats_table_name to which the final stats of the dq job will be written into</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>returns the dq_stats_table_name</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_enable_mail","title":"<code>get_enable_mail: bool</code>  <code>property</code>","text":"<p>This function return whether mail notification to enable or not</p> <p>Returns:</p> Name Type Description <code>str</code> <code>bool</code> <p>Returns  _enable_mail(bool)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_enable_slack","title":"<code>get_enable_slack: bool</code>  <code>property</code>","text":"<p>This function returns whether to enable slack notification or not</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_env","title":"<code>get_env: Optional[str]</code>  <code>property</code>","text":"<p>functions returns running environment type</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>Returns _env</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_error_count","title":"<code>get_error_count: int</code>  <code>property</code>","text":"<p>This functions return error count</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Returns _error_count(int)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_error_drop_percentage","title":"<code>get_error_drop_percentage: float</code>  <code>property</code>","text":"<p>This function returns error drop percentage percentage</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>error drop percentage</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_error_drop_threshold","title":"<code>get_error_drop_threshold: int</code>  <code>property</code>","text":"<p>This function return error threshold breach</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>error threshold breach</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_error_percentage","title":"<code>get_error_percentage: float</code>  <code>property</code>","text":"<p>This function returns error percentage</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>error percentage</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_error_table_name","title":"<code>get_error_table_name: str</code>  <code>property</code>","text":"<p>Get dq_stats_table_name to which the final stats of the dq job will be written into</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>returns the dq_stats_table_name</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_final_agg_dq_result","title":"<code>get_final_agg_dq_result: Optional[List[Dict[str, str]]]</code>  <code>property</code>","text":"<p>This function return status of the final_agg_dq_result</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[List[Dict[str, str]]]</code> <p>Returns final_agg_dq_result which in list of dict with str(key) and str(value)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_final_agg_dq_run_time","title":"<code>get_final_agg_dq_run_time: float</code>  <code>property</code>","text":"<p>This function implements time diff for final agg dq run</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>time in float</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_final_agg_dq_status","title":"<code>get_final_agg_dq_status: str</code>  <code>property</code>","text":"<p>This function is used to get final aggregation data quality status</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _final_agg_dq_status\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_final_query_dq_result","title":"<code>get_final_query_dq_result: Optional[List[Dict[str, str]]]</code>  <code>property</code>","text":"<p>This function return status of the final_query_dq_result</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[List[Dict[str, str]]]</code> <p>Returns final_query_dq_result which in list of dict with str(key) and str(value)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_final_query_dq_run_time","title":"<code>get_final_query_dq_run_time: float</code>  <code>property</code>","text":"<p>This function implements time diff for final query dq run</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>time in float</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_final_query_dq_status","title":"<code>get_final_query_dq_status: str</code>  <code>property</code>","text":"<p>This function is used to get final query dq  data quality status</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _final_query_dq_status\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_final_table_name","title":"<code>get_final_table_name: str</code>  <code>property</code>","text":"<p>Get dq_stats_table_name to which the final stats of the dq job will be written into</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>returns the dq_stats_table_name</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_input_count","title":"<code>get_input_count: int</code>  <code>property</code>","text":"<p>This function return input count</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Returns _input_count(int)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_mail_from","title":"<code>get_mail_from: str</code>  <code>property</code>","text":"<p>This function returns mail id to send email</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_mail_smtp_port","title":"<code>get_mail_smtp_port: int</code>  <code>property</code>","text":"<p>This functions returns smtp port</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>returns _mail_smtp_server port</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_mail_smtp_server","title":"<code>get_mail_smtp_server: str</code>  <code>property</code>","text":"<p>This functions returns smtp server host</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>returns _mail_smtp_server</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_mail_subject","title":"<code>get_mail_subject: str</code>  <code>property</code>","text":"<p>This function returns mail subject</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _mail_subject(str)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_notification_on_completion","title":"<code>get_notification_on_completion: bool</code>  <code>property</code>","text":"<p>This function returns notification on completion</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns _notification_on_completion</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_notification_on_fail","title":"<code>get_notification_on_fail: bool</code>  <code>property</code>","text":"<p>This function returns notification on fail</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns _notification_on_fail</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_notification_on_start","title":"<code>get_notification_on_start: bool</code>  <code>property</code>","text":"<p>This function returns notification on start</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns _notification_on_start</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_num_agg_dq_rules","title":"<code>get_num_agg_dq_rules: dict</code>  <code>property</code>","text":"<p>This function returns number agg dq rules applied for batch run</p> <p>Returns:</p> Name Type Description <code>int</code> <code>dict</code> <p>number of rules in int</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_num_dq_rules","title":"<code>get_num_dq_rules: int</code>  <code>property</code>","text":"<p>This function returns number dq rules applied for batch run</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of rules in int</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_num_query_dq_rules","title":"<code>get_num_query_dq_rules: dict</code>  <code>property</code>","text":"<p>This function returns number query dq rules applied for batch run</p> <p>Returns:</p> Name Type Description <code>int</code> <code>dict</code> <p>number of rules in int</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_num_row_dq_rules","title":"<code>get_num_row_dq_rules: int</code>  <code>property</code>","text":"<p>This function returns number row dq rules applied for batch run</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of rules in int</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_output_count","title":"<code>get_output_count: int</code>  <code>property</code>","text":"<p>This function returns output count</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Returns _output(int)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_output_percentage","title":"<code>get_output_percentage: float</code>  <code>property</code>","text":"<p>This function return output percentage</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>output percentage</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_query_dq_rule_type_name","title":"<code>get_query_dq_rule_type_name: str</code>  <code>property</code>","text":"<p>This function is used to get query data quality rule type name</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _query_dq_rule_type_name\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_row_dq_rule_type_name","title":"<code>get_row_dq_rule_type_name: str</code>  <code>property</code>","text":"<p>This function is used to get row data quality rule type name</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _row_dq_rule_type_name\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_row_dq_run_time","title":"<code>get_row_dq_run_time: float</code>  <code>property</code>","text":"<p>This function implements time diff for row dq run</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>time in float</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_row_dq_status","title":"<code>get_row_dq_status: str</code>  <code>property</code>","text":"<p>This function is used to get row data quality status</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _row_dq_status\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_rules_exceeds_threshold","title":"<code>get_rules_exceeds_threshold: Optional[List[dict]]</code>  <code>property</code>","text":"<p>This function returns error percentage for each rule</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_run_date","title":"<code>get_run_date: str</code>  <code>property</code>","text":"<p>Get run_date for the instance of the spark-expectations class</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>returns the run_date</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_run_date_name","title":"<code>get_run_date_name: str</code>  <code>property</code>","text":"<p>This function returns name for the run_date column</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>name of run_date in str</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_run_date_time_name","title":"<code>get_run_date_time_name: str</code>  <code>property</code>","text":"<p>This function returns name for the run_date_time column</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>name of run_date_time in str</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_run_id","title":"<code>get_run_id: str</code>  <code>property</code>","text":"<p>Get run_id for the instance of spark-expectations class</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>returns the run_id</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_run_id_name","title":"<code>get_run_id_name: str</code>  <code>property</code>","text":"<p>This function returns name for the run_id column</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>name of run_id in str</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_se_streaming_stats_dict","title":"<code>get_se_streaming_stats_dict: Dict[str, str]</code>  <code>property</code>","text":"<p>This function returns secret keys dict</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_se_streaming_stats_topic_name","title":"<code>get_se_streaming_stats_topic_name: str</code>  <code>property</code>","text":"<p>This function returns nsp topic name</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _se_streaming_stats_topic_name</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_secret_type","title":"<code>get_secret_type: Optional[str]</code>  <code>property</code>","text":"<p>This function helps in getting secret type</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>secret type in Optional[str]</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_server_url_key","title":"<code>get_server_url_key: Optional[str]</code>  <code>property</code>","text":"<p>This function helps in getting key / path for kafka server url  Returns:       kafka server url key / path in Optional[str]</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_slack_webhook_url","title":"<code>get_slack_webhook_url: str</code>  <code>property</code>","text":"<p>This function returns sack webhook url</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _webhook_url(str)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_source_agg_dq_result","title":"<code>get_source_agg_dq_result: Optional[List[Dict[str, str]]]</code>  <code>property</code>","text":"<p>This function return status of the source_agg_dq_result</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[List[Dict[str, str]]]</code> <p>Returns source_agg_dq_result which in list of dict with str(key) and str(value)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_source_agg_dq_run_time","title":"<code>get_source_agg_dq_run_time: float</code>  <code>property</code>","text":"<p>This function implements time diff for source agg dq run</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>time in float</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_source_agg_dq_status","title":"<code>get_source_agg_dq_status: str</code>  <code>property</code>","text":"<p>This function is used to get source aggregation data quality status</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _source_agg_dq_status\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_source_query_dq_result","title":"<code>get_source_query_dq_result: Optional[List[Dict[str, str]]]</code>  <code>property</code>","text":"<p>This function return status of the source_query_dq_result</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[List[Dict[str, str]]]</code> <p>Returns source_query_dq_result which in list of dict with str(key) and str(value)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_source_query_dq_run_time","title":"<code>get_source_query_dq_run_time: float</code>  <code>property</code>","text":"<p>This function implements time diff for source query dq run</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>time in float</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_source_query_dq_status","title":"<code>get_source_query_dq_status: str</code>  <code>property</code>","text":"<p>This function is used to get source query data quality status</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _source_query_dq_status\"</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_success_percentage","title":"<code>get_success_percentage: float</code>  <code>property</code>","text":"<p>This function returns success percentage</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>success percentage</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_summarised_row_dq_res","title":"<code>get_summarised_row_dq_res: Optional[List[Dict[str, str]]]</code>  <code>property</code>","text":"<p>This function returns row dq summarised res</p> <p>Returns:</p> Name Type Description <code>list</code> <code>dict</code> <p>Returns summarised_row_dq_res which in list of dict with str(key) and</p> <code>Optional[List[Dict[str, str]]]</code> <p>str(value) of rule meta data</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_supported_df_query_dq","title":"<code>get_supported_df_query_dq: DataFrame</code>  <code>property</code>","text":"<p>This function returns the place holder dataframe for query check</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>returns dataframe for query dq</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_table_name","title":"<code>get_table_name: str</code>  <code>property</code>","text":"<p>This function returns table name</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _table_name(str)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_to_mail","title":"<code>get_to_mail: str</code>  <code>property</code>","text":"<p>This function returns list of mail id's</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns _mail_id(str)</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_token","title":"<code>get_token: Optional[str]</code>  <code>property</code>","text":"<p>This function helps in getting key / path for token</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>token key / path in Optional[str]</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_token_endpoint_url","title":"<code>get_token_endpoint_url: Optional[str]</code>  <code>property</code>","text":"<p>This function helps in getting key / path for end point url  Returns:      end point url key / path in  Optional[str]</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_topic_name","title":"<code>get_topic_name: Optional[str]</code>  <code>property</code>","text":"<p>This function helps in getting key / path for topic name</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>topic name key / path in Optional[str]</p>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.product_id","title":"<code>product_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext-functions","title":"Functions","text":""},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.get_time_diff","title":"<code>get_time_diff(start_time: Optional[datetime], end_time: Optional[datetime]) -&gt; float</code>","text":"<p>This function implements time diff</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[datetime]</code> required <code>end_time</code> <code>Optional[datetime]</code> required Source code in <code>spark_expectations/core/context.py</code> <pre><code>def get_time_diff(\nself, start_time: Optional[datetime], end_time: Optional[datetime]\n) -&gt; float:\n\"\"\"\n    This function implements time diff\n    Args:\n        start_time:\n        end_time:\n    Returns:\n    \"\"\"\nif start_time and end_time:\ntime_diff = end_time - start_time\nreturn round(float(time_diff.total_seconds()), 1)\nelse:\nreturn 0.0\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.print_dataframe_with_debugger","title":"<code>print_dataframe_with_debugger(df: DataFrame) -&gt; None</code>","text":"<p>This function has a debugger that can print out the DataFrame</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def print_dataframe_with_debugger(self, df: DataFrame) -&gt; None:\n\"\"\"\n    This function has a debugger that can print out the DataFrame\n    Returns:\n    \"\"\"\nif self.get_debugger_mode:\ndf.show(truncate=False)\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.reset_num_agg_dq_rules","title":"<code>reset_num_agg_dq_rules() -&gt; None</code>","text":"<p>This function used to reset the_num_agg_dq_rules</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def reset_num_agg_dq_rules(self) -&gt; None:\n\"\"\"\n    This function used to reset the_num_agg_dq_rules\n    Returns:\n        None\n    \"\"\"\nself._num_agg_dq_rules = {\n\"num_agg_dq_rules\": 0,\n\"num_source_agg_dq_rules\": 0,\n\"num_final_agg_dq_rules\": 0,\n}\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.reset_num_dq_rules","title":"<code>reset_num_dq_rules() -&gt; None</code>","text":"<p>This function used to reset the _num_dq_rules</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def reset_num_dq_rules(self) -&gt; None:\n\"\"\"\n    This function used to reset the _num_dq_rules\n    Returns:\n        None\n    \"\"\"\nself._num_dq_rules = 0\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.reset_num_query_dq_rules","title":"<code>reset_num_query_dq_rules() -&gt; None</code>","text":"<p>This function used to rest the _num_query_dq_rules</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def reset_num_query_dq_rules(self) -&gt; None:\n\"\"\"\n    This function used to rest the _num_query_dq_rules\n    Returns:\n        None\n    \"\"\"\nself._num_query_dq_rules = {\n\"num_query_dq_rules\": 0,\n\"num_source_query_dq_rules\": 0,\n\"num_final_query_dq_rules\": 0,\n}\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.reset_num_row_dq_rules","title":"<code>reset_num_row_dq_rules() -&gt; None</code>","text":"<p>This function used to reset the _num_row_dq_rules</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def reset_num_row_dq_rules(self) -&gt; None:\n\"\"\"\n    This function used to reset the _num_row_dq_rules\n    Returns:\n        None\n    \"\"\"\nself._num_row_dq_rules = 0  # pragma: no cover\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_debugger_mode","title":"<code>set_debugger_mode(debugger_mode: bool) -&gt; None</code>","text":"<p>This function sets debugger mode</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_debugger_mode(self, debugger_mode: bool) -&gt; None:\n\"\"\"\n    This function sets debugger mode\n    Returns:\n    \"\"\"\nself._debugger_mode = debugger_mode\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_dq_end_time","title":"<code>set_dq_end_time() -&gt; None</code>","text":"<p>This function sets end time dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_dq_end_time(self) -&gt; None:\n\"\"\"\n    This function sets end time dq computation\n    Returns:\n        None\n    \"\"\"\nself._dq_end_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_dq_run_status","title":"<code>set_dq_run_status(dq_run_status: str = 'Failed') -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_dq_run_status(self, dq_run_status: str = \"Failed\") -&gt; None:\nself._dq_run_status = dq_run_status\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_dq_start_time","title":"<code>set_dq_start_time() -&gt; None</code>","text":"<p>This function sets start time dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_dq_start_time(self) -&gt; None:\n\"\"\"\n    This function sets start time dq computation\n    Returns:\n        None\n    \"\"\"\nself._dq_start_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_dq_stats_table_name","title":"<code>set_dq_stats_table_name(dq_stats_table_name: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_dq_stats_table_name(self, dq_stats_table_name: str) -&gt; None:\nself._dq_stats_table_name = dq_stats_table_name\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_enable_mail","title":"<code>set_enable_mail(enable_mail: bool) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_enable_mail(self, enable_mail: bool) -&gt; None:\nself._enable_mail = bool(enable_mail)\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_enable_slack","title":"<code>set_enable_slack(enable_slack: bool) -&gt; None</code>","text":"<p>Parameters:</p> Name Type Description Default <code>enable_slack</code> <code>bool</code> required Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_enable_slack(self, enable_slack: bool) -&gt; None:\n\"\"\"\n    Args:\n        enable_slack:\n    Returns:\n    \"\"\"\nself._enable_slack = enable_slack\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_end_time_when_dq_job_fails","title":"<code>set_end_time_when_dq_job_fails() -&gt; None</code>","text":"<p>function used to set end time when job fails in any one of the stages by using start time</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_end_time_when_dq_job_fails(self) -&gt; None:\n\"\"\"\n    function used to set end time when job fails in any one of the stages by using start time\n    Returns:\n    \"\"\"\nif self._source_agg_dq_start_time and self._source_agg_dq_end_time is None:\nself.set_source_agg_dq_end_time()\nelif (\nself._source_query_dq_start_time and self._source_query_dq_end_time is None\n):\nself.set_source_query_dq_end_time()\nelif self._row_dq_start_time and self._row_dq_end_time is None:\nself.set_row_dq_end_time()\nelif self._final_agg_dq_start_time and self._final_agg_dq_end_time is None:\nself.set_final_agg_dq_end_time()\nelif self._final_query_dq_start_time and self._final_query_dq_end_time is None:\nself.set_final_query_dq_end_time()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_env","title":"<code>set_env(env: Optional[str]) -&gt; None</code>","text":"<p>Parameters:</p> Name Type Description Default <code>env</code> <code>Optional[str]</code> <p>which accepts env type</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_env(self, env: Optional[str]) -&gt; None:\n\"\"\"\n    Args:\n        env: which accepts env type\n    Returns:\n        None\n    \"\"\"\nself._env = env\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_error_count","title":"<code>set_error_count(error_count: int = 0) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_error_count(self, error_count: int = 0) -&gt; None:\nself._error_count = error_count\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_error_drop_threshold","title":"<code>set_error_drop_threshold(error_drop_threshold: int) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_error_drop_threshold(self, error_drop_threshold: int) -&gt; None:\nself._error_drop_threshold = error_drop_threshold\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_error_table_name","title":"<code>set_error_table_name(error_table_name: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_error_table_name(self, error_table_name: str) -&gt; None:\nself._error_table_name = error_table_name\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_agg_dq_end_time","title":"<code>set_final_agg_dq_end_time() -&gt; None</code>","text":"<p>This function sets end time final agg dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_agg_dq_end_time(self) -&gt; None:\n\"\"\"\n    This function sets end time final agg dq computation\n    Returns:\n        None\n    \"\"\"\nself._final_agg_dq_end_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_agg_dq_result","title":"<code>set_final_agg_dq_result(final_agg_dq_result: Optional[List[Dict[str, str]]] = None) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_agg_dq_result(\nself, final_agg_dq_result: Optional[List[Dict[str, str]]] = None\n) -&gt; None:\nself._final_agg_dq_result = final_agg_dq_result\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_agg_dq_start_time","title":"<code>set_final_agg_dq_start_time() -&gt; None</code>","text":"<p>This function sets start time final agg dq computation</p> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_agg_dq_start_time(self) -&gt; None:\n\"\"\"\n    This function sets start time final agg dq computation\n    Returns:\n    None\n    \"\"\"\nself._final_agg_dq_start_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_agg_dq_status","title":"<code>set_final_agg_dq_status(final_agg_dq_status: str = 'Skipped') -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_agg_dq_status(self, final_agg_dq_status: str = \"Skipped\") -&gt; None:\nself._final_agg_dq_status = final_agg_dq_status\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_query_dq_end_time","title":"<code>set_final_query_dq_end_time() -&gt; None</code>","text":"<p>This function sets end time final query dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_query_dq_end_time(self) -&gt; None:\n\"\"\"\n    This function sets end time final query dq computation\n    Returns:\n        None\n    \"\"\"\nself._final_query_dq_end_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_query_dq_result","title":"<code>set_final_query_dq_result(final_query_dq_result: Optional[List[Dict[str, str]]] = None) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_query_dq_result(\nself, final_query_dq_result: Optional[List[Dict[str, str]]] = None\n) -&gt; None:\nself._final_query_dq_result = final_query_dq_result\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_query_dq_start_time","title":"<code>set_final_query_dq_start_time() -&gt; None</code>","text":"<p>This function sets start time final query dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_query_dq_start_time(self) -&gt; None:\n\"\"\"\n    This function sets start time final query dq computation\n    Returns:\n        None\n    \"\"\"\nself._final_query_dq_start_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_query_dq_status","title":"<code>set_final_query_dq_status(final_query_dq_status: str = 'Skipped') -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_query_dq_status(self, final_query_dq_status: str = \"Skipped\") -&gt; None:\nself._final_query_dq_status = final_query_dq_status\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_final_table_name","title":"<code>set_final_table_name(final_table_name: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_final_table_name(self, final_table_name: str) -&gt; None:\nself._final_table_name = final_table_name\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_input_count","title":"<code>set_input_count(input_count: int = 0) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_input_count(self, input_count: int = 0) -&gt; None:\nself._input_count = input_count\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_mail_from","title":"<code>set_mail_from(mail_from: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_mail_from(self, mail_from: str) -&gt; None:\nself._mail_from = mail_from\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_mail_smtp_port","title":"<code>set_mail_smtp_port(mail_smtp_port: int) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_mail_smtp_port(self, mail_smtp_port: int) -&gt; None:\nself._mail_smtp_port = mail_smtp_port\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_mail_smtp_server","title":"<code>set_mail_smtp_server(mail_smtp_server: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_mail_smtp_server(self, mail_smtp_server: str) -&gt; None:\nself._mail_smtp_server = mail_smtp_server\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_mail_subject","title":"<code>set_mail_subject(mail_subject: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_mail_subject(self, mail_subject: str) -&gt; None:\nself._mail_subject = mail_subject\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_notification_on_completion","title":"<code>set_notification_on_completion(notification_on_completion: bool) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_notification_on_completion(self, notification_on_completion: bool) -&gt; None:\nself._notification_on_completion = notification_on_completion\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_notification_on_fail","title":"<code>set_notification_on_fail(notification_on_fail: bool) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_notification_on_fail(self, notification_on_fail: bool) -&gt; None:\nself._notification_on_fail = notification_on_fail\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_notification_on_start","title":"<code>set_notification_on_start(notification_on_start: bool) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_notification_on_start(self, notification_on_start: bool) -&gt; None:\nself._notification_on_start = notification_on_start\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_num_agg_dq_rules","title":"<code>set_num_agg_dq_rules(source_agg_enabled: bool = False, final_agg_enabled: bool = False) -&gt; None</code>","text":"<p>This function sets number of applied agg dq rules for batch run source_agg_enabled: Marked True when agg rules set for source, by default False final_agg_enabled: Marked True when agg rules set for final, by default False</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_num_agg_dq_rules(\nself, source_agg_enabled: bool = False, final_agg_enabled: bool = False\n) -&gt; None:\n\"\"\"\n    This function sets number of applied agg dq rules for batch run\n    source_agg_enabled: Marked True when agg rules set for source, by default False\n    final_agg_enabled: Marked True when agg rules set for final, by default False\n    Returns:\n        None\n    \"\"\"\nself._num_agg_dq_rules[\"num_agg_dq_rules\"] += 1\nself._num_dq_rules += 1\nif source_agg_enabled:\nself._num_agg_dq_rules[\"num_source_agg_dq_rules\"] += 1\nif final_agg_enabled:\nself._num_agg_dq_rules[\"num_final_agg_dq_rules\"] += 1\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_num_query_dq_rules","title":"<code>set_num_query_dq_rules(source_query_enabled: bool = False, final_query_enabled: bool = False) -&gt; None</code>","text":"<p>This function sets number of applied query dq rules for batch run source_query_enabled: Marked True when query rules set for source, by default False final_query_enabled: Marked True when query rules set for final, by default False</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_num_query_dq_rules(\nself, source_query_enabled: bool = False, final_query_enabled: bool = False\n) -&gt; None:\n\"\"\"\n    This function sets number of applied query dq rules for batch run\n    source_query_enabled: Marked True when query rules set for source, by default False\n    final_query_enabled: Marked True when query rules set for final, by default False\n    Returns:\n        None\n    \"\"\"\nself._num_query_dq_rules[\"num_query_dq_rules\"] += 1\nself._num_dq_rules += 1\nif source_query_enabled:\nself._num_query_dq_rules[\"num_source_query_dq_rules\"] += 1\nif final_query_enabled:\nself._num_query_dq_rules[\"num_final_query_dq_rules\"] += 1\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_num_row_dq_rules","title":"<code>set_num_row_dq_rules() -&gt; None</code>","text":"<p>This function sets number of applied row dq rules for batch run</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_num_row_dq_rules(self) -&gt; None:\n\"\"\"\n    This function sets number of applied row dq rules for batch run\n    Returns:\n        None\n    \"\"\"\nself._num_row_dq_rules += 1\nself._num_dq_rules += 1\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_output_count","title":"<code>set_output_count(output_count: int = 0) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_output_count(self, output_count: int = 0) -&gt; None:\nself._output_count = output_count\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_row_dq_end_time","title":"<code>set_row_dq_end_time() -&gt; None</code>","text":"<p>This function sets end time row dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_row_dq_end_time(self) -&gt; None:\n\"\"\"\n    This function sets end time row dq computation\n    Returns:\n        None\n    \"\"\"\nself._row_dq_end_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_row_dq_start_time","title":"<code>set_row_dq_start_time() -&gt; None</code>","text":"<p>This function sets start time row dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_row_dq_start_time(self) -&gt; None:\n\"\"\"\n    This function sets start time row dq computation\n    Returns:\n        None\n    \"\"\"\nself._row_dq_start_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_row_dq_status","title":"<code>set_row_dq_status(row_dq_status: str = 'Skipped') -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_row_dq_status(self, row_dq_status: str = \"Skipped\") -&gt; None:\nself._row_dq_status = row_dq_status\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_rules_exceeds_threshold","title":"<code>set_rules_exceeds_threshold(rules: Optional[List[dict]] = None) -&gt; None</code>","text":"<p>This function implements error percentage for each rule type</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_rules_exceeds_threshold(self, rules: Optional[List[dict]] = None) -&gt; None:\n\"\"\"\n    This function implements error percentage for each rule type\n    \"\"\"\nself._rules_error_per = rules\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_run_date","title":"<code>set_run_date() -&gt; str</code>  <code>staticmethod</code>","text":"<p>This function is used to generate the current datatime in UTC</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns the current utc datatime in the format - \"%Y-%m-%d %H:%M:%S\"</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>@staticmethod\ndef set_run_date() -&gt; str:\n\"\"\"\n    This function is used to generate the current datatime in UTC\n    Returns:\n        str: Returns the current utc datatime in the format - \"%Y-%m-%d %H:%M:%S\"\n    \"\"\"\ncurrent_datetime: datetime = datetime.now(timezone.utc)\nreturn current_datetime.replace(tzinfo=timezone.utc).strftime(\n\"%Y-%m-%d %H:%M:%S\"\n)\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_se_streaming_stats_dict","title":"<code>set_se_streaming_stats_dict(se_streaming_stats_dict: Dict[str, str]) -&gt; None</code>","text":"<p>This function helps to set secret keys dict</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_se_streaming_stats_dict(\nself, se_streaming_stats_dict: Dict[str, str]\n) -&gt; None:\n\"\"\"\n    This function helps to set secret keys dict\"\"\"\nself._se_streaming_stats_dict = se_streaming_stats_dict\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_se_streaming_stats_topic_name","title":"<code>set_se_streaming_stats_topic_name(se_streaming_stats_topic_name: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_se_streaming_stats_topic_name(\nself, se_streaming_stats_topic_name: str\n) -&gt; None:\nself._se_streaming_stats_topic_name = se_streaming_stats_topic_name\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_slack_webhook_url","title":"<code>set_slack_webhook_url(slack_webhook_url: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_slack_webhook_url(self, slack_webhook_url: str) -&gt; None:\nself._slack_webhook_url = slack_webhook_url\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_agg_dq_end_time","title":"<code>set_source_agg_dq_end_time() -&gt; None</code>","text":"<p>This function sets end time source agg dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_agg_dq_end_time(self) -&gt; None:\n\"\"\"\n    This function sets end time source agg dq computation\n    Returns:\n        None\n    \"\"\"\nself._source_agg_dq_end_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_agg_dq_result","title":"<code>set_source_agg_dq_result(source_agg_dq_result: Optional[List[Dict[str, str]]] = None) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_agg_dq_result(\nself, source_agg_dq_result: Optional[List[Dict[str, str]]] = None\n) -&gt; None:\nself._source_agg_dq_result = source_agg_dq_result\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_agg_dq_start_time","title":"<code>set_source_agg_dq_start_time() -&gt; None</code>","text":"<p>This function sets start time source agg dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_agg_dq_start_time(self) -&gt; None:\n\"\"\"\n    This function sets start time source agg dq computation\n    Returns:\n         None\n    \"\"\"\nself._source_agg_dq_start_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_agg_dq_status","title":"<code>set_source_agg_dq_status(source_agg_dq_status: str = 'Skipped') -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_agg_dq_status(self, source_agg_dq_status: str = \"Skipped\") -&gt; None:\nself._source_agg_dq_status = source_agg_dq_status\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_query_dq_end_time","title":"<code>set_source_query_dq_end_time() -&gt; None</code>","text":"<p>This function sets end time source query dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_query_dq_end_time(self) -&gt; None:\n\"\"\"\n    This function sets end time source query dq computation\n    Returns:\n        None\n    \"\"\"\nself._source_query_dq_end_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_query_dq_result","title":"<code>set_source_query_dq_result(source_query_dq_result: Optional[List[Dict[str, str]]] = None) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_query_dq_result(\nself, source_query_dq_result: Optional[List[Dict[str, str]]] = None\n) -&gt; None:\nself._source_query_dq_result = source_query_dq_result\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_query_dq_start_time","title":"<code>set_source_query_dq_start_time() -&gt; None</code>","text":"<p>This function sets start time source query dq computation</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_query_dq_start_time(self) -&gt; None:\n\"\"\"\n    This function sets start time source query dq computation\n    Returns:\n        None\n    \"\"\"\nself._source_query_dq_start_time = datetime.now()\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_source_query_dq_status","title":"<code>set_source_query_dq_status(source_query_dq_status: str = 'Skipped') -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_source_query_dq_status(\nself, source_query_dq_status: str = \"Skipped\"\n) -&gt; None:\nself._source_query_dq_status = source_query_dq_status\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_summarised_row_dq_res","title":"<code>set_summarised_row_dq_res(summarised_row_dq_res: Optional[List[Dict[str, str]]] = None) -&gt; None</code>","text":"<p>This function implements or supports to set row dq summarised res</p> <p>Parameters:</p> Name Type Description Default <code>summarised_row_dq_res</code> <code>Optional[List[Dict[str, str]]]</code> <p>list(dict)</p> <code>None</code> Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_summarised_row_dq_res(\nself, summarised_row_dq_res: Optional[List[Dict[str, str]]] = None\n) -&gt; None:\n\"\"\"\n    This function implements or supports to set row dq summarised res\n    Args:\n        summarised_row_dq_res: list(dict)\n    Returns: None\n    \"\"\"\nself._summarised_row_dq_res = summarised_row_dq_res\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_supported_df_query_dq","title":"<code>set_supported_df_query_dq() -&gt; DataFrame</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_supported_df_query_dq(self) -&gt; DataFrame:\nreturn self.spark.createDataFrame(\n[\n{\n\"spark_expectations_query_check\": \"supported_place_holder_dataset_to_run_query_check\"\n}\n]\n)\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_table_name","title":"<code>set_table_name(table_name: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_table_name(self, table_name: str) -&gt; None:\nself._table_name = table_name\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context.SparkExpectationsContext.set_to_mail","title":"<code>set_to_mail(to_mail: str) -&gt; None</code>","text":"Source code in <code>spark_expectations/core/context.py</code> <pre><code>def set_to_mail(self, to_mail: str) -&gt; None:\nself._to_mail = to_mail\n</code></pre>"},{"location":"api/context/#spark_expectations.core.context-functions","title":"Functions","text":""},{"location":"api/core_init/","title":"Init","text":""},{"location":"api/core_init/#spark_expectations.core-attributes","title":"Attributes","text":""},{"location":"api/core_init/#spark_expectations.core.current_dir","title":"<code>spark_expectations.core.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/core_init/#spark_expectations.core-functions","title":"Functions","text":""},{"location":"api/core_init/#spark_expectations.core.get_spark_session","title":"<code>spark_expectations.core.get_spark_session() -&gt; SparkSession</code>","text":"Source code in <code>spark_expectations/core/__init__.py</code> <pre><code>def get_spark_session() -&gt; SparkSession:\nbuilder = SparkSession.builder\nif (\nos.environ.get(\"UNIT_TESTING_ENV\")\n== \"spark_expectations_unit_testing_on_github_actions\"\n) or (os.environ.get(\"SPARKEXPECTATIONS_ENV\") == \"local\"):\nbuilder = (\nbuilder.config(\n\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"\n)\n.config(\n\"spark.sql.catalog.spark_catalog\",\n\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n)\n.config(\"spark.sql.warehouse.dir\", \"/tmp/hive/warehouse\")\n.config(\"spark.driver.extraJavaOptions\", \"-Dderby.system.home=/tmp/derby\")\n.config(\"spark.jars.ivy\", \"/tmp/ivy2\")\n.config(  # below jars are used only in the local env, not coupled with databricks or EMR\n\"spark.jars\",\nf\"{current_dir}/../../jars/spark-sql-kafka-0-10_2.12-3.0.0.jar,\"\nf\"{current_dir}/../../jars/kafka-clients-3.0.0.jar,\"\nf\"{current_dir}/../../jars/commons-pool2-2.8.0.jar,\"\nf\"{current_dir}/../../jars/spark-token-provider-kafka-0-10_2.12-3.0.0.jar\",\n)\n# .config(\"spark.databricks.delta.checkLatestSchemaOnRead\", \"false\")\n)\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\nreturn spark\n</code></pre>"},{"location":"api/delta_sink_plugin/","title":"Delta_Sink_Plugin","text":""},{"location":"api/delta_sink_plugin/#spark_expectations.sinks.plugins.delta_writer-attributes","title":"Attributes","text":""},{"location":"api/delta_sink_plugin/#spark_expectations.sinks.plugins.delta_writer-classes","title":"Classes","text":""},{"location":"api/delta_sink_plugin/#spark_expectations.sinks.plugins.delta_writer.SparkExpectationsDeltaWritePluginImpl","title":"<code>spark_expectations.sinks.plugins.delta_writer.SparkExpectationsDeltaWritePluginImpl</code>","text":"<p>             Bases: <code>SparkExpectationsSinkWriter</code></p> <p>function implements/supports data into the delta table</p>"},{"location":"api/delta_sink_plugin/#spark_expectations.sinks.plugins.delta_writer.SparkExpectationsDeltaWritePluginImpl-functions","title":"Functions","text":""},{"location":"api/delta_sink_plugin/#spark_expectations.sinks.plugins.delta_writer.SparkExpectationsDeltaWritePluginImpl.writer","title":"<code>writer(_write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]) -&gt; None</code>","text":"<p>Parameters:</p> Name Type Description Default <code>_write_args</code> <code>Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]</code> required Source code in <code>spark_expectations/sinks/plugins/delta_writer.py</code> <pre><code>@spark_expectations_writer_impl\ndef writer(\nself, _write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]\n) -&gt; None:\n\"\"\"\n    Args:\n        _write_args:\n    Returns:\n    \"\"\"\ntry:\n_log.info(\"started writing data into delta stats table\")\ndf: DataFrame = _write_args.get(\"stats_df\")\ndf.write.saveAsTable(\nname=f\"{_write_args.get('table_name')}\",\n**{\"mode\": \"append\", \"format\": \"delta\", \"mergeSchema\": \"true\"},\n)\nget_spark_session().sql(\nf\"ALTER TABLE {_write_args.get('table_name')} \"\nf\"SET TBLPROPERTIES ('product_id' = '{_write_args.get('product_id')}')\"\n)\n_log.info(\"ended writing data into delta stats table\")\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while saving data into delta stats table {e}\"\n)\n</code></pre>"},{"location":"api/delta_sink_plugin/#spark_expectations.sinks.plugins.delta_writer-functions","title":"Functions","text":""},{"location":"api/email_plugin/","title":"Email_Notification_plugin","text":""},{"location":"api/email_plugin/#spark_expectations.notifications.plugins.email-attributes","title":"Attributes","text":""},{"location":"api/email_plugin/#spark_expectations.notifications.plugins.email-classes","title":"Classes","text":""},{"location":"api/email_plugin/#spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl","title":"<code>spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl</code>","text":"<p>             Bases: <code>SparkExpectationsNotification</code></p> <p>This class implements/supports functionality to send email</p>"},{"location":"api/email_plugin/#spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl-functions","title":"Functions","text":""},{"location":"api/email_plugin/#spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl.send_notification","title":"<code>send_notification(_context: SparkExpectationsContext, _config_args: Dict[Union[str], Union[str, bool]]) -&gt; None</code>","text":"<p>function to send email notification for requested mail id's</p> <p>Parameters:</p> Name Type Description Default <code>_context</code> <code>SparkExpectationsContext</code> <p>object of SparkExpectationsContext</p> required <code>_config_args</code> <code>Dict[Union[str], Union[str, bool]]</code> <p>dict(which consist to: receiver mail(str), subject: subject of           the mail(str) and body: body of the mail(str)</p> required Source code in <code>spark_expectations/notifications/plugins/email.py</code> <pre><code>@spark_expectations_notification_impl\ndef send_notification(\nself,\n_context: SparkExpectationsContext,\n_config_args: Dict[Union[str], Union[str, bool]],\n) -&gt; None:\n\"\"\"\n    function to send email notification for requested mail id's\n    Args:\n        _context: object of SparkExpectationsContext\n        _config_args: dict(which consist to: receiver mail(str), subject: subject of\n                      the mail(str) and body: body of the mail(str)\n    Returns:\n    \"\"\"\ntry:\nif _context.get_enable_mail is True:\nmsg = MIMEMultipart()\nmsg[\"From\"] = _context.get_mail_from\nmsg[\"To\"] = _context.get_to_mail\nmsg[\"Subject\"] = _context.get_mail_subject\n# body = _config_args.get('mail_body')\nmail_content = f\"\"\"{_config_args.get(\"message\")}\"\"\"\nmsg.attach(MIMEText(mail_content, \"plain\"))\n# mailhost.com\nserver = smtplib.SMTP(\n_context.get_mail_smtp_server, _context.get_mail_smtp_port\n)\nserver.starttls()\ntext = msg.as_string()\nserver.sendmail(_context.get_mail_from, _context.get_to_mail, text)\nserver.quit()\n_log.info(\"email send successfully\")\nexcept Exception as e:\nraise SparkExpectationsEmailException(\nf\"error occurred while sending email notification from spark expectations project {e}\"\n)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#spark_expectations.core.exceptions-classes","title":"Classes","text":""},{"location":"api/exceptions/#spark_expectations.core.exceptions.SparkExpectOrFailException","title":"<code>spark_expectations.core.exceptions.SparkExpectOrFailException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Throw this exception if a rule fails and is expected to fail the job</p>"},{"location":"api/exceptions/#spark_expectations.core.exceptions.SparkExpectationsDataframeNotReturnedException","title":"<code>spark_expectations.core.exceptions.SparkExpectationsDataframeNotReturnedException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Throw this exception if a function doesn't return a dataframe</p>"},{"location":"api/exceptions/#spark_expectations.core.exceptions.SparkExpectationsEmailException","title":"<code>spark_expectations.core.exceptions.SparkExpectationsEmailException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Throw this exception when spark expectations encounters miscellaneous exceptions</p>"},{"location":"api/exceptions/#spark_expectations.core.exceptions.SparkExpectationsErrorThresholdExceedsException","title":"<code>spark_expectations.core.exceptions.SparkExpectationsErrorThresholdExceedsException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Throw this exception when error percentage exceeds certain configured value</p>"},{"location":"api/exceptions/#spark_expectations.core.exceptions.SparkExpectationsMiscException","title":"<code>spark_expectations.core.exceptions.SparkExpectationsMiscException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Throw this exception when spark expectations encounters miscellaneous exceptions</p>"},{"location":"api/exceptions/#spark_expectations.core.exceptions.SparkExpectationsSlackNotificationException","title":"<code>spark_expectations.core.exceptions.SparkExpectationsSlackNotificationException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Throw this exception when spark expectations encounters miscellaneous exceptions</p>"},{"location":"api/exceptions/#spark_expectations.core.exceptions.SparkExpectationsUserInputOrConfigInvalidException","title":"<code>spark_expectations.core.exceptions.SparkExpectationsUserInputOrConfigInvalidException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Throw this exception when configured rule or value from the user is wrong</p>"},{"location":"api/expectations/","title":"Expectations","text":""},{"location":"api/expectations/#spark_expectations.core.expectations-classes","title":"Classes","text":""},{"location":"api/expectations/#spark_expectations.core.expectations.SparkExpectations","title":"<code>spark_expectations.core.expectations.SparkExpectations</code>  <code>dataclass</code>","text":"<p>This class implements/supports running the data quality rules on a dataframe returned by a function</p>"},{"location":"api/expectations/#spark_expectations.core.expectations.SparkExpectations-attributes","title":"Attributes","text":""},{"location":"api/expectations/#spark_expectations.core.expectations.SparkExpectations.debugger","title":"<code>debugger: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/expectations/#spark_expectations.core.expectations.SparkExpectations.product_id","title":"<code>product_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/expectations/#spark_expectations.core.expectations.SparkExpectations.stats_streaming_options","title":"<code>stats_streaming_options: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/expectations/#spark_expectations.core.expectations.SparkExpectations-functions","title":"Functions","text":""},{"location":"api/expectations/#spark_expectations.core.expectations.SparkExpectations.with_expectations","title":"<code>with_expectations(expectations: dict, write_to_table: bool = False, write_to_temp_table: bool = False, row_dq: bool = True, agg_dq: Optional[Dict[str, bool]] = None, query_dq: Optional[Dict[str, Union[str, bool]]] = None, spark_conf: Optional[Dict[str, Any]] = None, options: Optional[Dict[str, str]] = None, options_error_table: Optional[Dict[str, str]] = None) -&gt; Any</code>","text":"<p>This decorator helps to wrap a function which returns dataframe and apply dataframe rules on it</p> <p>Parameters:</p> Name Type Description Default <code>expectations</code> <code>dict</code> <p>Dict of dict's with table and rules as keys</p> required <code>write_to_table</code> <code>bool</code> <p>Mark it as \"True\" if the dataframe need to be written as table</p> <code>False</code> <code>write_to_temp_table</code> <code>bool</code> <p>Mark it as \"True\" if the input dataframe need to be written to the temp table to break                 the spark plan</p> <code>False</code> <code>row_dq</code> <code>bool</code> <p>Mark it as False to avoid row level expectation, by default is TRUE,</p> <code>True</code> <code>agg_dq</code> <code>Optional[Dict[str, bool]]</code> <p>There are several dictionary variables that are used for data quality (DQ) aggregation in both the</p> <code>None</code> <code>query_dq</code> <code>Optional[Dict[str, Union[str, bool]]]</code> <p>There are several dictionary variables that are used for data quality (DQ) using query in both</p> <code>None</code> <code>spark_conf</code> <code>Optional[Dict[str, Any]]</code> <p>Provide SparkConf to override the defaults, while writing into the table &amp; which also contains</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, str]]</code> <p>Provide Options to override the defaults, while writing into the table</p> <code>None</code> <code>options_error_table</code> <code>Optional[Dict[str, str]]</code> <p>Provide options to override the defaults, while writing into the error table</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Returns a function which applied the expectations on dataset</p> Source code in <code>spark_expectations/core/expectations.py</code> <pre><code>def with_expectations(\nself,\nexpectations: dict,\nwrite_to_table: bool = False,\nwrite_to_temp_table: bool = False,\nrow_dq: bool = True,\nagg_dq: Optional[Dict[str, bool]] = None,\nquery_dq: Optional[Dict[str, Union[str, bool]]] = None,\nspark_conf: Optional[Dict[str, Any]] = None,\noptions: Optional[Dict[str, str]] = None,\noptions_error_table: Optional[Dict[str, str]] = None,\n) -&gt; Any:\n\"\"\"\n    This decorator helps to wrap a function which returns dataframe and apply dataframe rules on it\n    Args:\n        expectations: Dict of dict's with table and rules as keys\n        write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n        write_to_temp_table: Mark it as \"True\" if the input dataframe need to be written to the temp table to break\n                            the spark plan\n        row_dq: Mark it as False to avoid row level expectation, by default is TRUE,\n        agg_dq:  There are several dictionary variables that are used for data quality (DQ) aggregation in both the\n        source and final DQ layers\n                 agg_dq =&gt; Mark it as True to run agg level expectation, by default is False\n                 source_agg_dq =&gt; Mark it as True to run source agg level expectation, by default is False\n                 final_agg_dq =&gt; Mark it as True to run final agg level expectation, by default is False\n        query_dq:  There are several dictionary variables that are used for data quality (DQ) using query in both\n        the source and final DQ layers\n                 query_dq =&gt; Mark it as True to run query level expectation, by default is False\n                 source_query_dq =&gt; Mark it as True to run query dq level expectation, by default is False\n                 final_query_dq =&gt; Mark it as True to run query dq level expectation, by default is False\n        spark_conf: Provide SparkConf to override the defaults, while writing into the table &amp; which also contains\n        notifications related variables\n        options: Provide Options to override the defaults, while writing into the table\n        options_error_table: Provide options to override the defaults, while writing into the error table\n    Returns:\n        Any: Returns a function which applied the expectations on dataset\n    \"\"\"\ndef _except(func: Any) -&gt; Any:\n# variable used for enabling source agg dq at different level\n_default_agg_dq_dict: Dict[str, bool] = {\nuser_config.se_agg_dq: False,\nuser_config.se_source_agg_dq: False,\nuser_config.se_final_agg_dq: False,\n}\n_agg_dq_dict: Dict[str, bool] = (\n{**_default_agg_dq_dict, **agg_dq} if agg_dq else _default_agg_dq_dict\n)\n# variable used for enabling query dq at different level\n_default_query_dq_dict: Dict[str, Union[str, bool]] = {\nuser_config.se_query_dq: False,\nuser_config.se_source_query_dq: False,\nuser_config.se_final_query_dq: False,\nuser_config.se_target_table_view: \"\",\n}\n_query_dq_dict: Dict[str, Union[str, bool]] = (\n{**_default_query_dq_dict, **query_dq}\nif query_dq\nelse _default_query_dq_dict\n)\n# variable used for enabling notification at different level\n_default_notification_dict: Dict[str, Union[str, int, bool]] = {\nuser_config.se_notifications_on_start: False,\nuser_config.se_notifications_on_completion: False,\nuser_config.se_notifications_on_fail: True,\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach: False,\nuser_config.se_notifications_on_error_drop_threshold: 100,\n}\n_notification_dict: Dict[str, Union[str, int, bool]] = (\n{**_default_notification_dict, **spark_conf}\nif spark_conf\nelse _default_notification_dict\n)\n_default_stats_streaming_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True,\nuser_config.secret_type: \"databricks\",\nuser_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\",\nuser_config.dbx_secret_scope: \"sole_common_prod\",\nuser_config.dbx_kafka_server_url: \"se_streaming_server_url_secret_key\",\nuser_config.dbx_secret_token_url: \"se_streaming_auth_secret_token_url_key\",\nuser_config.dbx_secret_app_name: \"se_streaming_auth_secret_appid_key\",\nuser_config.dbx_secret_token: \"se_streaming_auth_secret_token_key\",\nuser_config.dbx_topic_name: \"se_streaming_topic_name\",\n}\n_se_stats_streaming_dict: Dict[str, Any] = (\n{**self.stats_streaming_options}\nif self.stats_streaming_options\nelse _default_stats_streaming_dict\n)\n_agg_dq: bool = (\n_agg_dq_dict[user_config.se_agg_dq]\nif isinstance(_agg_dq_dict[user_config.se_agg_dq], bool)\nelse False\n)\n_source_agg_dq: bool = (\n_agg_dq_dict[user_config.se_source_agg_dq]\nif isinstance(_agg_dq_dict[user_config.se_source_agg_dq], bool)\nelse False\n)\n_final_agg_dq: bool = (\n_agg_dq_dict[user_config.se_final_agg_dq]\nif isinstance(_agg_dq_dict[user_config.se_final_agg_dq], bool)\nelse False\n)\n_query_dq: bool = (\nbool(_query_dq_dict[user_config.se_query_dq])\nif isinstance(_query_dq_dict[user_config.se_query_dq], bool)\nelse False\n)\n_source_query_dq: bool = (\nbool(_query_dq_dict[user_config.se_source_query_dq])\nif isinstance(_query_dq_dict[user_config.se_source_query_dq], bool)\nelse False\n)\n_final_query_dq: bool = (\nbool(_query_dq_dict[user_config.se_final_query_dq])\nif isinstance(_query_dq_dict[user_config.se_final_query_dq], bool)\nelse False\n)\n_target_table_view: str = (\nstr(_query_dq_dict[user_config.se_target_table_view])\nif isinstance(\n_query_dq_dict[user_config.se_target_table_view],\nstr,\n)\nelse \"\"\n)\n_notification_on_start: bool = (\nbool(_notification_dict[user_config.se_notifications_on_start])\nif isinstance(\n_notification_dict[user_config.se_notifications_on_start],\nbool,\n)\nelse False\n)\n_notification_on_completion: bool = (\nbool(_notification_dict[user_config.se_notifications_on_completion])\nif isinstance(\n_notification_dict[user_config.se_notifications_on_completion],\nbool,\n)\nelse False\n)\n_notification_on_fail: bool = (\nbool(_notification_dict[user_config.se_notifications_on_fail])\nif isinstance(\n_notification_dict[user_config.se_notifications_on_fail],\nbool,\n)\nelse False\n)\n_notification_on_error_drop_exceeds_threshold_breach: bool = (\nbool(\n_notification_dict[\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach\n]\n)\nif isinstance(\n_notification_dict[\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach\n],\nbool,\n)\nelse False\n)\n_error_drop_threshold: int = (\nint(\n_notification_dict[\nuser_config.se_notifications_on_error_drop_threshold\n]\n)\nif isinstance(\n_notification_dict[\nuser_config.se_notifications_on_error_drop_threshold\n],\nint,\n)\nelse 100\n)\nself.reader.set_notification_param(spark_conf)\nself._context.set_notification_on_start(_notification_on_start)\nself._context.set_notification_on_completion(_notification_on_completion)\nself._context.set_notification_on_fail(_notification_on_fail)\nself._context.set_se_streaming_stats_dict(_se_stats_streaming_dict)\n@self._notification.send_notification_decorator\n@self._statistics_decorator.collect_stats_decorator\n@functools.wraps(func)\ndef wrapper(*args: tuple, **kwargs: dict) -&gt; DataFrame:\ntry:\n_log.info(\"The function dataframe is getting created\")\n# _df: DataFrame = func(*args, **kwargs)\n_df: DataFrame = func(*args, **kwargs)\ntable_name: str = self._context.get_table_name\n_input_count = _df.count()\n_output_count: int = 0\n_error_count: int = 0\n_source_dq_df: Optional[DataFrame] = None\n_source_query_dq_df: Optional[DataFrame] = None\n_row_dq_df: Optional[DataFrame] = None\n_final_dq_df: Optional[DataFrame] = None\n_final_query_dq_df: Optional[DataFrame] = None\n# initialize variable with default values through set\nself._context.set_dq_run_status()\nself._context.set_source_agg_dq_status()\nself._context.set_source_query_dq_status()\nself._context.set_row_dq_status()\nself._context.set_final_agg_dq_status()\nself._context.set_final_query_dq_status()\nself._context.set_input_count()\nself._context.set_error_count()\nself._context.set_output_count()\nself._context.set_source_agg_dq_result()\nself._context.set_final_agg_dq_result()\nself._context.set_source_query_dq_result()\nself._context.set_final_query_dq_result()\nself._context.set_summarised_row_dq_res()\n# initialize variables of start and end time with default values\nself._context._source_agg_dq_start_time = None\nself._context._final_agg_dq_start_time = None\nself._context._source_query_dq_start_time = None\nself._context._final_query_dq_start_time = None\nself._context._row_dq_start_time = None\nself._context._source_agg_dq_end_time = None\nself._context._final_agg_dq_end_time = None\nself._context._source_query_dq_end_time = None\nself._context._final_query_dq_end_time = None\nself._context._row_dq_end_time = None\nself._context.set_input_count(_input_count)\nself._context.set_error_drop_threshold(_error_drop_threshold)\nif isinstance(_df, DataFrame):\n_log.info(\"The function dataframe is created\")\nself._context.set_table_name(table_name)\nif write_to_temp_table:\n_log.info(\"Dropping to temp table started\")\nself.spark.sql(f\"drop table if exists {table_name}_temp\")\n_log.info(\"Dropping to temp table completed\")\n_log.info(\"Writing to temp table started\")\nself._writer.write_df_to_table(\n_df,\nf\"{table_name}_temp\",\nspark_conf=spark_conf,\noptions=options,\n)\n_log.info(\"Read from temp table started\")\n_df = self.spark.sql(f\"select * from {table_name}_temp\")\n_log.info(\"Read from temp table completed\")\nfunc_process = self._process.execute_dq_process(\n_context=self._context,\n_actions=self.actions,\n_writer=self._writer,\n_notification=self._notification,\nexpectations=expectations,\ntable_name=table_name,\n_input_count=_input_count,\nwrite_to_table=write_to_table,\nspark_conf=spark_conf,\noptions=options,\noptions_error_table=options_error_table,\n)\nif _agg_dq is True and _source_agg_dq is True:\n_log.info(\n\"started processing data quality rules for agg level expectations on soure dataframe\"\n)\nself._context.set_source_agg_dq_status(\"Failed\")\nself._context.set_source_agg_dq_start_time()\n# In this steps source agg data quality expectations runs on raw_data\n# returns:\n#        _source_dq_df: applied data quality dataframe,\n#        _dq_source_agg_results: source aggregation result in dictionary\n#        _: place holder for error data at row level\n#        status: status of the execution\n(\n_source_dq_df,\n_dq_source_agg_results,\n_,\nstatus,\n) = func_process(\n_df,\nself._context.get_agg_dq_rule_type_name,\nsource_agg_dq_flag=True,\n)\nself._context.set_source_agg_dq_result(\n_dq_source_agg_results\n)\nself._context.set_source_agg_dq_status(status)\nself._context.set_source_agg_dq_end_time()\n_log.info(\n\"ended processing data quality rules for agg level expectations on source dataframe\"\n)\nif _query_dq is True and _source_query_dq is True:\n_log.info(\n\"started processing data quality rules for query level expectations on soure dataframe\"\n)\nself._context.set_source_query_dq_status(\"Failed\")\nself._context.set_source_query_dq_start_time()\n# In this steps source query data quality expectations runs on raw_data\n# returns:\n#        _source_query_dq_df: applied data quality dataframe,\n#        _dq_source_query_results: source query dq results in dictionary\n#        _: place holder for error data at row level\n#        status: status of the execution\n(\n_source_query_dq_df,\n_dq_source_query_results,\n_,\nstatus,\n) = func_process(\n_df,\nself._context.get_query_dq_rule_type_name,\nsource_query_dq_flag=True,\n)\nself._context.set_source_query_dq_result(\n_dq_source_query_results\n)\nself._context.set_source_query_dq_status(status)\nself._context.set_source_query_dq_end_time()\n_log.info(\n\"ended processing data quality rules for query level expectations on source dataframe\"\n)\nif row_dq is True:\n_log.info(\n\"started processing data quality rules for row level expectations\"\n)\nself._context.set_row_dq_status(\"Failed\")\nself._context.set_row_dq_start_time()\n# In this steps row level data quality expectations runs on raw_data\n# returns:\n#        _row_dq_df: applied data quality dataframe at row level on raw dataframe,\n#        _: place holder for aggregation\n#        _error_count: number of error records\n#        status: status of the execution\n(_row_dq_df, _, _error_count, status) = func_process(\n_df,\nself._context.get_row_dq_rule_type_name,\nrow_dq_flag=True,\n)\nself._context.set_error_count(_error_count)\nif _target_table_view:\n_row_dq_df.createOrReplaceTempView(_target_table_view)\n_output_count = _row_dq_df.count()\nself._context.set_output_count(_output_count)\nself._context.set_row_dq_status(status)\nself._context.set_row_dq_end_time()\nif (\n_notification_on_error_drop_exceeds_threshold_breach\nis True\nand (100 - self._context.get_output_percentage)\n&gt;= _error_drop_threshold\n):\nself._notification.notify_on_exceeds_of_error_threshold()\n# raise SparkExpectationsErrorThresholdExceedsException(\n#     \"An error has taken place because\"\n#     \" the set limit for acceptable\"\n#     \" errors, known as the error\"\n#     \" threshold, has been surpassed\"\n# )\n_log.info(\n\"ended processing data quality rules for row level expectations\"\n)\nif row_dq is True and _agg_dq is True and _final_agg_dq is True:\n_log.info(\n\"started processing data quality rules for agg level expectations on final dataframe\"\n)\nself._context.set_final_agg_dq_status(\"Failed\")\nself._context.set_final_agg_dq_start_time()\n# In this steps final agg data quality expectations run on final dataframe\n# returns:\n#        _final_dq_df: applied data quality dataframe at row level on raw dataframe,\n#        _dq_final_agg_results: final agg dq result in dictionary\n#        _: number of error records\n#        status: status of the execution\n(\n_final_dq_df,\n_dq_final_agg_results,\n_,\nstatus,\n) = func_process(\n_row_dq_df,\nself._context.get_agg_dq_rule_type_name,\nfinal_agg_dq_flag=True,\nerror_count=_error_count,\noutput_count=_output_count,\n)\nself._context.set_final_agg_dq_result(_dq_final_agg_results)\nself._context.set_final_agg_dq_status(status)\nself._context.set_final_agg_dq_end_time()\n_log.info(\n\"ended processing data quality rules for agg level expectations on final dataframe\"\n)\nif (\nrow_dq is True\nand _query_dq is True\nand _final_query_dq is True\n):\n_log.info(\n\"started processing data quality rules for query level expectations on final dataframe\"\n)\nself._context.set_final_query_dq_status(\"Failed\")\nself._context.set_final_query_dq_start_time()\n# In this steps final query dq data quality expectations run on final dataframe\n# returns:\n#        _final_query_dq_df: applied data quality dataframe at row level on raw dataframe,\n#        _dq_final_query_results: final query dq result in dictionary\n#        _: number of error records\n#        status: status of the execution\nif _target_table_view and _row_dq_df:\n_row_dq_df.createOrReplaceTempView(_target_table_view)\nelse:\nraise SparkExpectationsMiscException(\n\"final table view name is not supplied to run query dq\"\n)\n(\n_final_query_dq_df,\n_dq_final_query_results,\n_,\nstatus,\n) = func_process(\n_row_dq_df,\nself._context.get_query_dq_rule_type_name,\nfinal_query_dq_flag=True,\nerror_count=_error_count,\noutput_count=_output_count,\n)\nself._context.set_final_query_dq_result(\n_dq_final_query_results\n)\nself._context.set_final_query_dq_status(status)\nself._context.set_final_query_dq_end_time()\n_log.info(\n\"ended processing data quality rules for query level expectations on final dataframe\"\n)\nelse:\nraise SparkExpectationsDataframeNotReturnedException(\n\"error occurred while processing spark \"\n\"expectations due to given dataframe is not type of dataframe\"\n)\nself.spark.catalog.clearCache()\nreturn _row_dq_df\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while processing spark expectations {e}\"\n)\nreturn wrapper\nreturn _except\n</code></pre>"},{"location":"api/kafka_sink_plugin/","title":"Kafka_Sink_plugin","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer-attributes","title":"Attributes","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer-classes","title":"Classes","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl","title":"<code>spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl</code>","text":"<p>             Bases: <code>SparkExpectationsSinkWriter</code></p> <p>class helps to write the stats data into the NSP</p>"},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl-functions","title":"Functions","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl.writer","title":"<code>writer(_write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]) -&gt; None</code>","text":"<p>The functions helps to write data into the kafka topic</p> <p>Parameters:</p> Name Type Description Default <code>_write_args</code> <code>Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]</code> required Source code in <code>spark_expectations/sinks/plugins/kafka_writer.py</code> <pre><code>@spark_expectations_writer_impl\ndef writer(\nself, _write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]\n) -&gt; None:\n\"\"\"\n    The functions helps to write data into the kafka topic\n    Args:\n        _write_args:\n    Returns:\n    \"\"\"\ntry:\n# kafka_options = {\n#     \"kafka.bootstrap.servers\": \"localhost:9092\",\n#     \"topic\": _write_args.get(\"nsp_topic_name\"),\n#     \"failOnDataLoss\": \"true\",\n# }\nif _write_args.pop(\"enable_se_streaming\"):\n_log.info(\"started write stats data into nsp stats topic\")\ndf: DataFrame = _write_args.get(\"stats_df\")\ndf.selectExpr(\"to_json(struct(*)) AS value\").write.format(\"kafka\").mode(\n\"append\"\n).options(**_write_args.get(\"kafka_write_options\")).save()\n_log.info(\"ended writing stats data into nsp stats topic\")\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while saving data into NSP {e}\"\n)\n</code></pre>"},{"location":"api/notifications_decorater/","title":"Notifications decorater","text":""},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify-classes","title":"Classes","text":""},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify","title":"<code>spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify</code>  <code>dataclass</code>","text":"<p>This class implements Notification</p>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify-attributes","title":"Attributes","text":""},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.product_id","title":"<code>product_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify-functions","title":"Functions","text":""},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.construct_message_for_each_rules","title":"<code>construct_message_for_each_rules(rule_name: str, failed_row_count: int, error_drop_percentage: float, set_error_drop_threshold: float, action: str) -&gt; str</code>","text":"<p>This function supports constructing the notification message when rule threshold exceeds certain threshold  Args:     rule_name: name of the dq rule     failed_row_count: number of failed of dq rule     error_drop_percentage: error drop percentage  Returns: str</p> Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def construct_message_for_each_rules(\nself,\nrule_name: str,\nfailed_row_count: int,\nerror_drop_percentage: float,\nset_error_drop_threshold: float,\naction: str,\n) -&gt; str:\n\"\"\"\n    This function supports constructing the notification message when rule threshold exceeds certain threshold\n     Args:\n        rule_name: name of the dq rule\n        failed_row_count: number of failed of dq rule\n        error_drop_percentage: error drop percentage\n     Returns: str\n    \"\"\"\n_notification_message = (\nf\"{rule_name} has been exceeded above the threshold \"\nf\"value({set_error_drop_threshold}%) for `row_data` quality validation\\n\"\nf\"product_id: {self.product_id}\\n\"\nf\"table_name: {self._context.get_table_name}\\n\"\nf\"run_id: {self._context.get_run_id}\\n\"\nf\"run_date: {self._context.get_run_date}\\n\"\nf\"input_count: {self._context.get_input_count}\\n\"\nf\"rule_name: {rule_name}\\n\"\nf\"action: {action}\\n\"\nf\"failed_row_count: {failed_row_count}\\n\"\nf\"error_drop_percentage: {error_drop_percentage}\\n\\n\\n\"\n)\nreturn _notification_message\n</code></pre>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.notify_on_completion","title":"<code>notify_on_completion() -&gt; None</code>","text":"<p>This function sends notification on completion of spark expectations project</p> Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def notify_on_completion(self) -&gt; None:\n\"\"\"\n    This function sends notification on completion of spark expectations project\n    Returns: None\n    \"\"\"\n_notification_message = (\n\"Spark expectations job has been completed  \\n\\n\"\nf\"product_id: {self.product_id}\\n\"\nf\"table_name: {self._context.get_table_name}\\n\"\nf\"run_id: {self._context.get_run_id}\\n\"\nf\"run_date: {self._context.get_run_date}\\n\"\nf\"input_count: {self._context.get_input_count}\\n\"\nf\"error_percentage: {self._context.get_error_percentage}\\n\"\nf\"output_percentage: {self._context.get_output_percentage}\\n\"\nf\"success_percentage: {self._context.get_success_percentage}\\n\"\nf\"status: source_agg_dq_status = {self._context.get_source_agg_dq_status}\\n\"\nf\"            source_query_dq_status = {self._context.get_source_query_dq_status}\\n\"\nf\"            row_dq_status = {self._context.get_row_dq_status}\\n\"\nf\"            final_agg_dq_status = {self._context.get_final_agg_dq_status}\\n\"\nf\"            final_query_dq_status = {self._context.get_final_query_dq_status}\\n\"\nf\"            run_status = {self._context.get_dq_run_status}\"\n)\n_notification_hook.send_notification(\n_context=self._context, _config_args={\"message\": _notification_message}\n)\n</code></pre>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.notify_on_exceeds_of_error_threshold","title":"<code>notify_on_exceeds_of_error_threshold() -&gt; None</code>","text":"<p>This function sends notification on completion of spark expectations project</p> Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def notify_on_exceeds_of_error_threshold(self) -&gt; None:\n\"\"\"\n    This function sends notification on completion of spark expectations project\n    Returns: None\n    \"\"\"\n_notification_message = (\nf\"Spark expectations - dropped error percentage has been exceeded above the threshold \"\nf\"value({self._context.get_error_drop_threshold}%) for `row_data` quality validation  \\n\\n\"\nf\"product_id: {self.product_id}\\n\"\nf\"table_name: {self._context.get_table_name}\\n\"\nf\"run_id: {self._context.get_run_id}\\n\"\nf\"run_date: {self._context.get_run_date}\\n\"\nf\"input_count: {self._context.get_input_count}\\n\"\nf\"error_percentage: {self._context.get_error_percentage}\\n\"\nf\"error_drop_percentage: {self._context.get_error_drop_percentage}\\n\"\nf\"output_percentage: {self._context.get_output_percentage}\\n\"\nf\"success_percentage: {self._context.get_success_percentage}\"\n# f\"status: source_agg_dq_status = {self._context.get_source_agg_dq_status}\\n\"\n# f\"            source_query_dq_status = {self._context.get_source_query_dq_status}\\n\"\n# f\"            row_dq_status = {self._context.get_row_dq_status}\\n\"\n# f\"            final_agg_dq_status = {self._context.get_final_agg_dq_status}\\n\"\n# f\"            final_query_dq_status = {self._context.get_final_query_dq_status}\\n\"\n# f\"            run_status = {self._context.get_dq_run_status}\"\n)\n_notification_hook.send_notification(\n_context=self._context, _config_args={\"message\": _notification_message}\n)\n</code></pre>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.notify_on_exceeds_of_error_threshold_each_rules","title":"<code>notify_on_exceeds_of_error_threshold_each_rules(message: str) -&gt; None</code>","text":"<p>This function sends notification when specific rule error drop percentage exceeds above threshold</p> <p>Parameters:</p> Name Type Description Default <code>rule_name</code> <p>name of the dq rule</p> required <code>failed_row_count</code> <p>number of failed of dq rule</p> required <code>error_drop_percentage</code> <p>error drop percentage</p> required Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def notify_on_exceeds_of_error_threshold_each_rules(\nself,\nmessage: str,\n) -&gt; None:\n\"\"\"\n    This function sends notification when specific rule error drop percentage exceeds above threshold\n    Args:\n        rule_name: name of the dq rule\n        failed_row_count: number of failed of dq rule\n        error_drop_percentage: error drop percentage\n    Returns: None\n    \"\"\"\n_notification_message = (\nf\"Spark expectations - The number of notifications for rules being followed has surpassed \"\nf\"the specified threshold \\n\\n\\n{message}\"\n)\n_notification_hook.send_notification(\n_context=self._context, _config_args={\"message\": _notification_message}\n)\n</code></pre>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.notify_on_failure","title":"<code>notify_on_failure(_error: str) -&gt; None</code>","text":"<p>This function sends notification on failure of spark expectations project</p> <p>Parameters:</p> Name Type Description Default <code>_error</code> <code>str</code> <p>message or exception  for the failure(str)</p> required Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def notify_on_failure(self, _error: str) -&gt; None:\n\"\"\"\n    This function sends notification on failure of spark expectations project\n    Args:\n        _error: message or exception  for the failure(str)\n    Returns: None\n    \"\"\"\n_notification_message = (\n\"Spark expectations job has been failed  \\n\\n\"\nf\"product_id: {self.product_id}\\n\"\nf\"table_name: {self._context.get_table_name}\\n\"\nf\"run_id: {self._context.get_run_id}\\n\"\nf\"run_date: {self._context.get_run_date}\\n\"\nf\"input_count: {self._context.get_input_count}\\n\"\nf\"error_percentage: {self._context.get_error_percentage}\\n\"\nf\"output_percentage: {self._context.get_output_percentage}\\n\"\nf\"status: source_agg_dq_status = {self._context.get_source_agg_dq_status}\\n\"\nf\"            source_query_dq_status = {self._context.get_source_query_dq_status}\\n\"\nf\"            row_dq_status = {self._context.get_row_dq_status}\\n\"\nf\"            final_agg_dq_status = {self._context.get_final_agg_dq_status}\\n\"\nf\"            final_query_dq_status = {self._context.get_final_query_dq_status}\\n\"\nf\"            run_status = {self._context.get_dq_run_status}\"\n)\n_notification_hook.send_notification(\n_context=self._context,\n_config_args={\n\"message\": _notification_message,\n},\n)\n</code></pre>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.notify_on_start","title":"<code>notify_on_start() -&gt; None</code>","text":"<p>This function sends notification on start of spark expectations project</p> Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def notify_on_start(self) -&gt; None:\n\"\"\"\n    This function sends notification on start of spark expectations project\n    Returns: None\n    \"\"\"\n_notification_message = (\n\"Spark expectations job has started \\n\\n\"\nf\"table_name: {self._context.get_table_name}\\n\"\nf\"run_id: {self._context.get_run_id}\\n\"\nf\"run_date: {self._context.get_run_date}\"\n)\n_notification_hook.send_notification(\n_context=self._context,\n_config_args={\n\"message\": _notification_message,\n},\n)\n</code></pre>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.notify_on_start_completion_failure","title":"<code>notify_on_start_completion_failure(_on_start: Any, _on_completion: Any, _on_failure: Any) -&gt; Any</code>","text":"<p>This function orchestrate notification</p> <p>Parameters:</p> Name Type Description Default <code>_on_start</code> <code>Any</code> <p>function to send notification on start of spark expectations</p> required <code>_on_completion</code> <code>Any</code> <p>function to send notification on completion of spark expectations</p> required <code>_on_failure</code> <code>Any</code> <p>function to send notification on failure</p> required Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def notify_on_start_completion_failure(\nself, _on_start: Any, _on_completion: Any, _on_failure: Any\n) -&gt; Any:\n\"\"\"\n    This function orchestrate notification\n    Args:\n        _on_start: function to send notification on start of spark expectations\n        _on_completion: function to send notification on completion of spark expectations\n        _on_failure: function to send notification on failure\n    Returns: decorated notification function\n    \"\"\"\ndef decorator(func: Any) -&gt; Any:\ndef wrapper(*args: List, **kwargs: Dict) -&gt; DataFrame:\nif self._context.get_notification_on_start is True:\n_on_start()\ntry:\n# self._context.set_dq_start_time()\nresult = func(*args, **kwargs)\nif self._context.get_notification_on_completion is True:\n_on_completion()\n# self._context.set_dq_end_time()\nexcept Exception as e:\n# self._context.set_dq_run_status(\"Failed\")\nif self._context.get_notification_on_fail is True:\n_on_failure(e)\n# self._context.set_dq_end_time()\nraise SparkExpectationsMiscException(e)\nreturn result\nreturn wrapper\nreturn decorator\n</code></pre>"},{"location":"api/notifications_decorater/#spark_expectations.notifications.push.spark_expectations_notify.SparkExpectationsNotify.notify_rules_exceeds_threshold","title":"<code>notify_rules_exceeds_threshold(rules: dict) -&gt; None</code>","text":"<p>This functions identifies error drop percentage for rules which exceeds above set threshold</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>dict</code> <p>lsit of rules which set to do data quality checks</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/notifications/push/spark_expectations_notify.py</code> <pre><code>def notify_rules_exceeds_threshold(self, rules: dict) -&gt; None:\n\"\"\"\n    This functions identifies error drop percentage for rules which exceeds above set threshold\n    Args:\n        rules: lsit of rules which set to do data quality checks\n    Returns:\n        None\n    \"\"\"\ntry:\nrules_failed_row_count: Dict[str, int] = {}\nnotification_body = \"\"\nif self._context.get_summarised_row_dq_res is None:\nreturn None\nrules_failed_row_count = {\nitr[\"rule\"]: int(itr[\"failed_row_count\"])\nfor itr in self._context.get_summarised_row_dq_res\n}\nfor rule in rules[f\"{self._context.get_row_dq_rule_type_name}_rules\"]:\nif not rule[\"enable_error_drop_alert\"]:\ncontinue\nrule_name = rule[\"rule\"]\nrule_action = rule[\"action_if_failed\"]\nfailed_row_count = int(rules_failed_row_count[rule_name])\nif failed_row_count is not None and failed_row_count &gt; 0:\nset_error_drop_threshold = int(rule[\"error_drop_threshold\"])\nerror_drop_percentage = round(\n(failed_row_count / self._context.get_input_count) * 100, 2\n)\nif error_drop_percentage &gt;= set_error_drop_threshold:\nnotification_body = (\nnotification_body\n+ self.construct_message_for_each_rules(\nrule_name=rule_name,\nfailed_row_count=failed_row_count,\nerror_drop_percentage=error_drop_percentage,\nset_error_drop_threshold=set_error_drop_threshold,\naction=rule_action,\n)\n)\nif notification_body != \"\":\nself.notify_on_exceeds_of_error_threshold_each_rules(\nnotification_body\n)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"An error occurred while sending notification when the error threshold is breached: {e}\"\n)\n</code></pre>"},{"location":"api/notifications_init/","title":"Init","text":""},{"location":"api/notifications_init/#spark_expectations.notifications-attributes","title":"Attributes","text":""},{"location":"api/notifications_init/#spark_expectations.notifications-classes","title":"Classes","text":""},{"location":"api/notifications_init/#spark_expectations.notifications-functions","title":"Functions","text":""},{"location":"api/notifications_init/#spark_expectations.notifications.get_notifications_hook","title":"<code>spark_expectations.notifications.get_notifications_hook() -&gt; pluggy.PluginManager</code>  <code>cached</code>","text":"<p>function provides pluggy hook manger to send email and slack notification</p> <p>Returns:</p> Name Type Description <code>PluginManager</code> <code>pluggy.PluginManager</code> <p>pluggy Manager object</p> Source code in <code>spark_expectations/notifications/__init__.py</code> <pre><code>@functools.lru_cache\ndef get_notifications_hook() -&gt; pluggy.PluginManager:\n\"\"\"\n    function provides pluggy hook manger to send email and slack notification\n    Returns:\n        PluginManager: pluggy Manager object\n    \"\"\"\npm = pluggy.PluginManager(SPARK_EXPECTATIONS_NOTIFICATION_PLUGIN)\npm.add_hookspecs(SparkExpectationsNotification)\npm.register(\nSparkExpectationsEmailPluginImpl(), \"spark_expectations_email_notification\"\n)\npm.register(\nSparkExpectationsSlackPluginImpl(), \"spark_expectations_slack_notification\"\n)\nfor name, plugin_instance in pm.list_name_plugin():\n_log.info(\n\"Loaded plugin with name: %s and class: %s\",\nname,\nplugin_instance.__class__.__name__,\n)\nreturn pm\n</code></pre>"},{"location":"api/reader/","title":"Reader","text":""},{"location":"api/reader/#spark_expectations.utils.reader-classes","title":"Classes","text":""},{"location":"api/reader/#spark_expectations.utils.reader.SparkExpectationsReader","title":"<code>spark_expectations.utils.reader.SparkExpectationsReader</code>  <code>dataclass</code>","text":"<p>This class implements/supports reading data from source system</p>"},{"location":"api/reader/#spark_expectations.utils.reader.SparkExpectationsReader-attributes","title":"Attributes","text":""},{"location":"api/reader/#spark_expectations.utils.reader.SparkExpectationsReader.product_id","title":"<code>product_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/reader/#spark_expectations.utils.reader.SparkExpectationsReader-functions","title":"Functions","text":""},{"location":"api/reader/#spark_expectations.utils.reader.SparkExpectationsReader.get_rules_dlt","title":"<code>get_rules_dlt(product_rules_table: str, table_name: str, action: Union[list, str], tag: Optional[str] = None) -&gt; dict</code>","text":"<p>This function supports creating a dict of expectations that is acceptable by DLT</p> <p>Parameters:</p> Name Type Description Default <code>product_rules_table</code> <code>str</code> <p>Provide the full table name, which has your data quality rules</p> required <code>table_name</code> <code>str</code> <p>Provide the full table name for which the data quality rules are being run</p> required <code>action</code> <code>Union[list, str]</code> <p>Provide the action which you want to filter from rules table. Value should only from one of these          - \"fail\" or \"drop\" or \"ignore\" or provide the needed in a list [\"fail\", \"drop\", \"ignore\"]</p> required <code>tag</code> <code>Optional[str]</code> <p>Provide the KPI for which you are running the data quality rule</p> <code>None</code> <code>Returns</code> <p>dict: returns a dict with key as 'rule' and 'expectation' as value</p> required Source code in <code>spark_expectations/utils/reader.py</code> <pre><code>def get_rules_dlt(\nself,\nproduct_rules_table: str,\ntable_name: str,\naction: Union[list, str],\ntag: Optional[str] = None,\n) -&gt; dict:\n\"\"\"\n    This function supports creating a dict of expectations that is acceptable by DLT\n    Args:\n         product_rules_table: Provide the full table name, which has your data quality rules\n         table_name: Provide the full table name for which the data quality rules are being run\n         action: Provide the action which you want to filter from rules table. Value should only from one of these\n                       - \"fail\" or \"drop\" or \"ignore\" or provide the needed in a list [\"fail\", \"drop\", \"ignore\"]\n         tag: Provide the KPI for which you are running the data quality rule\n         Returns:\n               dict: returns a dict with key as 'rule' and 'expectation' as value\n    \"\"\"\ntry:\n_actions: List[str] = [].append(action) if isinstance(action, str) else action  # type: ignore\n_expectations: dict = {}\n_rules_df: DataFrame = self.spark.sql(\nf\"\"\"\n                                   select rule, tag, expectation from {product_rules_table}                                    where product_id='{self.product_id}' and table_name='{table_name}' and \n                                   action_if_failed in ('{\"', '\".join(_actions)}')\n                                   \"\"\"\n)\nif tag:\nfor row in _rules_df.filter(col(\"tag\") == tag).collect():\n_expectations[row[\"rule\"]] = row[\"expectation\"]\nelse:\nfor row in _rules_df.collect():\n_expectations[row[\"rule\"]] = row[\"expectation\"]\nreturn _expectations\nexcept Exception as e:\nraise SparkExpectationsUserInputOrConfigInvalidException(\nf\"error occurred while reading or getting rules from the rules table {e}\"\n)\n</code></pre>"},{"location":"api/reader/#spark_expectations.utils.reader.SparkExpectationsReader.get_rules_from_table","title":"<code>get_rules_from_table(product_rules_table: str, dq_stats_table_name: str, target_table_name: str, actions_if_failed: Optional[List[str]] = None) -&gt; dict</code>","text":"<p>This function fetches the data quality rules from the table and return it as a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>product_rules_table</code> <code>str</code> <p>Provide the full table name, which has your data quality rules</p> required <code>table_name</code> <p>Provide the full table name for which the data quality rules are being run</p> required <code>dq_stats_table_name</code> <code>str</code> <p>Provide the table name, to which Data Quality Stats have to be written to</p> required <code>actions_if_failed</code> <code>Optional[List[str]]</code> <p>Provide the list of actions in [\"fail\", \"drop\", 'ignore'], which need to be applied on a particular row if a rule failed</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dict with table and rules as keys</p> Source code in <code>spark_expectations/utils/reader.py</code> <pre><code>def get_rules_from_table(\nself,\nproduct_rules_table: str,\ndq_stats_table_name: str,\ntarget_table_name: str,\nactions_if_failed: Optional[List[str]] = None,\n) -&gt; dict:\n\"\"\"\n    This function fetches the data quality rules from the table and return it as a dictionary\n    Args:\n        product_rules_table: Provide the full table name, which has your data quality rules\n        table_name: Provide the full table name for which the data quality rules are being run\n        dq_stats_table_name: Provide the table name, to which Data Quality Stats have to be written to\n        actions_if_failed: Provide the list of actions in [\"fail\", \"drop\", 'ignore'], which need to be applied on a\n            particular row if a rule failed\n    Returns:\n        dict: The dict with table and rules as keys\n    \"\"\"\ntry:\nself._context.set_dq_stats_table_name(dq_stats_table_name)\nself._context.set_final_table_name(target_table_name)\nself._context.set_error_table_name(f\"{target_table_name}_error\")\nself._context.set_table_name(target_table_name)\nself._context.set_env(os.environ.get(\"SPARKEXPECTATIONS_ENV\"))\nself._context.reset_num_agg_dq_rules()\nself._context.reset_num_dq_rules()\nself._context.reset_num_row_dq_rules()\nself._context.reset_num_query_dq_rules()\n_actions_if_failed: List[str] = actions_if_failed or [\n\"fail\",\n\"drop\",\n\"ignore\",\n]\n_rules_df: DataFrame = self.spark.sql(\nf\"\"\"\n                    select * from {product_rules_table} where product_id='{self.product_id}' \n                    and table_name='{target_table_name}'\n                    and action_if_failed in ('{\"', '\".join(_actions_if_failed)}') and is_active=true\n                    \"\"\"\n)\nself._context.print_dataframe_with_debugger(_rules_df)\n_expectations: dict = {}\nfor row in _rules_df.collect():\ncolumn_map = {\n\"product_id\": row[\"product_id\"],\n\"table_name\": row[\"table_name\"],\n\"rule_type\": row[\"rule_type\"],\n\"rule\": row[\"rule\"],\n\"column_name\": row[\"column_name\"],\n\"expectation\": row[\"expectation\"],\n\"action_if_failed\": row[\"action_if_failed\"],\n\"enable_for_source_dq_validation\": row[\n\"enable_for_source_dq_validation\"\n],\n\"enable_for_target_dq_validation\": row[\n\"enable_for_target_dq_validation\"\n],\n\"tag\": row[\"tag\"],\n\"description\": row[\"description\"],\n\"enable_error_drop_alert\": row[\"enable_error_drop_alert\"],\n\"error_drop_threshold\": row[\"error_drop_threshold\"],\n}\nif f\"{row['rule_type']}_rules\" in _expectations:\n_expectations[f\"{row['rule_type']}_rules\"].append(column_map)\nelse:\n_expectations[f\"{row['rule_type']}_rules\"] = [column_map]\n# count the rules enabled for the current run\nif row[\"rule_type\"] == self._context.get_row_dq_rule_type_name:\nself._context.set_num_row_dq_rules()\nelif row[\"rule_type\"] == self._context.get_agg_dq_rule_type_name:\nself._context.set_num_agg_dq_rules(\nrow[\"enable_for_source_dq_validation\"],\nrow[\"enable_for_target_dq_validation\"],\n)\nelif row[\"rule_type\"] == self._context.get_query_dq_rule_type_name:\nself._context.set_num_query_dq_rules(\nrow[\"enable_for_source_dq_validation\"],\nrow[\"enable_for_target_dq_validation\"],\n)\n_expectations[\"target_table_name\"] = target_table_name\nreturn _expectations\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while retrieving rules list from the table {e}\"\n)\n</code></pre>"},{"location":"api/reader/#spark_expectations.utils.reader.SparkExpectationsReader.set_notification_param","title":"<code>set_notification_param(notification: Optional[Dict[str, Union[int, str, bool]]] = None) -&gt; None</code>","text":"<p>This function supports to read notifications configurations</p> Source code in <code>spark_expectations/utils/reader.py</code> <pre><code>def set_notification_param(\nself, notification: Optional[Dict[str, Union[int, str, bool]]] = None\n) -&gt; None:\n\"\"\"\n    This function supports to read notifications configurations\n    Returns: None\n    \"\"\"\ntry:\n_default_spark_conf: Dict[str, Union[str, int, bool]] = {\nuser_config.se_notifications_enable_email: False,\nuser_config.se_notifications_email_smtp_host: \"\",\nuser_config.se_notifications_email_smtp_port: 25,\nuser_config.se_notifications_email_from: \"\",\nuser_config.se_notifications_email_to_other_mail_id: \"\",\nuser_config.se_notifications_email_subject: \"spark-expectations-testing\",\nuser_config.se_notifications_enable_slack: False,\nuser_config.se_notifications_slack_webhook_url: \"\",\n}\n_notification_dict: Dict[str, Union[str, int, bool]] = (\n{**_default_spark_conf, **notification}\nif notification\nelse _default_spark_conf\n)\nif (\n_notification_dict.get(user_config.se_notifications_enable_email)\nis True\n):\nif (\n_notification_dict[user_config.se_notifications_email_smtp_host]\nand _notification_dict[user_config.se_notifications_email_from]\nand _notification_dict[\nuser_config.se_notifications_email_to_other_mail_id\n]\nand _notification_dict[user_config.se_notifications_email_subject]\n):\nself._context.set_enable_mail(True)\nself._context.set_to_mail(\nstr(\n_notification_dict[\nuser_config.se_notifications_email_to_other_mail_id\n]\n)\n)\nself._context.set_mail_subject(\nstr(\n_notification_dict[\nuser_config.se_notifications_email_subject\n]\n)\n)\nself._context.set_mail_smtp_server(\nstr(\n_notification_dict[\nuser_config.se_notifications_email_smtp_host\n]\n)\n)\nself._context.set_mail_smtp_port(\nint(\n_notification_dict[\nuser_config.se_notifications_email_smtp_port\n]\n)\n)\nself._context.set_mail_from(\nstr(_notification_dict[user_config.se_notifications_email_from])\n)\nelse:\nraise SparkExpectationsMiscException(\n\"All params/variables required for email notification is not configured or supplied\"\n)\nif _notification_dict[user_config.se_notifications_enable_slack] is True:\nif _notification_dict[user_config.se_notifications_slack_webhook_url]:\nself._context.set_enable_slack(True)\nself._context.set_slack_webhook_url(\nstr(\n_notification_dict[\nuser_config.se_notifications_slack_webhook_url\n]\n)\n)\nelse:\nraise SparkExpectationsMiscException(\n\"All params/variables required for slack notification is not configured or supplied\"\n)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while reading notification configurations {e}\"\n)\n</code></pre>"},{"location":"api/reader/#spark_expectations.utils.reader-functions","title":"Functions","text":""},{"location":"api/regulate_flow/","title":"Regulate_flow","text":""},{"location":"api/regulate_flow/#spark_expectations.utils.regulate_flow-classes","title":"Classes","text":""},{"location":"api/regulate_flow/#spark_expectations.utils.regulate_flow.SparkExpectationsRegulateFlow","title":"<code>spark_expectations.utils.regulate_flow.SparkExpectationsRegulateFlow</code>  <code>dataclass</code>","text":"<p>This is helper class and implements/supports running data quality flow</p>"},{"location":"api/regulate_flow/#spark_expectations.utils.regulate_flow.SparkExpectationsRegulateFlow-attributes","title":"Attributes","text":""},{"location":"api/regulate_flow/#spark_expectations.utils.regulate_flow.SparkExpectationsRegulateFlow.product_id","title":"<code>product_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/regulate_flow/#spark_expectations.utils.regulate_flow.SparkExpectationsRegulateFlow-functions","title":"Functions","text":""},{"location":"api/regulate_flow/#spark_expectations.utils.regulate_flow.SparkExpectationsRegulateFlow.execute_dq_process","title":"<code>execute_dq_process(_context: SparkExpectationsContext, _actions: SparkExpectationsActions, _writer: SparkExpectationsWriter, _notification: SparkExpectationsNotify, expectations: Dict[str, List[dict]], table_name: str, _input_count: int = 0, write_to_table: bool = False, spark_conf: Optional[Dict[str, Any]] = None, options: Optional[Dict[str, str]] = None, options_error_table: Optional[Dict[str, str]] = None) -&gt; Any</code>","text":"<p>This functions takes required static variable and returns the function</p> <p>Parameters:</p> Name Type Description Default <code>_actions</code> <code>SparkExpectationsActions</code> <p>SparkExpectationsActions class object</p> required <code>_writer</code> <code>SparkExpectationsWriter</code> <p>SparkExpectationsWriter class object</p> required <code>expectations</code> <code>Dict[str, List[dict]]</code> <p>expectations dictionary which contains rules</p> required <code>table_name</code> <code>str</code> <p>name of the table</p> required <code>_input_count</code> <code>int</code> <p>number of records in the source dataframe</p> <code>0</code> <code>write_to_table</code> <code>bool</code> <p>Mark it as \"True\" if the dataframe need to be written as table</p> <code>False</code> <code>spark_conf</code> <code>Optional[Dict[str, Any]]</code> <p>spark configurations(which is optional)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, str]]</code> <p>spark configurations to write data into the final table(which is optional)</p> <code>None</code> <code>options_error_table</code> <code>Optional[Dict[str, str]]</code> <p>spark configurations to write data into the error table(which is optional)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>returns function</p> Source code in <code>spark_expectations/utils/regulate_flow.py</code> <pre><code>def execute_dq_process(\nself,\n_context: SparkExpectationsContext,\n_actions: SparkExpectationsActions,\n_writer: SparkExpectationsWriter,\n_notification: SparkExpectationsNotify,\nexpectations: Dict[str, List[dict]],\ntable_name: str,\n_input_count: int = 0,\nwrite_to_table: bool = False,\nspark_conf: Optional[Dict[str, Any]] = None,\noptions: Optional[Dict[str, str]] = None,\noptions_error_table: Optional[Dict[str, str]] = None,\n) -&gt; Any:\n\"\"\"\n    This functions takes required static variable and returns the function\n    Args:\n        _actions: SparkExpectationsActions class object\n        _writer: SparkExpectationsWriter class object\n        expectations: expectations dictionary which contains rules\n        table_name: name of the table\n        _input_count: number of records in the source dataframe\n        write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n        spark_conf: spark configurations(which is optional)\n        options: spark configurations to write data into the final table(which is optional)\n        options_error_table: spark configurations to write data into the error table(which is optional)\n    Returns:\n           Any: returns function\n    \"\"\"\ndef func_process(\ndf: DataFrame,\n_rule_type: str,\nrow_dq_flag: bool = False,\nsource_agg_dq_flag: bool = False,\nfinal_agg_dq_flag: bool = False,\nsource_query_dq_flag: bool = False,\nfinal_query_dq_flag: bool = False,\nerror_count: int = 0,\noutput_count: int = 0,\n) -&gt; Tuple[DataFrame, Optional[List[Dict[str, str]]], int, str]:\n\"\"\"\n        This inner function helps to process data quality rules based on different rules types\n        Args:\n            df: dataframe for data quality\n            _rule_type: type of the rule\n            row_dq_flag: default false, Mark True tp process row level data quality\n            source_agg_dq_flag: default false, Mark True tp process agg level data quality on source dataframe\n            final_agg_dq_flag: default false, Mark True tp process agg level data quality on final dataframe\n            source_query_dq_flag: default false, Mark True tp process query level data quality on source dataframe\n            final_query_dq_flag: default false, Mark True tp process query level data quality on final dataframe\n            error_count: number of records error records default zero)\n            output_count: number of output records from expectations (default zero)\n        Returns:\n               Tuples with data frame which contains dq result, agg result in list, error count and\n               status of the flow\n        \"\"\"\ntry:\n_df_dq: Optional[DataFrame] = None\n_error_df: Optional[DataFrame] = None\n_error_count: int = error_count\n_running_rule_type_name = (\n_context.get_row_dq_rule_type_name\nif row_dq_flag\nelse (\n_context.get_agg_dq_rule_type_name\nif (source_agg_dq_flag or final_agg_dq_flag)\nelse _context.get_query_dq_rule_type_name\n)\n)\n_log.info(\n\"The data quality dataframe is getting created for expectations\"\n)\n_df_dq = _actions.run_dq_rules(\n_context,\ndf,\nexpectations,\n_running_rule_type_name,\n_source_dq_enabled=(\nsource_query_dq_flag is True or source_agg_dq_flag is True\n),\n_target_dq_enabled=(\nfinal_query_dq_flag is True or final_agg_dq_flag is True\n),\n)\n_log.info(\"The data quality dataframe is created for expectations\")\n_context.print_dataframe_with_debugger(_df_dq)\nagg_dq_res = (\n_actions.create_agg_dq_results(\n_context, _df_dq, _running_rule_type_name\n)\nif row_dq_flag is False\nelse None\n)\nif row_dq_flag:\n_log.info(\"Writing error records into the table started\")\n_error_count, _error_df = _writer.write_error_records_final(\n_df_dq,\nf\"{table_name}_error\",\n_context.get_row_dq_rule_type_name,\nspark_conf,\noptions_error_table,\n)\nif _context.get_summarised_row_dq_res:\n_notification.notify_rules_exceeds_threshold(expectations)\n_writer.generate_rules_exceeds_threshold(expectations)\n_context.print_dataframe_with_debugger(_error_df)\n# set the error count\n_context.set_error_count(_error_count)\n# set agg result\nif source_agg_dq_flag:\n_context.set_source_agg_dq_result(agg_dq_res)\nelif final_agg_dq_flag:\n_context.set_final_agg_dq_result(agg_dq_res)\nelif source_query_dq_flag:\n_context.set_source_query_dq_result(agg_dq_res)\nelif final_query_dq_flag:\n_context.set_final_query_dq_result(agg_dq_res)\ndf = _actions.action_on_rules(\n_context,\n_error_df if row_dq_flag else _df_dq,\ntable_name,\n_input_count,\n_error_count=_error_count,\n_output_count=output_count,\n_rule_type=_running_rule_type_name,\n_row_dq_flag=row_dq_flag,\n_source_agg_dq_flag=source_agg_dq_flag,\n_final_agg_dq_flag=final_agg_dq_flag,\n_source_query_dq_flag=source_query_dq_flag,\n_final_query_dq_flag=final_query_dq_flag,\n)\n_context.print_dataframe_with_debugger(df)\nif row_dq_flag and write_to_table:\n_log.info(\"Writing into the final table started\")\n_writer.write_df_to_table(\ndf,\nf\"{table_name}\",\nspark_conf=spark_conf,\noptions=options,\n)\n_log.info(\"Writing into the final table ended\")\nreturn df, agg_dq_res, _error_count, \"Passed\"\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while executing func_process {e}\"\n)\nreturn func_process\n</code></pre>"},{"location":"api/sample_dq/","title":"Sample_DQ","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq-attributes","title":"Attributes","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq.current_dir","title":"<code>spark_expectations.examples.sample_dq.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq.global_spark_Conf","title":"<code>spark_expectations.examples.sample_dq.global_spark_Conf = {user_config.se_notifications_enable_email: False, user_config.se_notifications_email_smtp_host: 'mailhost.com', user_config.se_notifications_email_smtp_port: 25, user_config.se_notifications_email_from: '', user_config.se_notifications_email_to_other_mail_id: '', user_config.se_notifications_email_subject: 'spark expectations - data quality - notifications', user_config.se_notifications_enable_slack: False, user_config.se_notifications_slack_webhook_url: '', user_config.se_notifications_on_start: True, user_config.se_notifications_on_completion: True, user_config.se_notifications_on_fail: True, user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True, user_config.se_notifications_on_error_drop_threshold: 15}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq.se","title":"<code>spark_expectations.examples.sample_dq.se: SparkExpectations = SparkExpectations(product_id='your_product', debugger=False)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq.spark","title":"<code>spark_expectations.examples.sample_dq.spark = get_spark_session()</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq-classes","title":"Classes","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq-functions","title":"Functions","text":""},{"location":"api/sample_dq/#spark_expectations.examples.sample_dq.build_new","title":"<code>spark_expectations.examples.sample_dq.build_new() -&gt; DataFrame</code>","text":"Source code in <code>spark_expectations/examples/sample_dq.py</code> <pre><code>@se.with_expectations(\nse.reader.get_rules_from_table(\nproduct_rules_table=\"dq_spark_local.dq_rules\",\ntarget_table_name=\"dq_spark_local.customer_order\",\ndq_stats_table_name=\"dq_spark_local.dq_stats\",\n),\nwrite_to_table=True,\nrow_dq=True,\nagg_dq={\nuser_config.se_agg_dq: True,\nuser_config.se_source_agg_dq: True,\nuser_config.se_final_agg_dq: True,\n},\nquery_dq={\nuser_config.se_query_dq: True,\nuser_config.se_source_query_dq: True,\nuser_config.se_final_query_dq: True,\nuser_config.se_target_table_view: \"order\",\n},\nspark_conf=global_spark_Conf,\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")\nreturn _df_order\n</code></pre>"},{"location":"api/sinks_decorater/","title":"Sinks decorater","text":""},{"location":"api/sinks_decorater/#spark_expectations.sinks.utils.collect_statistics-classes","title":"Classes","text":""},{"location":"api/sinks_decorater/#spark_expectations.sinks.utils.collect_statistics.SparkExpectationsCollectStatistics","title":"<code>spark_expectations.sinks.utils.collect_statistics.SparkExpectationsCollectStatistics</code>  <code>dataclass</code>","text":"<p>This class implements logging statistics on success and failure</p>"},{"location":"api/sinks_decorater/#spark_expectations.sinks.utils.collect_statistics.SparkExpectationsCollectStatistics-attributes","title":"Attributes","text":""},{"location":"api/sinks_decorater/#spark_expectations.sinks.utils.collect_statistics.SparkExpectationsCollectStatistics.product_id","title":"<code>product_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/sinks_decorater/#spark_expectations.sinks.utils.collect_statistics.SparkExpectationsCollectStatistics-functions","title":"Functions","text":""},{"location":"api/sinks_decorater/#spark_expectations.sinks.utils.collect_statistics.SparkExpectationsCollectStatistics.collect_stats_on_success_failure","title":"<code>collect_stats_on_success_failure() -&gt; Any</code>","text":"<p>The function implements decorator to log statistics on success and failure</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>function</p> Source code in <code>spark_expectations/sinks/utils/collect_statistics.py</code> <pre><code>def collect_stats_on_success_failure(self) -&gt; Any:\n\"\"\"\n    The function implements decorator to log statistics on success and failure\n    Returns:\n        Any: function\n    \"\"\"\ndef decorator(func: Any) -&gt; Any:\ndef wrapper(*args: List, **kwargs: Dict) -&gt; DataFrame:\ntry:\nself._context.set_dq_start_time()\nresult = func(*args, **kwargs)\nself._context.set_dq_run_status(\"Passed\")\nself._context.set_dq_end_time()\nself._writer.write_error_stats()\nexcept Exception as e:\nself._context.set_dq_run_status(\"Failed\")\nself._context.set_end_time_when_dq_job_fails()\nself._context.set_dq_end_time()\nself._writer.write_error_stats()\nraise SparkExpectationsMiscException(e)\nreturn result\nreturn wrapper\nreturn decorator\n</code></pre>"},{"location":"api/sinks_init/","title":"Init","text":""},{"location":"api/sinks_init/#spark_expectations.sinks-attributes","title":"Attributes","text":""},{"location":"api/sinks_init/#spark_expectations.sinks-classes","title":"Classes","text":""},{"location":"api/sinks_init/#spark_expectations.sinks-functions","title":"Functions","text":""},{"location":"api/sinks_init/#spark_expectations.sinks.get_sink_hook","title":"<code>spark_expectations.sinks.get_sink_hook() -&gt; pluggy.PluginManager</code>  <code>cached</code>","text":"<p>function provides pluggy hook manager to write data into delta and nsp</p> <p>Returns:</p> Name Type Description <code>PluginManager</code> <code>pluggy.PluginManager</code> <p>pluggy Manager object</p> Source code in <code>spark_expectations/sinks/__init__.py</code> <pre><code>@functools.lru_cache\ndef get_sink_hook() -&gt; pluggy.PluginManager:\n\"\"\"\n    function provides pluggy hook manager to write data into delta and nsp\n    Returns:\n        PluginManager: pluggy Manager object\n    \"\"\"\npm = pluggy.PluginManager(SPARK_EXPECTATIONS_WRITER_PLUGIN)\npm.add_hookspecs(SparkExpectationsSinkWriter)\npm.register(\nSparkExpectationsDeltaWritePluginImpl(), \"spark_expectations_delta_write\"\n)\npm.register(\nSparkExpectationsKafkaWritePluginImpl(), \"spark_expectations_kafka_write\"\n)\nfor name, plugin_instance in pm.list_name_plugin():\n_log.info(\n\"Loaded plugin with name: %s and class: %s\",\nname,\nplugin_instance.__class__.__name__,\n)\nreturn pm\n</code></pre>"},{"location":"api/slack_plugin/","title":"Slack_Notification_plugin","text":""},{"location":"api/slack_plugin/#spark_expectations.notifications.plugins.email-attributes","title":"Attributes","text":""},{"location":"api/slack_plugin/#spark_expectations.notifications.plugins.email-classes","title":"Classes","text":""},{"location":"api/slack_plugin/#spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl","title":"<code>spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl</code>","text":"<p>             Bases: <code>SparkExpectationsNotification</code></p> <p>This class implements/supports functionality to send email</p>"},{"location":"api/slack_plugin/#spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl-functions","title":"Functions","text":""},{"location":"api/slack_plugin/#spark_expectations.notifications.plugins.email.SparkExpectationsEmailPluginImpl.send_notification","title":"<code>send_notification(_context: SparkExpectationsContext, _config_args: Dict[Union[str], Union[str, bool]]) -&gt; None</code>","text":"<p>function to send email notification for requested mail id's</p> <p>Parameters:</p> Name Type Description Default <code>_context</code> <code>SparkExpectationsContext</code> <p>object of SparkExpectationsContext</p> required <code>_config_args</code> <code>Dict[Union[str], Union[str, bool]]</code> <p>dict(which consist to: receiver mail(str), subject: subject of           the mail(str) and body: body of the mail(str)</p> required Source code in <code>spark_expectations/notifications/plugins/email.py</code> <pre><code>@spark_expectations_notification_impl\ndef send_notification(\nself,\n_context: SparkExpectationsContext,\n_config_args: Dict[Union[str], Union[str, bool]],\n) -&gt; None:\n\"\"\"\n    function to send email notification for requested mail id's\n    Args:\n        _context: object of SparkExpectationsContext\n        _config_args: dict(which consist to: receiver mail(str), subject: subject of\n                      the mail(str) and body: body of the mail(str)\n    Returns:\n    \"\"\"\ntry:\nif _context.get_enable_mail is True:\nmsg = MIMEMultipart()\nmsg[\"From\"] = _context.get_mail_from\nmsg[\"To\"] = _context.get_to_mail\nmsg[\"Subject\"] = _context.get_mail_subject\n# body = _config_args.get('mail_body')\nmail_content = f\"\"\"{_config_args.get(\"message\")}\"\"\"\nmsg.attach(MIMEText(mail_content, \"plain\"))\n# mailhost.com\nserver = smtplib.SMTP(\n_context.get_mail_smtp_server, _context.get_mail_smtp_port\n)\nserver.starttls()\ntext = msg.as_string()\nserver.sendmail(_context.get_mail_from, _context.get_to_mail, text)\nserver.quit()\n_log.info(\"email send successfully\")\nexcept Exception as e:\nraise SparkExpectationsEmailException(\nf\"error occurred while sending email notification from spark expectations project {e}\"\n)\n</code></pre>"},{"location":"api/udf/","title":"Udf","text":""},{"location":"api/udf/#spark_expectations.utils.udf-functions","title":"Functions","text":""},{"location":"api/udf/#spark_expectations.utils.udf.get_actions_list","title":"<code>spark_expectations.utils.udf.get_actions_list(column: Column) -&gt; List[str]</code>","text":"<p>This Spark UDF takes column of type array(map(str,str)) and creates list by picking action_if_failed from dict</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>Provide a column of type array(map(str,str))</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>returns list of action_if_failed from the set expectations rules</p> Source code in <code>spark_expectations/utils/udf.py</code> <pre><code>@udf(returnType=ArrayType(StringType()))\ndef get_actions_list(column: Column) -&gt; List[str]:\n\"\"\"\n    This Spark UDF takes column of type array(map(str,str)) and creates list by picking action_if_failed from dict\n    Args:\n        column: Provide a column of type array(map(str,str))\n    Returns:\n           list: returns list of action_if_failed from the set expectations rules\n    \"\"\"\naction_failed = [itr.get(\"action_if_failed\") for itr in column]  # pragma: no cover\nreturn action_failed if len(action_failed) &gt; 0 else [\"ignore\"]  # pragma: no cover\n</code></pre>"},{"location":"api/udf/#spark_expectations.utils.udf.remove_empty_maps","title":"<code>spark_expectations.utils.udf.remove_empty_maps(column: Column) -&gt; List[Union[Dict[str, str], None]]</code>","text":"<p>This Spark UDF takes a column of type array(map(str,str)) and removes empty maps from it</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>Provide a column of type array(map(str,str))</p> required <code>Returns</code> <pre><code>list: Returns a list which is not having empty maps\n</code></pre> required Source code in <code>spark_expectations/utils/udf.py</code> <pre><code>@udf(returnType=ArrayType(MapType(StringType(), StringType())))\ndef remove_empty_maps(column: Column) -&gt; List[Union[Dict[str, str], None]]:\n\"\"\"\n    This Spark UDF takes a column of type array(map(str,str)) and removes empty maps from it\n    Args:\n        column: Provide a column of type array(map(str,str))\n        Returns:\n                list: Returns a list which is not having empty maps\n    \"\"\"\n# The below line is already tested in test_udf.py but not shown in coverage. So ignoring it for now!\nreturn [\nx for x in column if isinstance(x, dict) and len(x) != 0\n]  # pragma: no cover\n</code></pre>"},{"location":"api/writer/","title":"Writer","text":""},{"location":"api/writer/#spark_expectations.sinks.utils.writer-classes","title":"Classes","text":""},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter","title":"<code>spark_expectations.sinks.utils.writer.SparkExpectationsWriter</code>  <code>dataclass</code>","text":"<p>This class implements/supports writing data into the sink system</p>"},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter-attributes","title":"Attributes","text":""},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter.product_id","title":"<code>product_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter-functions","title":"Functions","text":""},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter.generate_rules_exceeds_threshold","title":"<code>generate_rules_exceeds_threshold(rules: dict) -&gt; None</code>","text":"<p>This function implements/supports summarising row dq error threshold</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>dict</code> <p>accepts rule metadata within dict</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/sinks/utils/writer.py</code> <pre><code>def generate_rules_exceeds_threshold(self, rules: dict) -&gt; None:\n\"\"\"\n    This function implements/supports summarising row dq error threshold\n    Args:\n        rules: accepts rule metadata within dict\n    Returns:\n        None\n    \"\"\"\ntry:\nerror_threshold_list = []\nrules_failed_row_count: Dict[str, int] = {}\nif self._context.get_summarised_row_dq_res is None:\nreturn None\nrules_failed_row_count = {\nitr[\"rule\"]: int(itr[\"failed_row_count\"])\nfor itr in self._context.get_summarised_row_dq_res\n}\nfor rule in rules[f\"{self._context.get_row_dq_rule_type_name}_rules\"]:\n# if (\n#         not rule[\"enable_error_drop_alert\"]\n#         or rule[\"rule\"] not in rules_failed_row_count.keys()\n# ):\n#     continue  # pragma: no cover\nrule_name = rule[\"rule\"]\nrule_action = rule[\"action_if_failed\"]\nif rule_name in rules_failed_row_count.keys():\nfailed_row_count = int(rules_failed_row_count[rule_name])\nelse:\nfailed_row_count = 0\nif failed_row_count is not None and failed_row_count &gt; 0:\nerror_drop_percentage = round(\n(failed_row_count / self._context.get_input_count) * 100, 2\n)\nerror_threshold_list.append(\n{\n\"rule_name\": rule_name,\n\"action_if_failed\": rule_action,\n\"description\": rule[\"description\"],\n\"rule_type\": rule[\"rule_type\"],\n\"error_drop_threshold\": str(rule[\"error_drop_threshold\"]),\n\"error_drop_percentage\": str(error_drop_percentage),\n}\n)\nif len(error_threshold_list) &gt; 0:\nself._context.set_rules_exceeds_threshold(error_threshold_list)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"An error occurred while creating error threshold list : {e}\"\n)\n</code></pre>"},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter.generate_summarised_row_dq_res","title":"<code>generate_summarised_row_dq_res(df: DataFrame, rule_type: str) -&gt; None</code>","text":"<p>This function implements/supports summarising row dq error result</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>error dataframe(DataFrame)</p> required <code>rule_type</code> <code>str</code> <p>type of the rule(str)</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spark_expectations/sinks/utils/writer.py</code> <pre><code>def generate_summarised_row_dq_res(self, df: DataFrame, rule_type: str) -&gt; None:\n\"\"\"\n    This function implements/supports summarising row dq error result\n    Args:\n        df: error dataframe(DataFrame)\n        rule_type: type of the rule(str)\n    Returns:\n        None\n    \"\"\"\ntry:\ndef update_dict(accumulator: dict) -&gt; dict:  # pragma: no cover\nif accumulator.get(\"failed_row_count\") is None:  # pragma: no cover\naccumulator[\"failed_row_count\"] = str(2)  # pragma: no cover\nelse:  # pragma: no cover\naccumulator[\"failed_row_count\"] = str(  # pragma: no cover\nint(accumulator[\"failed_row_count\"]) + 1  # pragma: no cover\n)  # pragma: no cover\nreturn accumulator  # pragma: no cover\nsummarised_row_dq_dict: Dict[str, Dict[str, str]] = (\ndf.select(explode(f\"meta_{rule_type}_results\").alias(\"row_dq_res\"))\n.rdd.map(\nlambda rule_meta_dict: (\nrule_meta_dict[0][\"rule\"],\n{**rule_meta_dict[0], \"failed_row_count\": 1},\n)\n)\n.reduceByKey(lambda acc, itr: update_dict(acc))\n).collectAsMap()\nself._context.set_summarised_row_dq_res(\nlist(summarised_row_dq_dict.values())\n)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred created summarised row dq statistics {e}\"\n)\n</code></pre>"},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter.save_df_as_table","title":"<code>save_df_as_table(df: DataFrame, table_name: str, spark_conf: Dict[str, str], options: Dict[str, str]) -&gt; None</code>","text":"<p>This function takes a dataframe and writes into a table</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Provide the dataframe which need to be written as a table</p> required <code>table_name</code> <code>str</code> <p>Provide the table name to which the dataframe need to be written to</p> required <code>spark_conf</code> <code>Dict[str, str]</code> <p>Provide the spark conf that need to be set on the SparkSession</p> required <code>options</code> <code>Dict[str, str]</code> <p>Provide the options that need to be used while writing the table</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> Source code in <code>spark_expectations/sinks/utils/writer.py</code> <pre><code>def save_df_as_table(\nself,\ndf: DataFrame,\ntable_name: str,\nspark_conf: Dict[str, str],\noptions: Dict[str, str],\n) -&gt; None:\n\"\"\"\n    This function takes a dataframe and writes into a table\n    Args:\n        df: Provide the dataframe which need to be written as a table\n        table_name: Provide the table name to which the dataframe need to be written to\n        spark_conf: Provide the spark conf that need to be set on the SparkSession\n        options: Provide the options that need to be used while writing the table\n    Returns:\n        None:\n    \"\"\"\ntry:\nprint(\"run date \", self._context.get_run_date)\nfor key, value in spark_conf.items():\nself.spark.conf.set(key, value)\n_df = df.withColumn(\nself._context.get_run_id_name, lit(f\"{self._context.get_run_id}\")\n).withColumn(\nself._context.get_run_date_name,\nto_timestamp(lit(f\"{self._context.get_run_date}\")),\n)\n_log.info(\"_save_df_as_table started\")\n_df.write.saveAsTable(name=table_name, **options)\nself.spark.sql(\nf\"ALTER TABLE {table_name} SET TBLPROPERTIES ('product_id' = '{self.product_id}')\"\n)\n_log.info(\"finished writing records to table: %s\", table_name)\nexcept Exception as e:\nraise SparkExpectationsUserInputOrConfigInvalidException(\nf\"error occurred while writing data in to the table {e}\"\n)\n</code></pre>"},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter.write_df_to_table","title":"<code>write_df_to_table(df: DataFrame, table: str, spark_conf: Optional[Dict[str, Any]] = None, options: Optional[Dict[str, str]] = None) -&gt; None</code>","text":"<p>This function takes in a dataframe which has dq results publish it</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Provide a dataframe to write the records to a table.</p> required <code>table</code> <p>Provide the full original table name into which the data need to be written to</p> required <code>spark_conf</code> <code>Optional[Dict[str, Any]]</code> <p>Provide the spark conf, if you want to set/override the configuration</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, str]]</code> <p>Provide the options, if you want to override the default.     default options available are - {\"mode\": \"append\", \"format\": \"delta\"}</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> Source code in <code>spark_expectations/sinks/utils/writer.py</code> <pre><code>def write_df_to_table(\nself,\ndf: DataFrame,\ntable: str,\nspark_conf: Optional[Dict[str, Any]] = None,\noptions: Optional[Dict[str, str]] = None,\n) -&gt; None:\n\"\"\"\n    This function takes in a dataframe which has dq results publish it\n    Args:\n        df: Provide a dataframe to write the records to a table.\n        table : Provide the full original table name into which the data need to be written to\n        spark_conf: Provide the spark conf, if you want to set/override the configuration\n        options: Provide the options, if you want to override the default.\n                default options available are - {\"mode\": \"append\", \"format\": \"delta\"}\n    Returns:\n        None:\n    \"\"\"\ntry:\n_spark_conf = (\n{**{\"spark.sql.session.timeZone\": \"Etc/UTC\"}, **spark_conf}\nif spark_conf\nelse {\"spark.sql.session.timeZone\": \"Etc/UTC\"}\n)\n_options = (\n{**{\"mode\": \"append\", \"format\": \"delta\"}, **options}\nif options\nelse {\"mode\": \"append\", \"format\": \"delta\"}\n)\nself.save_df_as_table(df, table, _spark_conf, _options)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while saving the data into the table  {e}\"\n)\n</code></pre>"},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter.write_error_records_final","title":"<code>write_error_records_final(df: DataFrame, error_table: str, rule_type: str, spark_conf: Optional[Dict[str, str]] = None, options: Optional[Dict[str, str]] = None) -&gt; Tuple[int, DataFrame]</code>","text":"Source code in <code>spark_expectations/sinks/utils/writer.py</code> <pre><code>def write_error_records_final(\nself,\ndf: DataFrame,\nerror_table: str,\nrule_type: str,\nspark_conf: Optional[Dict[str, str]] = None,\noptions: Optional[Dict[str, str]] = None,\n) -&gt; Tuple[int, DataFrame]:\ntry:\n_log.info(\"_write_error_records_final started\")\n_spark_conf = (\n{**{\"spark.sql.session.timeZone\": \"Etc/UTC\"}, **spark_conf}\nif spark_conf\nelse {\"spark.sql.session.timeZone\": \"Etc/UTC\"}\n)\n_options = (\n{**{\"mode\": \"append\", \"format\": \"delta\"}, **options}\nif options\nelse {\"mode\": \"append\", \"format\": \"delta\"}\n)\nfailed_records = [\nf\"size({dq_column}) != 0\"\nfor dq_column in df.columns\nif dq_column.startswith(f\"{rule_type}\")\n]\nfailed_records_rules = \" or \".join(failed_records)\n# df = df.filter(expr(failed_records_rules))\ndf = df.withColumn(\nf\"meta_{rule_type}_results\",\nwhen(\nexpr(failed_records_rules),\narray(\n*[\n_col\nfor _col in df.columns\nif _col.startswith(f\"{rule_type}\")\n]\n),\n).otherwise(array(create_map())),\n).drop(*[_col for _col in df.columns if _col.startswith(f\"{rule_type}\")])\ndf = (\ndf.withColumn(\nf\"meta_{rule_type}_results\",\nremove_empty_maps(df[f\"meta_{rule_type}_results\"]),\n)\n.withColumn(\nself._context.get_run_id_name, lit(self._context.get_run_id)\n)\n.withColumn(\nself._context.get_run_date_name,\nlit(self._context.get_run_date),\n)\n)\nerror_df = df.filter(f\"size(meta_{rule_type}_results) != 0\")\nself._context.print_dataframe_with_debugger(error_df)\nself.save_df_as_table(error_df, error_table, _spark_conf, _options)\n_error_count = error_df.count()\nif _error_count &gt; 0:\nself.generate_summarised_row_dq_res(error_df, rule_type)\n_log.info(\"_write_error_records_final ended\")\nreturn _error_count, df\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while saving data into the final error table {e}\"\n)\n</code></pre>"},{"location":"api/writer/#spark_expectations.sinks.utils.writer.SparkExpectationsWriter.write_error_stats","title":"<code>write_error_stats() -&gt; None</code>","text":"<p>This functions takes the stats table and write it into error table</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p>Provide the full table name to which the dq stats will be written to</p> required <code>input_count</code> <p>Provide the original input dataframe count</p> required <code>error_count</code> <p>Provide the error record count</p> required <code>output_count</code> <p>Provide the output dataframe count</p> required <code>source_agg_dq_result</code> <p>source aggregated dq results</p> required <code>final_agg_dq_result</code> <p>final aggregated dq results</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> Source code in <code>spark_expectations/sinks/utils/writer.py</code> <pre><code>def write_error_stats(self) -&gt; None:\n\"\"\"\n    This functions takes the stats table and write it into error table\n    Args:\n        table_name: Provide the full table name to which the dq stats will be written to\n        input_count: Provide the original input dataframe count\n        error_count: Provide the error record count\n        output_count: Provide the output dataframe count\n        source_agg_dq_result: source aggregated dq results\n        final_agg_dq_result: final aggregated dq results\n    Returns:\n        None:\n    \"\"\"\ntry:\nself.spark.conf.set(\"spark.sql.session.timeZone\", \"Etc/UTC\")\nfrom datetime import date\n# table_name: str,\n# input_count: int,\n# error_count: int = 0,\n# output_count: int = 0,\n# source_agg_dq_result: Optional[List[Dict[str, str]]] = None,\n# final_agg_dq_result: Optional[List[Dict[str, str]]] = None,\ntable_name: str = self._context.get_table_name\ninput_count: int = self._context.get_input_count\nerror_count: int = self._context.get_error_count\noutput_count: int = self._context.get_output_count\nsource_agg_dq_result: Optional[\nList[Dict[str, str]]\n] = self._context.get_source_agg_dq_result\nfinal_agg_dq_result: Optional[\nList[Dict[str, str]]\n] = self._context.get_final_agg_dq_result\nsource_query_dq_result: Optional[\nList[Dict[str, str]]\n] = self._context.get_source_query_dq_result\nfinal_query_dq_result: Optional[\nList[Dict[str, str]]\n] = self._context.get_final_query_dq_result\nerror_stats_data = [\n(\nself.product_id,\ntable_name,\ninput_count,\nerror_count,\noutput_count,\nself._context.get_output_percentage,\nself._context.get_success_percentage,\nself._context.get_error_percentage,\nsource_agg_dq_result\nif source_agg_dq_result and len(source_agg_dq_result) &gt; 0\nelse None,\nfinal_agg_dq_result\nif final_agg_dq_result and len(final_agg_dq_result) &gt; 0\nelse None,\nsource_query_dq_result\nif source_query_dq_result and len(source_query_dq_result) &gt; 0\nelse None,\nfinal_query_dq_result\nif final_query_dq_result and len(final_query_dq_result) &gt; 0\nelse None,\nself._context.get_summarised_row_dq_res,\nself._context.get_rules_exceeds_threshold,\n{\n\"run_status\": self._context.get_dq_run_status,\n\"source_agg_dq\": self._context.get_source_agg_dq_status,\n\"source_query_dq\": self._context.get_source_query_dq_status,\n\"row_dq\": self._context.get_row_dq_status,\n\"final_agg_dq\": self._context.get_final_agg_dq_status,\n\"final_query_dq\": self._context.get_final_query_dq_status,\n},\n{\n\"run_time\": self._context.get_dq_run_time,\n\"source_agg_dq_run_time\": self._context.get_source_agg_dq_run_time,\n\"source_query_dq_run_time\": self._context.get_source_query_dq_run_time,\n\"row_dq_run_time\": self._context.get_row_dq_run_time,\n\"final_agg_dq_run_time\": self._context.get_final_agg_dq_run_time,\n\"final_query_dq_run_time\": self._context.get_final_query_dq_run_time,\n},\n{\n\"rules\": {\n\"num_row_dq_rules\": self._context.get_num_row_dq_rules,\n\"num_dq_rules\": self._context.get_num_dq_rules,\n},\n\"agg_dq_rules\": self._context.get_num_agg_dq_rules,\n\"query_dq_rules\": self._context.get_num_query_dq_rules,\n},\nself._context.get_run_id,\ndate.fromisoformat(self._context.get_run_date[0:10]),\ndatetime.strptime(\nself._context.get_run_date,\n\"%Y-%m-%d %H:%M:%S\",\n),\n)\n]\nerror_stats_rdd = self.spark.sparkContext.parallelize(error_stats_data)\nfrom pyspark.sql.types import (\nStructType,\nStructField,\nStringType,\nIntegerType,\nLongType,\nFloatType,\nDateType,\nArrayType,\nMapType,\nTimestampType,\n)\nerror_stats_schema = StructType(\n[\nStructField(\"product_id\", StringType(), True),\nStructField(\"table_name\", StringType(), True),\nStructField(\"input_count\", LongType(), True),\nStructField(\"error_count\", LongType(), True),\nStructField(\"output_count\", LongType(), True),\nStructField(\"output_percentage\", FloatType(), True),\nStructField(\"success_percentage\", FloatType(), True),\nStructField(\"error_percentage\", FloatType(), True),\nStructField(\n\"source_agg_dq_results\",\nArrayType(MapType(StringType(), StringType())),\nTrue,\n),\nStructField(\n\"final_agg_dq_results\",\nArrayType(MapType(StringType(), StringType())),\nTrue,\n),\nStructField(\n\"source_query_dq_results\",\nArrayType(MapType(StringType(), StringType())),\nTrue,\n),\nStructField(\n\"final_query_dq_results\",\nArrayType(MapType(StringType(), StringType())),\nTrue,\n),\nStructField(\n\"row_dq_res_summary\",\nArrayType(MapType(StringType(), StringType())),\nTrue,\n),\nStructField(\n\"row_dq_error_threshold\",\nArrayType(MapType(StringType(), StringType())),\nTrue,\n),\nStructField(\"dq_status\", MapType(StringType(), StringType()), True),\nStructField(\n\"dq_run_time\", MapType(StringType(), FloatType()), True\n),\nStructField(\n\"dq_rules\",\nMapType(StringType(), MapType(StringType(), IntegerType())),\nTrue,\n),\nStructField(self._context.get_run_id_name, StringType(), True),\nStructField(self._context.get_run_date_name, DateType(), True),\nStructField(\nself._context.get_run_date_time_name, TimestampType(), True\n),\n]\n)\ndf = self.spark.createDataFrame(error_stats_rdd, schema=error_stats_schema)\nself._context.print_dataframe_with_debugger(df)\ndf = (\ndf.withColumn(\"output_percentage\", sql_round(df.output_percentage, 2))\n.withColumn(\"success_percentage\", sql_round(df.success_percentage, 2))\n.withColumn(\"error_percentage\", sql_round(df.error_percentage, 2))\n)\n_log.info(\n\"Writing metrics to the stats table: %s, started\",\nself._context.get_dq_stats_table_name,\n)\n_se_stats_dict = self._context.get_se_streaming_stats_dict\nsecret_handler = SparkExpectationsSecretsBackend(secret_dict=_se_stats_dict)\nkafka_write_options: dict = (\n{\n\"kafka.bootstrap.servers\": \"localhost:9092\",\n\"topic\": self._context.get_se_streaming_stats_topic_name,\n\"failOnDataLoss\": \"true\",\n}\nif self._context.get_env == \"local\"\nelse (\n{\n\"kafka.bootstrap.servers\": f\"{secret_handler.get_secret(self._context.get_server_url_key)}\",\n\"kafka.security.protocol\": \"SASL_SSL\",\n\"kafka.sasl.mechanism\": \"OAUTHBEARER\",\n\"kafka.sasl.jaas.config\": \"kafkashaded.org.apache.kafka.common.security.oauthbearer.\"\n\"OAuthBearerLoginModule required oauth.client.id=\"\nf\"'{secret_handler.get_secret(self._context.get_client_id)}'  \"\n+ \"oauth.client.secret=\"\nf\"'{secret_handler.get_secret(self._context.get_token)}' \"\n\"oauth.token.endpoint.uri=\"\nf\"'{secret_handler.get_secret(self._context.get_token_endpoint_url)}'; \",\n\"kafka.sasl.login.callback.handler.class\": \"io.strimzi.kafka.oauth.client\"\n\".JaasClientOauthLoginCallbackHandler\",\n\"topic\": (\nself._context.get_se_streaming_stats_topic_name\nif self._context.get_env == \"local\"\nelse secret_handler.get_secret(self._context.get_topic_name)\n),\n}\nif bool(_se_stats_dict[user_config.se_enable_streaming])\nelse {}\n)\n)\n_sink_hook.writer(\n_write_args={\n\"product_id\": self.product_id,\n\"enable_se_streaming\": _se_stats_dict[\nuser_config.se_enable_streaming\n],\n\"table_name\": self._context.get_dq_stats_table_name,\n\"kafka_write_options\": kafka_write_options,\n\"stats_df\": df,\n}\n)\n_log.info(\n\"Writing metrics to the stats table: %s, ended\",\nself._context.get_dq_stats_table_name,\n)\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while saving the data into the stats table {e}\"\n)\n</code></pre>"},{"location":"api/writer/#spark_expectations.sinks.utils.writer-functions","title":"Functions","text":""},{"location":"configurations/adoption_versions_comparsion/","title":"comparision","text":"<p>Please find the difference in the changes with different version, latest three versions changes are documented</p>"},{"location":"configurations/adoption_versions_comparsion/#modifications-made-to-the-version-during-implementation-or-integration","title":"Modifications made to the version during implementation or integration","text":"stage 0.6.0 0.7.0 0.8.0 rules table schema changes refer rule table creation here added three additional column  1.<code>enable_for_source_dq_validation(boolean)</code>  2.<code>enable_for_target_dq_validation(boolean)</code>  3.<code>is_active(boolean)</code>  documentation found here added additional two column  1.<code>enable_error_drop_alert(boolean)</code>  2.<code>error_drop_thresholdt(int)</code>  documentation found here rule table creation required yes yes - creation not required if you're upgrading from old version but schema changes required yes - creation not required if you're upgrading from old version but schema changes required stats table schema changes refer rule table creation here added additional columns  1. <code>source_query_dq_results</code>  2. <code>final_query_dq_results</code>  3. <code>row_dq_res_summary</code>  4. <code>dq_run_time</code>  5. <code>dq_rules</code>  renamed columns  1. <code>runtime</code> to <code>meta_dq_run_time</code>  2. <code>run_date</code> to <code>meta_dq_run_date</code>  3. <code>run_id</code> to <code>meta_dq_run_id</code>  documentation found here remains same stats table creation required yes yes - creation not required if you're upgrading from old version but schema changes required automated notification config setting define global notification param, register as env variable and place in the <code>__init__.py</code> file for multiple usage, example Define a global notification parameter in the <code>__init__.py</code> file to be used in multiple instances where the spark_conf parameter needs to be passed within the with_expectations function. example remains same secret store and kafka authentication details not applicable not applicable Create a dictionary that contains your secret configuration values and register in <code>__init__.py</code> for multiple usage, example spark expectations initialisation create SparkExpectations class object using the <code>SparkExpectations</code> library and by passing the <code>product_id</code> create spark expectations class object using <code>SpakrExpectations</code> by passing <code>product_id</code> and optional parameter <code>debugger</code> example create spark expectations class object using <code>SpakrExpectations</code> by passing <code>product_id</code> and additional optional parameter <code>debugger</code>, <code>stats_streaming_options</code> example spark expectations decorator The decorator allows for configuration by passing individual parameters to each decorator. However, registering a DataFrame view within a decorated function is not supported for implementations of query_dq example The decorator allows configurations to be logically grouped through a dictionary passed as a parameter to the decorator. Additionally, registering a DataFrame view within a decorated function is supported for implementations of query_dq. example remains same"},{"location":"configurations/configure_rules/","title":"Configure_Rules","text":""},{"location":"configurations/configure_rules/#configure-rules-in-the-catalogschemaproduct_rules","title":"Configure Rules in the <code>catalog</code>.<code>schema</code>.<code>{product}_rules</code>","text":"<p>Please find the data set which used for the data quality rules setup order.csv</p>"},{"location":"configurations/configure_rules/#example-of-row-aggregation-and-query-rules-for-data-quality","title":"Example Of Row, Aggregation And Query Rules For Data Quality","text":"<p>To perform row data quality checks for artificially order table, please set up rules using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description, enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active) values\n--The row data qulaity has been set on customer_id when customer_id is null, drop respective row into error table \n--as \"action_if_failed\" tagged \"drop\"\n('apla_nd', '`catalog`.`schema`.customer_order',  'row_dq', 'customer_id_is_not_null', 'customer_id', 'customer_id is not null','drop', 'validity', 'customer_id ishould not be null', false, false, true)\n--The row data qulaity has been set on sales when sales is less than zero, drop respective row into error table as \n--'action_if_failed' tagged \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'sales_greater_than_zero', 'sales', 'sales &gt; 0', 'drop', 'accuracy', 'sales value should be greater than zero', false, false, true)\n--The row data qulaity has been set on discount when discount is less than 60, drop respective row into error table\n--and final table  as \"action_if_failed\" tagged 'ignore'\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'discount_threshold', 'discount', 'discount*100 &lt; 60',\n'ignore', 'validity', 'discount should be less than 40', false, false, true)\n--The row data qulaity has been set on ship_mode when ship_mode not in (\"second class\", \"standard class\", \n--\"standard class\"), drop respective row into error table and fail the framewok  as \"action_if_failed\" tagged \"fail\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'ship_mode_in_set', 'ship_mode', 'lower(trim(ship_mode))\nin('second class', 'standard class', 'standard class')', 'fail', 'validity', 'ship_mode mode belongs in the sets',\nfalse, false, true)\n--The row data qulaity has been set on profit when profit is less than or equals to 0, drop respective row into \n--error table and final table as \"action_if_failed\" tagged \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'profit_threshold', 'profit', 'profit&gt;0', 'ignore', 'validity', 'profit threshold should be greater tahn 0', false, false, true)\n--The rule has been established to identify and remove completely identical records in which rows repeat with the \n--same value more than once, while keeping one instance of the row. Any additional duplicated rows will be dropped \n--into error table as action_if_failed set to \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'complete_duplicate', 'All', 'row_number() \n over(partition by cutomer_id, order_id order by 1)=1', 'drop', 'uniqueness', 'drop complete duplicate records', false, false, true)\n</code></pre> <p>Please set up rules for checking the quality of the columns in the artificially order table, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description,  enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active) values\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales values falls below 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales', 'sales', 'sum(sales)&gt;10000', 'ignore', 'validity', 'sum of sales must be greater than 10000',  true, true, true)\n--The aggregation rule is established on the 'ship_mode' column and the metadata of the rule will be captured in \n--the statistics table when distinct ship_mode greater than 3 and enabled for only source data set\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'distinct_of_ship_mode', 'ship_mode', 'count(distinct ship_mode)&lt;=3', 'ignore', 'validity', 'regex format validation for quantity', true, false, true)\n-- The aggregation rule is established on the table countand the metadata of the rule will be captured in the \n--statistics table when distinct count greater than 10000 and failes the job as \"action_if_failed\" set to \"fail\" \n--and enabled only for validated datset\n,('apla_nd', '`catalog`.`schema`..customer_order', 'agg_dq', 'row_count', '*', 'count(*)&gt;=10000', 'fail', 'validity',\n'distinct ship_mode must be less or equals to 3', false, true, true)\n</code></pre> <p>Please set up rules for checking the quality of artificially order table by implementing query data quality option, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description) values\n--The query dq rule is established to check product_id differemce between two table if differnce is more than 20% \n--from source table, the metadata of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_missing_count_threshold', '*', '((select count(distinct product_id) from product) - (select count(distinct product_id) from order))&gt;\n(select count(distinct product_id) from product)*0.2', 'ignore', 'validity', 'row count threshold difference msut \nbe less than 20%', true, true, true)\n--The query dq rule is established to check distinct proudtc_id in the product table is less than 5, if not the \n--metadata of the rule will be captured in the statistics table along with fails the job as \"action_if_failed\" is \n--\"fail\" and enabled for source dataset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_category', '*', '(select count(distinct category) \nfrom product) &lt; 5', 'fail', 'validity', 'distinct product category must be less than 5', true, False, true)\n--The query dq rule is established to check count of the dataset should be less than 10000 other wise the metadata \n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled only for target datset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'row_count_in_order', '*', '(select count(*) from order)&lt;10000', 'ignore', 'accuracy', 'count of the row in order dataset must be less then 10000', false, true, true)\n</code></pre>"},{"location":"configurations/databricks_setup_guide/","title":"Databricks setup guide","text":""},{"location":"configurations/databricks_setup_guide/#effortlessly-explore-spark-expectations-on-example-dataset-with-automated-setup-in-databricks","title":"Effortlessly Explore Spark Expectations on Example Dataset with Automated Setup in Databricks","text":"<p>This section provides instructions on how to set up a sample notebook in the Databricks environment to investigate, comprehend, and conduct a feasibility study on the Spark Expectations framework.</p>"},{"location":"configurations/databricks_setup_guide/#prerequisite","title":"Prerequisite:","text":"<ol> <li>Recommended databricks run time environment for better experience - DBS 11.0 and above</li> <li>Please install the kafka jar using the path <code>dbfs:/kafka-jars/databricks-shaded-strimzi-kafka-oauth-client-1.1.jar</code>, If the jar is not available in the dbfs location, please raise a ticket with GAP Support team to add the jar to your workspace</li> <li>Please follow the steps provided here to integrate and clone repo from git databricks</li> <li>Please follow the steps to create the wbhook-hook URL for team-specific channel here</li> </ol>"},{"location":"configurations/rules/","title":"Rules","text":""},{"location":"configurations/rules/#different-types-of-expectations","title":"Different Types of Expectations","text":"<p>Please find the different types of possible expectations </p>"},{"location":"configurations/rules/#possible-row-data-quality-expectations","title":"Possible Row Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect that the values in the column should not be null/empty null_validation completeness <code>[col_name] is not null</code> Ensure that the primary key values are unique and not duplicated primary_key_validation uniqueness <code>count(*) over(partition by [primary_key_or_combination_of_primary_key] order by 1)=1</code> Perform a thorough check to make sure that there are no duplicate values, if there are duplicates preserve one row into target complete_duplicate_validation uniqueness <code>row_number() over(partition by [all_the_column_in_dataset_b_ comma_separated] order by 1)=1</code> Verify that the date values are in the correct format date_format_validation validity <code>to_date([date_col_name], '[mention_expected_date_format]') is not null</code> Verify that the date values are in the correct format using regex date_format_validation_with_regex validity <code>[date_col_name] rlike '[regex_format_of_date]'</code> Expect column value is date parseable expect_column_values_to_be_date_parseable validity <code>try_cast([date_col_name] as date)</code> Verify values in a column to conform to a specified regular expression pattern expect_column_values_to_match_regex validity <code>[col_name]  rlike '[regex_format]'</code> Verify values in a column to not conform to a specified regular expression pattern expect_column_values_to_not_match_regex validity <code>[col_name] not rlike '[regex_format]'</code> Verify values in a column to match regex in list expect_column_values_to_match_regex_list validity <code>[col_name] not rlike '[regex format1]' or [col_name] not rlike '[regex_format2]' or [col_name] not rlike '[regex_format3]'</code> Expect the values in a column to belong to a specified set expect_column_values_to_be_in_set accuracy <code>[col_name] in ([values_in_comma_separated])</code> Expect the values in a column not to belong to a specified set expect_column_values_to_be_not_in_set accuracy <code>[col_name] not in ([values_in_comma_separated])</code> Expect the values in a column to fall within a defined range expect_column_values_to_be_in_range accuracy <code>[col_name] between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be within a specified range expect_column_value_lengths_to_be_between accuracy <code>length([col_name]) between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be equal to a certain value expect_column_value_lengths_to_be_equal accuracy <code>length([col_name])=[threshold]</code> Expect values in the column to exceed a certain limit expect_column_value_to_be_greater_than accuracy <code>[col_name] &gt; [threshold_value]</code> Expect values in the column  not to exceed a certain limit expect_column_value_to_be_lesser_than accuracy <code>[col_name] &lt; [threshold_value]</code> Expect values in the column to be equal to or exceed a certain limit expect_column_value_greater_than_equal accuracy <code>[col_name] &gt;= [threshold_value]</code> Expect values in the column to be equal to or not exceed a certain limit expect_column_value_lesser_than_equal accuracy <code>[col_name] &lt;= [threshold_value]</code> Expect values in column A to be greater than values in column B expect_column_pair_values_A_to_be_greater_than_B accuracy <code>[col_A] &gt; [col_B]</code> Expect values in column A to be lesser than values in column B expect_column_pair_values_A_to_be_lesser_than_B accuracy <code>[col_A] &lt; [col_B]</code> Expect values in column A to be greater than or equals to values in column B expect_column_A_to_be_greater_than_B accuracy <code>[col_A] &gt;= [col_B]</code> Expect values in column A to be lesser than or equals to values in column B expect_column_A_to_be_lesser_than_or_equals_B accuracy <code>[col_A] &lt;= [col_B]</code> Expect the sum of values across multiple columns to be equal to a certain value expect_multicolumn_sum_to_equal accuracy <code>[col_1] + [col_2] + [col_3] = [threshold_value]</code> Expect sum of values in each category equals certain value expect_sum_of_value_in_subset_equal accuracy <code>sum([col_name]) over(partition by [category_col] order by 1)</code> Expect count of values in each category equals certain value expect_count_of_value_in_subset_equal accuracy <code>count(*) over(partition by [category_col] order by 1)</code> Expect distinct value in each category exceeds certain range expect_distinct_value_in_subset_exceeds accuracy <code>count(distinct [col_name]) over(partition by [category_col] order by 1)</code>"},{"location":"configurations/rules/#possible-aggregation-data-quality-expectations","title":"Possible Aggregation Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column that are present in a given list expect_column_distinct_values_to_be_in_set accuracy <code>array_intersect(collect_list(distinct [col_name]), Array($compare_values_string)) = Array($compare_values_string)</code> Expect the mean value of a column to fall within a specified range expect_column_mean_to_be_between consistency <code>avg([col_name]) between [lower_bound] and [upper_bound]</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>percentile_approx([column_name], 0.5) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>stddev([col_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>count(distinct [col_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>max([col_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_sum_to_be_between accuracy <code>min([col_name]) between [lower_bound] and [upper_bound]</code> Expect row count of the dataset fall within certain range expect_row_count_to_be_between accuracy <code>count(*) between [lower_bound] and [upper_bound]</code>"},{"location":"configurations/rules/#possible-query-data-quality-expectations","title":"Possible Query Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column must be greater than threshold value expect_column_distinct_values_greater_than_threshold_value accuracy <code>(select count(distinct [col_name]) from [table_name]) &gt; [threshold_value]</code> Expect count between two table or view must be same expect_count_between_two_table_same consistency <code>(select count(*) from [table_a]) = (select count(*) from [table_b])</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>(select percentile_approx([column_name], 0.5) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>(select stddev([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>(select count(distinct [col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>(select max([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_min_to_be_between accuracy <code>(select min([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect referential integrity expect_referential_integrity_between_two_table_should_be_less_than_100 accuracy <code>with refrentail_check as ( select * from [table_a] left join [table_b] on [condition] where [table_b.column] is null) select count(*) from refrentail_check) &lt; 100</code>"},{"location":"getting-started/setup/","title":"Setup","text":""},{"location":"getting-started/setup/#installation","title":"Installation","text":"<p>The library is available in the pypi and can be installed in your environment using the below command or  please add the library \"spark-expectations\" into the requirements.txt or poetry dependencies.</p> <pre><code>pip install -U spark-expectations\n</code></pre>"},{"location":"getting-started/setup/#required-tables","title":"Required Tables","text":"<p>There are two tables that need to be created for spark-expectations to run seamlessly and integrate with a spark job. The below sql statements used three namespaces which works with Databricks UnityCatalog, but if you are using hive please update the namespaces accordingly and also provide necessary table metadata.</p>"},{"location":"getting-started/setup/#rules-table","title":"Rules Table","text":"<p>We need to create a rules tables which contains all the data quality rules. Please use the below template to create your rules table for your project.</p> <pre><code>create table if not exists `catalog`.`schema`.`{product}_rules` (\nproduct_id STRING,\ntable_name STRING,\nrule_type STRING,\nrule STRING,\ncolumn_name STRING,\nexpectation STRING,\naction_if_failed STRING,\ntag STRING,\ndescription STRING,\nenable_for_source_dq_validation BOOLEAN, enable_for_target_dq_validation BOOLEAN,\nis_active BOOLEAN,\nenable_error_drop_alert BOOLEAN,\nerror_drop_threshold INT\n);\n</code></pre>"},{"location":"getting-started/setup/#rule-type-for-rules","title":"Rule Type For Rules","text":"<p>The rules column has a column called \"rule_type\". It is important that this column should only accept one of  these three values - <code>[row_dq, agg_dq, query_dq]</code>. If other values are provided, the library may cause unforseen errors. Please run the below command to add constriants to the above created rules table</p> <pre><code>ALTER TABLE `catalog`.`schema`.`{product}_rules` ADD CONSTRAINT rule_type_action CHECK (rule_type in ('row_dq', 'agg_dq', 'query_dq'));\n</code></pre>"},{"location":"getting-started/setup/#action-if-failed-for-row-aggregation-and-query-data-quality-rules","title":"Action If Failed For Row, Aggregation and Query Data Quality Rules","text":"<p>The rules column has a column called \"action_if_failed\". It is important that this column should only accept one of  these values - <code>[fail, drop or ignore]</code> for <code>'rule_type'='row_dq'</code> and <code>[fail, ignore]</code> for <code>'rule_type'='agg_dq' and 'rule_type'='query_dq'</code>.  If other values are provided, the library may cause unforseen errors. Please run the below command to add constriants to the above created rules table</p> <pre><code>ALTER TABLE apla_nd_dq_rules ADD CONSTRAINT action CHECK ((rule_type = 'row_dq' and action_if_failed IN ('ignore', 'drop', 'fail')) or (rule_type = 'agg_dq' and action_if_failed in ('ignore', 'fail')) or (rule_type = 'query_dq' and action_if_failed in ('ignore', 'fail')));\n</code></pre>"},{"location":"getting-started/setup/#dq-stats-table","title":"DQ Stats Table","text":"<p>In order to collect the stats/metrics for each data quality job run, the spark-expectations job will automatically create the stats table if it does not exist. The below sql statement can be used to create the table if you want to create it manually.</p> <pre><code>create table if not exists `catalog`.`schema`.`dq_stats` (\nproduct_id STRING,\ntable_name STRING,\ninput_count LONG,\nerror_count LONG,\noutput_count LONG,\noutput_percentage FLOAT,\nsuccess_percentage FLOAT,\nerror_percentage FLOAT,\nsource_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,\nfinal_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,\nsource_query_dq_results array&lt;map&lt;string, string&gt;&gt;,\nfinal_query_dq_results array&lt;map&lt;string, string&gt;&gt;,\nrow_dq_res_summary array&lt;map&lt;string, string&gt;&gt;,\nrow_dq_error_threshold array&lt;map&lt;string, string&gt;&gt;,\ndq_status map&lt;string, string&gt;,\ndq_run_time map&lt;string, float&gt;,\ndq_rules map&lt;string, map&lt;string,int&gt;&gt;,\nmeta_dq_run_id STRING,\nmeta_dq_run_date DATE,\nmeta_dq_run_datetime TIMESTAMP\n);\n</code></pre>"}]}