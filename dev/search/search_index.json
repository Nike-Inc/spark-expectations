{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Spark-Expectations","text":"<p>Taking inspiration from DLT - data quality expectations: Spark-Expectations is built, so that the data quality rules can  run using decorator pattern while the spark job is in flight and Additionally, the framework able to perform data  quality checks when the data is at rest.</p>"},{"location":"#concept","title":"Concept","text":"<p>Most of the data quality tools do the data quality checks or data validation on a table at rest and provide metrics in  different forms. <code>While the existing tools are good to do profiling and provide metrics, below are the problems that we  commonly see</code> </p> <ul> <li>The existing tools do not perform any action or remove the malformed data in the original table </li> <li>Most existing frameworks do not offer the capability to perform both row and column level data quality checks  within a single tool.</li> <li>User have to manually check the provided metrics, and it becomes cumbersome to find the records which doesn't meet  the data quality standards</li> <li>Downstream users have to consume the same data with error, or they have to do additional computation to remove the  records that doesn't meet the standards</li> <li>Another process is required as a corrective action to rectify the errors in the data and lot of planning is usually  required for this acitivity</li> </ul> <p><code>Spark-Expectations solves all of the above problems by following the below principles</code></p> <ul> <li>Spark Expectations provides the ability to run both individual row-based and overall aggregated data quality rules  on both the source and validated data sets. In case a rules fails, the row-level error is recorded in the <code>_error</code> table  and a summarized report of all failed aggregated data quality rules is compiled in the <code>_stats</code> table</li> <li>All the records which fail one or more data quality rules, are by default quarantined in an <code>_error</code> table along with  the metadata on rules that failed, job information etc. This helps analysts or products to look at the error data easily  and work with the teams required to correct the data and reprocess it easily</li> <li>Aggregated Metrics are provided on the job level along with necessary metadata so that recalculation or compute is  avoided</li> <li>The data that doesn't meet the data quality contract or the standards is not written into the final table unless or otherwise specified. </li> <li>By default, frameworks have the capability to send notifications only upon failure, but they have the ability to  send notifications at the start, as well as upon completion</li> </ul> <p>There is a field in the rules table called action_if_failed, which determines what needs to be done if a rule fails</p> <ul> <li>Let's consider a hypothetical scenario, where we have 100 columns and with 200 row level data quality rules, 10 aggregation data quality rules and 5 query data quality rules  computed against. When the dq job is run, there are 10 rules that failed on a particular row and 4 aggregation rules fails- what determines if that row should end up in  final table or not? Below are the heirarchy of checks that happens?</li> <li>Among the row level 10 rules failed, if there is atleast one rule which has an action_if_failed as fail -    then the job will be failed </li> <li>Among the 10 row level rules failed, if there is no rule that has an action_if_failed as fail, but atleast    has one rule with action_if_failed as drop - then the record/row will be dropped</li> <li>Among the 10 row level rules failed, if no rule neither has fail nor drop as an action_if_failed - then    the record will be end up in the final table. Note that, this record would also exist in the <code>_error</code> table</li> <li>The aggregation and query dq rules have a setting called <code>action_if_failed</code> with two options: <code>fail</code> or <code>ignore</code>. If any of   the 10 aggregation rules and 5 query dq rules which failed has an action_if_failed_as_fail, then the metadata summary will be    recorded in the <code>_stats</code> table and the job will be considered a failure. However, if none of the failed rules    has an action_if_failed_as_fail, then summary of the aggregated rules' metadata will still be collected in the    <code>_stats</code> table for failed aggregated and  query dq rules.</li> </ul> <p>Please find the spark-expectations flow and feature diagrams here</p>"},{"location":"examples/","title":"Initialization_Examples","text":""},{"location":"examples/#configurations","title":"Configurations","text":"<p>In order to establish the global configuration parameter for DQ Spark Expectations, you must define and complete the required fields within a variable. This involves creating a variable and ensuring that all the necessary information is provided in the appropriate fields.</p> <pre><code>from spark_expectations.config.user_config import Constants as user_config\nse_global_spark_Conf = {\nuser_config.se_notifications_enable_email: False,  # (1)!\nuser_config.se_notifications_email_smtp_host: \"mailhost.com\",  # (2)!\nuser_config.se_notifications_email_smtp_port: 25,  # (3)!\nuser_config.se_notifications_email_from: \"&lt;sender_email_id&gt;\",  # (4)!\nuser_config.se_notifications_email_to_other_mail_id: \"&lt;receiver_email_id's&gt;\",  # (5)!\nuser_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",  # (6)!\nuser_config.se_notifications_enable_slack: True,  # (7)!\nuser_config.se_notifications_slack_webhook_url: \"&lt;slack-webhook-url&gt;\",  # (8)!\nuser_config.se_notifications_on_start: True,  # (9)!\nuser_config.se_notifications_on_completion: True,  # (10)!\nuser_config.se_notifications_on_fail: True,  # (11)!\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,  # (12)!\nuser_config.se_notifications_on_error_drop_threshold: 15,  # (13)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_notifications_enable_email</code> parameter, which controls whether notifications are sent via email, is set to false by default</li> <li>The <code>user_config.se_notifications_email_smtp_host</code> parameter is set to \"mailhost.com\" by default and is used to specify the email SMTP domain host</li> <li>The <code>user_config.se_notifications_email_smtp_port</code> parameter, which accepts a port number, is set to \"25\" by default</li> <li>The <code>user_config.se_notifications_email_from</code> parameter is used to specify the email ID that will trigger the email notification</li> <li>The <code>user_configse_notifications_email_to_other_mail_id</code> parameter accepts a list of recipient email IDs</li> <li>The <code>user_config.se_notifications_email_subject</code> parameter captures the subject line of the email</li> <li>The <code>user_config.se_notifications_enable_slack</code> parameter, which controls whether notifications are sent via slack, is set to false by default</li> <li>The <code>user_config/se_notifications_slack_webhook_url</code> parameter accepts the webhook URL of a Slack channel for sending notifications</li> <li>When <code>user_config.se_notifications_on_start</code> parameter set to <code>True</code> enables notification on start of the spark-expectations, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_completion</code> parameter set to <code>True</code> enables notification on completion of spark-expectations framework, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_fail</code> parameter set to <code>True</code> enables notification on failure of spark-expectations data qulaity framework, variable by default set to <code>True</code></li> <li>When <code>user_config.se_notifications_on_error_drop_exceeds_threshold_breach</code> parameter set to <code>True</code> enables notification when error threshold reaches above the configured value</li> <li>The <code>user_config.se_notifications_on_error_drop_threshold</code> parameter captures error drop threshold value</li> </ol>"},{"location":"examples/#spark-expectations-initialization","title":"Spark Expectations Initialization","text":"<p>For all the below examples the below import and SparkExpectations class instantiation is mandatory</p> <p>When store for sensitive details is Databricks secret scope,construct config dictionary for authentication of kafka and  avoid duplicate construction every time your project is initialized, you can create a dictionary with the following keys and their appropriate values.  This dictionary can be placed in the init.py file of your project or declared as a global variable. <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.dbx_workspace_url  : \"https://workspace.cloud.databricks.com\", # (3)!\nuser_config.dbx_secret_scope: \"sole_common_prod\", # (4)!\nuser_config.dbx_kafka_server_url: \"se_streaming_server_url_secret_key\", # (5)!\nuser_config.dbx_secret_token_url: \"se_streaming_auth_secret_token_url_key\", # (6)!\nuser_config.dbx_secret_app_name: \"se_streaming_auth_secret_appid_key\", # (7)!\nuser_config.dbx_secret_token: \"se_streaming_auth_secret_token_key\", # (8)!\nuser_config.dbx_topic_name: \"se_streaming_topic_name\", # (9)!\n}\n</code></pre></p> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cererus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.dbx_workspace_url</code> used to pass databricks workspace in the format <code>https://&lt;workspace_name&gt;.cloud.databricks.com</code></li> <li>The <code>user_config.dbx_secret_scope</code> captures name of the secret scope</li> <li>The <code>user_config.dbx_kafka_server_url</code> captures secret key for the kafka url</li> <li>The <code>user_config.dbx_secret_token_url</code> captures secret key for the kafka authentication app url</li> <li>The <code>user_config.dbx_secret_app_name</code> captures secret key for the kafka authentication app name</li> <li>The <code>user_config.dbx_secret_token</code> captures secret key for the kafka authentication app secret token</li> <li>The <code>user_config.dbx_topic_name</code> captures secret key for the kafka topic name</li> </ol> <p>Similarly when sensitive store is cerberus: </p> <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.cbs_url  : \"https://&lt;url&gt;.cerberus.com\", # (3)!\nuser_config.cbs_sdb_path: \"cerberus_sdb_path\", # (4)!\nuser_config.cbs_kafka_server_url: \"se_streaming_server_url_secret_sdb_path\", # (5)!\nuser_config.cbs_secret_token_url: \"se_streaming_auth_secret_token_url_sdb_apth\", # (6)!\nuser_config.cbs_secret_app_name: \"se_streaming_auth_secret_appid_sdb_path\", # (7)!\nuser_config.cbs_secret_token: \"se_streaming_auth_secret_token_sdb_path\", # (8)!\nuser_config.cbs_topic_name: \"se_streaming_topic_name_sdb_path\", # (9)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cererus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.cbs_url</code> used to pass cerberus url</li> <li>The <code>user_config.cbs_sdb_path</code> captures cerberus secure data store path</li> <li>The <code>user_config.cbs_kafka_server_url</code> captures path where kafka url stored in the cerberus sdb</li> <li>The <code>user_config.cbs_secret_token_url</code> captures path where kafka authentication app stored in the cerberus sdb</li> <li>The <code>user_config.cbs_secret_app_name</code> captures path where kafka authentication app name stored in the cerberus sdb</li> <li>The <code>user_config.cbs_secret_token</code> captures path where kafka authentication app name secret token stored in the cerberus sdb</li> <li>The <code>user_config.cbs_topic_name</code>  captures path where kafka topic name stored in the cerberus sdb</li> </ol> <pre><code>from spark_expectations.core.expectations import SparkExpectations\n# product_id should match with the \"product_id\" in the rules table\nse: SparkExpectations = SparkExpectations(product_id=\"your-products-id\", stats_streaming_options=stats_streaming_config_dict)  # (1)!\n</code></pre> <ol> <li>Instantiate <code>SparkExpectations</code> class which has all the required functions for running data quality rules</li> </ol>"},{"location":"examples/#example-1","title":"Example 1","text":"<pre><code>from spark_expectations.config.user_config import *  # (7)!\n@se.with_expectations(  # (6)!\nse.reader.get_rules_from_table(  # (5)!\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",  # (1)!\ntable_name=\"pilot_nonpub.dq_employee.employee\",  # (2)!\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"  # (3)!\n),\nwrite_to_table=True,  # (4)!\nwrite_to_temp_table=True,  # (8)!\nrow_dq=True,  # (9)!\nagg_dq={  # (10)!\nuser_config.se_agg_dq: True,  # (11)!\nuser_config.se_source_agg_dq: True,  # (12)!\nuser_config.se_final_agg_dq: True,  # (13)!\n},\nquery_dq={  # (14)!\nuser_config.se_query_dq: True,  # (15)!\nuser_config.se_source_query_dq: True,  # (16)!\nuser_config.se_final_query_dq: True,  # (17)!\nuser_config.se_target_table_view: \"order\",  # (18)!\n},\nspark_conf=se_global_spark_Conf,  # (19)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")  # (20)!\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")  # (20)!\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")  # (20)!\nreturn _df_order  # (21)!\n</code></pre> <ol> <li>Provide the full table name of the table which contains the rules</li> <li>Provide the table name using which the <code>_error</code> table will be created, which contains all the failed records.     Note if you are also wanting to write the data using <code>write_df</code>, then the table_name provided to both the functions     should be same</li> <li>Provide the full table name where the stats will be written into</li> <li>Use this argument to write the final data into the table. By default, it is False.    This is optional, if you just want to run the data quality checks.    A good example will be a staging table or temporary view.</li> <li>This functions reads the rules from the table and return them as a dict, which is an input to the <code>with_expectations</code> function</li> <li>This is the decorator that helps us run the data quality rules. After running the rules the results will be written into <code>_stats</code> table and <code>error</code> table</li> <li>import necessary configurable variables from <code>user_config</code> package for the specific functionality to configure in spark-expectations</li> <li>Use this argument to write the input dataframe into the temp table, so that it breaks the spark plan and might speed     up the job in cases of complex dataframe lineage</li> <li>The argument row_dq is optional and enables the conducting of row-based data quality checks. By default, this     argument is set to True, however, if desired, these checks can be skipped by setting the argument to False.</li> <li>The <code>agg_dq</code> argument is a dictionary that is used to gather different settings and options for the purpose of configuring the <code>agg_dq</code></li> <li>The argument <code>se_agg_dq</code> is utilized to activate the aggregate data quality check, and its default setting is True.</li> <li>The <code>se_source_agg_dq</code> argument is optional and enables the conducting of aggregate-based data quality checks on the      source data. By default, this argument is set to True, and this option depends on the <code>agg_dq</code> value.      If desired, these checks can be skipped by setting the source_agg_dq argument to False.</li> <li>This optional argument <code>se_final_agg_dq</code> allows to perform agg-based data quality checks on final data, with the      default setting being <code>True</code>, which depended on <code>row_agg</code> and <code>agg_dq</code>. skip these checks by setting argument to <code>False</code></li> <li>The <code>query_dq</code> argument is a dictionary that is used to gather different settings and options for the purpose of configuring the <code>query_dq</code></li> <li>The argument <code>se_query_dq</code> is utilized to activate the aggregate data quality check, and its default setting is True. </li> <li>The <code>se_source_query_dq</code> argument is optional and enables the conducting of query-based data quality checks on the      source data. By default, this argument is set to True, and this option depends on the <code>agg_dq</code> value.      If desired, these checks can be skipped by setting the source_agg_dq argument to False.</li> <li>This optional argument <code>se_final_query_dq</code> allows to perform query_based data quality checks on final data, with the      default setting being <code>True</code>, which depended on <code>row_agg</code> and <code>agg_dq</code>. skip these checks by setting argument to <code>False</code></li> <li>The parameter <code>se_target_table_view</code> can be provided with the name of a view that represents the target validated dataset for implementation of <code>query_dq</code> on the clean dataset from <code>row_dq</code></li> <li>The <code>spark_conf</code> parameter is utilized to gather all the configurations that are associated with notifications</li> <li>View registration can be utilized when implementing <code>query_dq</code> expectations.</li> <li>Returning a dataframe is mandatory for the <code>spark_expectations</code> to work, if we do not return a dataframe - then an exceptionm will be raised</li> </ol>"},{"location":"examples/#example-2","title":"Example 2","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\")\n),\nrow_dq=True  # (2)!\n)\ndef build_new() -&gt; DataFrame:\n_df: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/employee.csv\"))\n)\nreturn df \n</code></pre> <ol> <li>Conduct only row-based data quality checks while skipping the aggregate data quality checks</li> <li>Disabled the aggregate data quality checks</li> </ol>"},{"location":"examples/#example-3","title":"Example 3","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=False,  # (2)!\nagg_dq={\nuser_config.se_agg_dq: True, \nuser_config.se_source_agg_dq: True, \nuser_config.se_final_agg_dq: False, \n}\n)\ndef build_new() -&gt; DataFrame:\n_df: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/employee.csv\"))\n)\nreturn df \n</code></pre> <ol> <li>Perform only aggregate-based data quality checks while avoiding both row-based data quality checks and aggregate data    quality checks on the validated dataset, since row validation has not taken place</li> <li>Disabled the row data quality checks</li> </ol>"},{"location":"examples/#example-4","title":"Example 4","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=True, \nquery_dq={  # (2)!\nuser_config.se_query_dq: True,\nuser_config.se_source_query_dq: True,\nuser_config.se_final_query_dq: True,\nuser_config.se_target_table_view: \"order\",\n},\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\") \n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\")) \n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")\nreturn _df_order\n</code></pre> <ol> <li>Conduct row-based and query-based data quality checks only on the source and target dataset, while skipping the aggregate     data quality checks on the validated dataset</li> <li>Enabled the query data quality checks</li> </ol>"},{"location":"examples/#example-5","title":"Example 5","text":"<pre><code>@se.with_expectations(  # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=True, \nagg_dq={ # (10)!\nuser_config.user_configse_agg_dq: True, \nuser_config.se_source_agg_dq: True,\nuser_config.se_final_agg_dq: False, # (2)!\n}, \n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>Conduct row-based and aggregate-based data quality checks only on the source dataset, while skipping the aggregate     data quality checks on the validated dataset</li> <li>Disabled the final aggregate data quality quality checks</li> </ol>"},{"location":"examples/#example-6","title":"Example 6","text":"<pre><code>import os\n@se.with_expectations(\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nspark_conf=se_global_spark_Conf, # (2)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>There are four types of notifications: notification_on_start, notification_on_completion, notification_on_fail and notification_on_error_threshold_breach.     Enable notifications for all four stages by setting the values to <code>True</code></li> <li>To provide the absolute file path for a configuration variable that holds information regarding notifications, use the    decalared global variable, <code>se_global_spark_Conf</code></li> </ol>"},{"location":"examples/#example-7","title":"Example 7","text":"<pre><code>@se.with_expectations( # (1)!\nse.reader.get_rules_from_table(  \nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\", \ntarget_table_name=\"pilot_nonpub.customer_order\", \ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nrow_dq=False, \nagg_dq={ \nuser_config.se_agg_dq: False,\nuser_config.se_source_agg_dq: False, \nuser_config.se_final_agg_dq: True,\n},  \nquery_dq={ \nuser_config.se_query_dq: False,\nuser_config.se_source_query_dq: True,\nuser_config.se_final_query_dq: True,\nuser_config.se_target_table_view: \"order\", \n},\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>Below combination of <code>row_dq, agg_dq, source_agg_dq, final_agg_dq, query_dq, source_query_dq and final_query_dq</code> skips the data quality checks because     source_agg_dq depends on agg_dq and final_agg_dq depends on row_dq and agg_dq</li> </ol>"},{"location":"examples/#example-8","title":"Example 8","text":"<pre><code>@se.with_expectations( # (1)!\nse.reader.get_rules_from_table(\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",\ntarget_table_name=\"pilot_nonpub.customer_order\",\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\",\nactions_if_failed=[\"drop\", \"ignore\"]  # (1)!\n)\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order \n</code></pre> <ol> <li>By default <code>action_if_failed</code> contains [\"fail\", \"drop\", \"ignore\"], but if we want to run only rules which has a     particular action then we can pass them as list shown in the example</li> </ol>"},{"location":"examples/#example-9","title":"Example 9","text":"<pre><code>@se.with_expectations( # (1)!\nse.reader.get_rules_from_table(\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",\ntarget_table_name=\"pilot_nonpub.customer_order\",\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\",\nactions_if_failed=[\"drop\", \"ignore\"]  # (1)!\n),\nrow_dq=True,  # (2)!\nagg_dq={ \nuser_config.se_agg_dq: True, \nuser_config.se_source_agg_dq: True,\nuser_config.se_final_agg_dq: True, \n},  \nquery_dq={ \nuser_config.se_query_dq: True,\nuser_config.se_source_query_dq: True,\nuser_config.se_final_query_dq: True,\nuser_config.se_target_table_view: \"order\", \n}\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\") \n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\")) \n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\") \nreturn _df_order\n</code></pre> <ol> <li>The default options for the action_if_failed field are [\"fail\", \"drop\", or \"ignore\"], but you can specify which of     these actions to run by providing a list of the desired actions in the example when selecting which data quality rules     set to apply</li> <li>Data quality rules will only be applied if they have [\"drop\" or \"ignore\"] specified in the action_if_failed field</li> </ol>"},{"location":"examples/#example-10","title":"Example 10","text":"<pre><code>@se.with_expectations(\nse.reader.get_rules_from_table(\nproduct_rules_table=\"pilot_nonpub.dq.dq_rules\",\ntarget_table_name=\"pilot_nonpub.customer_order\",\ndq_stats_table_name=\"pilot_nonpub.dq.dq_stats\"\n),\nspark_conf={\"spark.files.maxPartitionBytes\": \"134217728\"},  # (1)!\noptions={\"mode\": \"overwrite\", \"partitionBy\": \"order_month\", \n\"overwriteSchema\": \"true\"},  # (2)!\noptions_error_table={\"partition_by\": \"id\"}  # (3)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\nreturn _df_order\n</code></pre> <ol> <li>Provide the optional <code>spark_conf</code> if needed, this is used while writing the data into the <code>final</code> and <code>error</code> table along with notification related configurations</li> <li>Provide the optional <code>options</code> if needed, this is used while writing the data into the <code>final</code> table</li> <li>Provide the optional <code>options_error_table</code> if needed, this is used while writing the data into the <code>error</code> table</li> </ol>"},{"location":"api/kafka_sink_plugin/","title":"Kafka_Sink_plugin","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer-attributes","title":"Attributes","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer-classes","title":"Classes","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl","title":"<code>spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl</code>","text":"<p>             Bases: <code>SparkExpectationsSinkWriter</code></p> <p>class helps to write the stats data into the NSP</p>"},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl-functions","title":"Functions","text":""},{"location":"api/kafka_sink_plugin/#spark_expectations.sinks.plugins.kafka_writer.SparkExpectationsKafkaWritePluginImpl.writer","title":"<code>writer(_write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]) -&gt; None</code>","text":"<p>The functions helps to write data into the kafka topic</p> <p>Parameters:</p> Name Type Description Default <code>_write_args</code> <code>Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]</code> required Source code in <code>spark_expectations/sinks/plugins/kafka_writer.py</code> <pre><code>@spark_expectations_writer_impl\ndef writer(\nself, _write_args: Dict[Union[str], Union[str, bool, Dict[str, str], DataFrame]]\n) -&gt; None:\n\"\"\"\n    The functions helps to write data into the kafka topic\n    Args:\n        _write_args:\n    Returns:\n    \"\"\"\ntry:\n# kafka_options = {\n#     \"kafka.bootstrap.servers\": \"localhost:9092\",\n#     \"topic\": _write_args.get(\"nsp_topic_name\"),\n#     \"failOnDataLoss\": \"true\",\n# }\nif _write_args.pop(\"enable_se_streaming\"):\n_log.info(\"started write stats data into nsp stats topic\")\ndf: DataFrame = _write_args.get(\"stats_df\")\ndf.selectExpr(\"to_json(struct(*)) AS value\").write.format(\"kafka\").mode(\n\"append\"\n).options(**_write_args.get(\"kafka_write_options\")).save()\n_log.info(\"ended writing stats data into nsp stats topic\")\nexcept Exception as e:\nraise SparkExpectationsMiscException(\nf\"error occurred while saving data into NSP {e}\"\n)\n</code></pre>"},{"location":"configurations/adoption_versions_comparsion/","title":"comparision","text":"<p>Please find the difference in the changes with different version, latest three versions changes are documented</p>"},{"location":"configurations/adoption_versions_comparsion/#modifications-made-to-the-version-during-implementation-or-integration","title":"Modifications made to the version during implementation or integration","text":"stage 0.6.0 0.7.0 0.8.0 rules table schema changes refer rule table creation here added three additional column  1.<code>enable_for_source_dq_validation(boolean)</code>  2.<code>enable_for_target_dq_validation(boolean)</code>  3.<code>is_active(boolean)</code>  documentation found here added additional two column  1.<code>enable_error_drop_alert(boolean)</code>  2.<code>error_drop_thresholdt(int)</code>  documentation found here rule table creation required yes yes - creation not required if you're upgrading from old version but schema changes required yes - creation not required if you're upgrading from old version but schema changes required stats table schema changes refer rule table creation here added additional columns  1. <code>source_query_dq_results</code>  2. <code>final_query_dq_results</code>  3. <code>row_dq_res_summary</code>  4. <code>dq_run_time</code>  5. <code>dq_rules</code>  renamed columns  1. <code>runtime</code> to <code>meta_dq_run_time</code>  2. <code>run_date</code> to <code>meta_dq_run_date</code>  3. <code>run_id</code> to <code>meta_dq_run_id</code>  documentation found here remains same stats table creation required yes yes - creation not required if you're upgrading from old version but schema changes required automated notification config setting define global notification param, register as env variable and place in the <code>__init__.py</code> file for multiple usage, example Define a global notification parameter in the <code>__init__.py</code> file to be used in multiple instances where the spark_conf parameter needs to be passed within the with_expectations function. example remains same secret store and kafka authentication details not applicable not applicable Create a dictionary that contains your secret configuration values and register in <code>__init__.py</code> for multiple usage, example spark expectations initialisation create SparkExpectations class object using the <code>SparkExpectations</code> library and by passing the <code>product_id</code> create spark expectations class object using <code>SpakrExpectations</code> by passing <code>product_id</code> and optional parameter <code>debugger</code> example create spark expectations class object using <code>SpakrExpectations</code> by passing <code>product_id</code> and additional optional parameter <code>debugger</code>, <code>stats_streaming_options</code> example spark expectations decorator The decorator allows for configuration by passing individual parameters to each decorator. However, registering a DataFrame view within a decorated function is not supported for implementations of query_dq example The decorator allows configurations to be logically grouped through a dictionary passed as a parameter to the decorator. Additionally, registering a DataFrame view within a decorated function is supported for implementations of query_dq. example remains same"},{"location":"configurations/configure_rules/","title":"Configure_Rules","text":""},{"location":"configurations/configure_rules/#configure-rules-in-the-catalogschemaproduct_rules","title":"Configure Rules in the <code>catalog</code>.<code>schema</code>.<code>{product}_rules</code>","text":"<p>Please find the data set which used for the data quality rules setup order.csv</p>"},{"location":"configurations/configure_rules/#example-of-row-aggregation-and-query-rules-for-data-quality","title":"Example Of Row, Aggregation And Query Rules For Data Quality","text":"<p>To perform row data quality checks for artificially order table, please set up rules using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description, enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active) values\n--The row data qulaity has been set on customer_id when customer_id is null, drop respective row into error table \n--as \"action_if_failed\" tagged \"drop\"\n('apla_nd', '`catalog`.`schema`.customer_order',  'row_dq', 'customer_id_is_not_null', 'customer_id', 'customer_id is not null','drop', 'validity', 'customer_id ishould not be null', false, false, true)\n--The row data qulaity has been set on sales when sales is less than zero, drop respective row into error table as \n--'action_if_failed' tagged \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'sales_greater_than_zero', 'sales', 'sales &gt; 0', 'drop', 'accuracy', 'sales value should be greater than zero', false, false, true)\n--The row data qulaity has been set on discount when discount is less than 60, drop respective row into error table\n--and final table  as \"action_if_failed\" tagged 'ignore'\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'discount_threshold', 'discount', 'discount*100 &lt; 60',\n'ignore', 'validity', 'discount should be less than 40', false, false, true)\n--The row data qulaity has been set on ship_mode when ship_mode not in (\"second class\", \"standard class\", \n--\"standard class\"), drop respective row into error table and fail the framewok  as \"action_if_failed\" tagged \"fail\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'ship_mode_in_set', 'ship_mode', 'lower(trim(ship_mode))\nin('second class', 'standard class', 'standard class')', 'fail', 'validity', 'ship_mode mode belongs in the sets',\nfalse, false, true)\n--The row data qulaity has been set on profit when profit is less than or equals to 0, drop respective row into \n--error table and final table as \"action_if_failed\" tagged \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'profit_threshold', 'profit', 'profit&gt;0', 'ignore', 'validity', 'profit threshold should be greater tahn 0', false, false, true)\n--The rule has been established to identify and remove completely identical records in which rows repeat with the \n--same value more than once, while keeping one instance of the row. Any additional duplicated rows will be dropped \n--into error table as action_if_failed set to \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'complete_duplicate', 'All', 'row_number() \n over(partition by cutomer_id, order_id order by 1)=1', 'drop', 'uniqueness', 'drop complete duplicate records', false, false, true)\n</code></pre> <p>Please set up rules for checking the quality of the columns in the artificially order table, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description,  enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active) values\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales values falls below 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales', 'sales', 'sum(sales)&gt;10000', 'ignore', 'validity', 'sum of sales must be greater than 10000',  true, true, true)\n--The aggregation rule is established on the 'ship_mode' column and the metadata of the rule will be captured in \n--the statistics table when distinct ship_mode greater than 3 and enabled for only source data set\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'distinct_of_ship_mode', 'ship_mode', 'count(distinct ship_mode)&lt;=3', 'ignore', 'validity', 'regex format validation for quantity', true, false, true)\n-- The aggregation rule is established on the table countand the metadata of the rule will be captured in the \n--statistics table when distinct count greater than 10000 and failes the job as \"action_if_failed\" set to \"fail\" \n--and enabled only for validated datset\n,('apla_nd', '`catalog`.`schema`..customer_order', 'agg_dq', 'row_count', '*', 'count(*)&gt;=10000', 'fail', 'validity',\n'distinct ship_mode must be less or equals to 3', false, true, true)\n</code></pre> <p>Please set up rules for checking the quality of artificially order table by implementing query data quality option, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description) values\n--The query dq rule is established to check product_id differemce between two table if differnce is more than 20% \n--from source table, the metadata of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_missing_count_threshold', '*', '((select count(distinct product_id) from product) - (select count(distinct product_id) from order))&gt;\n(select count(distinct product_id) from product)*0.2', 'ignore', 'validity', 'row count threshold difference msut \nbe less than 20%', true, true, true)\n--The query dq rule is established to check distinct proudtc_id in the product table is less than 5, if not the \n--metadata of the rule will be captured in the statistics table along with fails the job as \"action_if_failed\" is \n--\"fail\" and enabled for source dataset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_category', '*', '(select count(distinct category) \nfrom product) &lt; 5', 'fail', 'validity', 'distinct product category must be less than 5', true, False, true)\n--The query dq rule is established to check count of the dataset should be less than 10000 other wise the metadata \n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled only for target datset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'row_count_in_order', '*', '(select count(*) from order)&lt;10000', 'ignore', 'accuracy', 'count of the row in order dataset must be less then 10000', false, true, true)\n</code></pre>"},{"location":"configurations/databricks_setup_guide/","title":"Databricks setup guide","text":""},{"location":"configurations/databricks_setup_guide/#effortlessly-explore-spark-expectations-on-example-dataset-with-automated-setup-in-databricks","title":"Effortlessly Explore Spark Expectations on Example Dataset with Automated Setup in Databricks","text":"<p>This section provides instructions on how to set up a sample notebook in the Databricks environment to investigate, comprehend, and conduct a feasibility study on the Spark Expectations framework.</p>"},{"location":"configurations/databricks_setup_guide/#prerequisite","title":"Prerequisite:","text":"<ol> <li>Recommended databricks run time environment for better experience - DBS 11.0 and above</li> <li>Please install the kafka jar using the path <code>dbfs:/kafka-jars/databricks-shaded-strimzi-kafka-oauth-client-1.1.jar</code>, If the jar is not available in the dbfs location, please raise a ticket with GAP Support team to add the jar to your workspace</li> <li>Please follow the steps provided here to integrate and clone repo from git databricks</li> <li>Please follow the steps to create the wbhook-hook URL for team-specific channel here</li> </ol>"},{"location":"configurations/rules/","title":"Rules","text":""},{"location":"configurations/rules/#different-types-of-expectations","title":"Different Types of Expectations","text":"<p>Please find the different types of possible expectations </p>"},{"location":"configurations/rules/#possible-row-data-quality-expectations","title":"Possible Row Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect that the values in the column should not be null/empty null_validation completeness <code>[col_name] is not null</code> Ensure that the primary key values are unique and not duplicated primary_key_validation uniqueness <code>count(*) over(partition by [primary_key_or_combination_of_primary_key] order by 1)=1</code> Perform a thorough check to make sure that there are no duplicate values, if there are duplicates preserve one row into target complete_duplicate_validation uniqueness <code>row_number() over(partition by [all_the_column_in_dataset_b_ comma_separated] order by 1)=1</code> Verify that the date values are in the correct format date_format_validation validity <code>to_date([date_col_name], '[mention_expected_date_format]') is not null</code> Verify that the date values are in the correct format using regex date_format_validation_with_regex validity <code>[date_col_name] rlike '[regex_format_of_date]'</code> Expect column value is date parseable expect_column_values_to_be_date_parseable validity <code>try_cast([date_col_name] as date)</code> Verify values in a column to conform to a specified regular expression pattern expect_column_values_to_match_regex validity <code>[col_name]  rlike '[regex_format]'</code> Verify values in a column to not conform to a specified regular expression pattern expect_column_values_to_not_match_regex validity <code>[col_name] not rlike '[regex_format]'</code> Verify values in a column to match regex in list expect_column_values_to_match_regex_list validity <code>[col_name] not rlike '[regex format1]' or [col_name] not rlike '[regex_format2]' or [col_name] not rlike '[regex_format3]'</code> Expect the values in a column to belong to a specified set expect_column_values_to_be_in_set accuracy <code>[col_name] in ([values_in_comma_separated])</code> Expect the values in a column not to belong to a specified set expect_column_values_to_be_not_in_set accuracy <code>[col_name] not in ([values_in_comma_separated])</code> Expect the values in a column to fall within a defined range expect_column_values_to_be_in_range accuracy <code>[col_name] between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be within a specified range expect_column_value_lengths_to_be_between accuracy <code>length([col_name]) between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be equal to a certain value expect_column_value_lengths_to_be_equal accuracy <code>length([col_name])=[threshold]</code> Expect values in the column to exceed a certain limit expect_column_value_to_be_greater_than accuracy <code>[col_name] &gt; [threshold_value]</code> Expect values in the column  not to exceed a certain limit expect_column_value_to_be_lesser_than accuracy <code>[col_name] &lt; [threshold_value]</code> Expect values in the column to be equal to or exceed a certain limit expect_column_value_greater_than_equal accuracy <code>[col_name] &gt;= [threshold_value]</code> Expect values in the column to be equal to or not exceed a certain limit expect_column_value_lesser_than_equal accuracy <code>[col_name] &lt;= [threshold_value]</code> Expect values in column A to be greater than values in column B expect_column_pair_values_A_to_be_greater_than_B accuracy <code>[col_A] &gt; [col_B]</code> Expect values in column A to be lesser than values in column B expect_column_pair_values_A_to_be_lesser_than_B accuracy <code>[col_A] &lt; [col_B]</code> Expect values in column A to be greater than or equals to values in column B expect_column_A_to_be_greater_than_B accuracy <code>[col_A] &gt;= [col_B]</code> Expect values in column A to be lesser than or equals to values in column B expect_column_A_to_be_lesser_than_or_equals_B accuracy <code>[col_A] &lt;= [col_B]</code> Expect the sum of values across multiple columns to be equal to a certain value expect_multicolumn_sum_to_equal accuracy <code>[col_1] + [col_2] + [col_3] = [threshold_value]</code> Expect sum of values in each category equals certain value expect_sum_of_value_in_subset_equal accuracy <code>sum([col_name]) over(partition by [category_col] order by 1)</code> Expect count of values in each category equals certain value expect_count_of_value_in_subset_equal accuracy <code>count(*) over(partition by [category_col] order by 1)</code> Expect distinct value in each category exceeds certain range expect_distinct_value_in_subset_exceeds accuracy <code>count(distinct [col_name]) over(partition by [category_col] order by 1)</code>"},{"location":"configurations/rules/#possible-aggregation-data-quality-expectations","title":"Possible Aggregation Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column that are present in a given list expect_column_distinct_values_to_be_in_set accuracy <code>array_intersect(collect_list(distinct [col_name]), Array($compare_values_string)) = Array($compare_values_string)</code> Expect the mean value of a column to fall within a specified range expect_column_mean_to_be_between consistency <code>avg([col_name]) between [lower_bound] and [upper_bound]</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>percentile_approx([column_name], 0.5) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>stddev([col_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>count(distinct [col_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>max([col_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_sum_to_be_between accuracy <code>min([col_name]) between [lower_bound] and [upper_bound]</code> Expect row count of the dataset fall within certain range expect_row_count_to_be_between accuracy <code>count(*) between [lower_bound] and [upper_bound]</code>"},{"location":"configurations/rules/#possible-query-data-quality-expectations","title":"Possible Query Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column must be greater than threshold value expect_column_distinct_values_greater_than_threshold_value accuracy <code>(select count(distinct [col_name]) from [table_name]) &gt; [threshold_value]</code> Expect count between two table or view must be same expect_count_between_two_table_same consistency <code>(select count(*) from [table_a]) = (select count(*) from [table_b])</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>(select percentile_approx([column_name], 0.5) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>(select stddev([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>(select count(distinct [col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>(select max([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_min_to_be_between accuracy <code>(select min([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect referential integrity expect_referential_integrity_between_two_table_should_be_less_than_100 accuracy <code>with refrentail_check as ( select * from [table_a] left join [table_b] on [condition] where [table_b.column] is null) select count(*) from refrentail_check) &lt; 100</code>"},{"location":"getting-started/setup/","title":"Setup","text":""},{"location":"getting-started/setup/#installation","title":"Installation","text":"<p>The library is available in the pypi and can be installed in your environment using the below command or  please add the library \"spark-expectations\" into the requirements.txt or poetry dependencies.</p> <pre><code>pip install -U spark-expectations\n</code></pre>"},{"location":"getting-started/setup/#required-tables","title":"Required Tables","text":"<p>There are two tables that need to be created for spark-expectations to run seamlessly and integrate with a spark job. The below sql statements used three namespaces which works with Databricks UnityCatalog, but if you are using hive please update the namespaces accordingly and also provide necessary table metadata.</p>"},{"location":"getting-started/setup/#rules-table","title":"Rules Table","text":"<p>We need to create a rules tables which contains all the data quality rules. Please use the below template to create your rules table for your project.</p> <pre><code>create table if not exists `catalog`.`schema`.`{product}_rules` (\nproduct_id STRING,\ntable_name STRING,\nrule_type STRING,\nrule STRING,\ncolumn_name STRING,\nexpectation STRING,\naction_if_failed STRING,\ntag STRING,\ndescription STRING,\nenable_for_source_dq_validation BOOLEAN, enable_for_target_dq_validation BOOLEAN,\nis_active BOOLEAN,\nenable_error_drop_alert BOOLEAN,\nerror_drop_threshold INT\n);\n</code></pre>"},{"location":"getting-started/setup/#rule-type-for-rules","title":"Rule Type For Rules","text":"<p>The rules column has a column called \"rule_type\". It is important that this column should only accept one of  these three values - <code>[row_dq, agg_dq, query_dq]</code>. If other values are provided, the library may cause unforseen errors. Please run the below command to add constriants to the above created rules table</p> <pre><code>ALTER TABLE `catalog`.`schema`.`{product}_rules` ADD CONSTRAINT rule_type_action CHECK (rule_type in ('row_dq', 'agg_dq', 'query_dq'));\n</code></pre>"},{"location":"getting-started/setup/#action-if-failed-for-row-aggregation-and-query-data-quality-rules","title":"Action If Failed For Row, Aggregation and Query Data Quality Rules","text":"<p>The rules column has a column called \"action_if_failed\". It is important that this column should only accept one of  these values - <code>[fail, drop or ignore]</code> for <code>'rule_type'='row_dq'</code> and <code>[fail, ignore]</code> for <code>'rule_type'='agg_dq' and 'rule_type'='query_dq'</code>.  If other values are provided, the library may cause unforseen errors. Please run the below command to add constriants to the above created rules table</p> <pre><code>ALTER TABLE apla_nd_dq_rules ADD CONSTRAINT action CHECK ((rule_type = 'row_dq' and action_if_failed IN ('ignore', 'drop', 'fail')) or (rule_type = 'agg_dq' and action_if_failed in ('ignore', 'fail')) or (rule_type = 'query_dq' and action_if_failed in ('ignore', 'fail')));\n</code></pre>"},{"location":"getting-started/setup/#dq-stats-table","title":"DQ Stats Table","text":"<p>In order to collect the stats/metrics for each data quality job run, the spark-expectations job will automatically create the stats table if it does not exist. The below sql statement can be used to create the table if you want to create it manually, but it is not recommended.</p> <pre><code>create table if not exists `catalog`.`schema`.`dq_stats` (\nproduct_id STRING,\ntable_name STRING,\ninput_count LONG,\nerror_count LONG,\noutput_count LONG,\noutput_percentage FLOAT,\nsuccess_percentage FLOAT,\nerror_percentage FLOAT,\nsource_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,\nfinal_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,\nsource_query_dq_results array&lt;map&lt;string, string&gt;&gt;,\nfinal_query_dq_results array&lt;map&lt;string, string&gt;&gt;,\nrow_dq_res_summary array&lt;map&lt;string, string&gt;&gt;,\nrow_dq_error_threshold array&lt;map&lt;string, string&gt;&gt;,\ndq_status map&lt;string, string&gt;,\ndq_run_time map&lt;string, float&gt;,\ndq_rules map&lt;string, map&lt;string,int&gt;&gt;,\nmeta_dq_run_id STRING,\nmeta_dq_run_date DATE,\nmeta_dq_run_datetime TIMESTAMP\n);\n</code></pre>"}]}