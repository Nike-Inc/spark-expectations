{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43cd9fb9-5629-43ca-bb16-e4e22fdabea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"color:#fff; background:#0070c0; font-weight:bold; border:2px solid #0070c0; padding:10px; border-radius:6px;\">\n",
    "**Purpose:** This notebook demonstrates how to use <span style=\"color:yellow;\">Spark Expectations</span> with <span style=\"color:yellow;\"> PagerDuty by creating incidents`</span>.<br>\n",
    "Its main focus is to show how data quality alerts and results can create incidents for PagerDuty.</span>\n",
    "</div>\n",
    "\n",
    "### Spark - Expectations - User - Guide - Documentation\n",
    "\n",
    "<div style=\"color:red; font-weight:bold; border:2px solid red; padding:8px;\">\n",
    "⚠️ ALERT: Notebook meant to be ran by spinning up local docker compose (containers/compose.yaml) !\n",
    "</div>\n",
    "\n",
    "* Please read through the [Spark Expectation Documentation](https://engineering.nike.com/spark-expectations) before proceeding with this demo\n",
    "\n",
    "#### widgets \n",
    "* `catalog`, `schema` - leave default values \n",
    "  * Tables are going to be prefixed with value provided in user widget text field\n",
    "\n",
    "<div style=\"color:orange; font-weight:regular; border:1px solid orange; padding:8px;\">\n",
    "⚠️ Container comes with SparkExpectation by default. If SE version is overriden Kernel will need to be restarted!\n",
    "</div>\n",
    "\n",
    "* `Override SE version` check box to install different SparkExpectation library version\n",
    "* `library_source` combo box defines library url(git branch or pypi) from where to pull library \n",
    "  * `pypi` ( installs latest published version available in PyPi)\n",
    "  * `git` ( installs library from specified git branch)\n",
    "    * Set `git_branch` input field to match git branch (example `main`)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d038a7-c093-4166-a260-684dcd53b38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Method 1: Use Integration Key for Notebook Testing\n",
    "\n",
    "This is used by default for the example notebook. This will just use the key throughout the notebook for a quick and easy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c75c751-6241-4ebf-a2b5-d31f505501cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# create secret scope for Databricks and store it (check afterwards if secret can be pulled)\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.errors import PySparkException\n",
    "w = WorkspaceClient()\n",
    "w.secrets.list_scopes()\n",
    "\n",
    "scope=\"pagerduty\"\n",
    "# enter in your pagerduty integration key to use in notebook (or retrieve from workspace)\n",
    "integration_key=\"your_integration_key\"\n",
    "\n",
    "try:\n",
    "    w.secrets.create_scope(scope=scope)\n",
    "except PySparkException as pyex:\n",
    "    print(f\"PysparkException: {pyex}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")\n",
    "\n",
    "w.secrets.put_secret(scope=scope, key=integration_key, string_value=\"pagerduty_secret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78972fce-0b3c-491a-a64b-fd3604529d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Widget Setup\n",
    "\n",
    "Widgets used in this notebook will be created and then set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "42b3c7ee-67b8-465b-bc56-05548000738b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GENERATE INPUT WIDGETS \n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "widget_user = widgets.Text(\n",
    "    value='user',\n",
    "    placeholder='Type something',\n",
    "    description='user: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_git_org = widgets.Text(\n",
    "    value='catalog_name',\n",
    "    placeholder='Type something',\n",
    "    description='git_org ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_catalog = widgets.Text(\n",
    "    value='development',\n",
    "    placeholder='Type something',\n",
    "    description='catalog:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_schema = widgets.Text(\n",
    "    value='team_name',\n",
    "    placeholder='Type something',\n",
    "    description='schema:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_library_source = widgets.Combobox(\n",
    "    placeholder='Choose source',\n",
    "    options=['pypi', 'git'],\n",
    "    description='library_source:',\n",
    "    ensure_option=True,\n",
    "    value='git',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_git_branch_or_commit = widgets.Text(\n",
    "    value='main',\n",
    "    placeholder='Type branch name or commit hash',\n",
    "    description='git_branch_or_commit:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "widget_override_version = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Override SE version',\n",
    "    disabled=False,\n",
    "    style={'description_width': '30px'}\n",
    "    \n",
    ")\n",
    "\n",
    "hbox = widgets.HBox([\n",
    "    widget_user,\n",
    "    widget_catalog, \n",
    "    widget_schema,\n",
    "    widget_override_version, \n",
    "    widget_library_source, \n",
    "    widget_git_org,\n",
    "    widget_git_branch_or_commit\n",
    "])\n",
    "\n",
    "# Display widgets\n",
    "display(hbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e67bd270-cfa0-401e-a7c6-030df9075909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "user = re.sub(r'[^a-zA-Z]', '', widget_user.value).lower()\n",
    "catalog = widget_catalog.value\n",
    "schema = widget_schema.value\n",
    "override_se_version = widget_override_version.value\n",
    "library = widget_library_source.value\n",
    "org = widget_git_org.value\n",
    "branch_or_commit = widget_git_branch_or_commit.value\n",
    "\n",
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": \"development\",\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_product\",\n",
    "    \"in_memory_source\": f\"se_{user}_source\",\n",
    "    \"rules_table\": f\"development.{schema}.se_{user}_rules\",\n",
    "    \"stats_table\": f\"development.{schema}.se_{user}_stats\",\n",
    "    \"target_table\": f\"development.{schema}.se_{user}_target\",\n",
    "    \"override_se_version\" : override_se_version,\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a657107-2149-4ee5-a658-e2cab6f29384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Install Spark Expectation\n",
    "\n",
    "If Running from local container it will come with latest spark-expectation library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "814610f4-8611-45dd-b4f5-eaaeae2cbf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Override Spark Expectations based on user input\n",
    "if override_se_version:\n",
    "    print(\"-----OVERRIDING SPARK-EXPECTATIONS VERSION\")\n",
    "    if CONFIG[\"library\"] == \"pypi\":\n",
    "      print(\"-----INSTALLING SPARK-EXPECTATIONS from PyPi\")\n",
    "      %pip install spark-expectations\n",
    "    elif CONFIG[\"library\"] == \"git\":\n",
    "      print(f\"-----INSTALLING SPARK-EXPECTATIONS from Git Org/User {CONFIG['org']}, Branch/Commit {CONFIG['branch_or_commit']}\")\n",
    "      giturl = f\"git+https://github.com/{CONFIG['org']}/spark-expectations.git@{CONFIG['branch_or_commit']}\"\n",
    "      %pip install --force-reinstall {giturl}    \n",
    "else:\n",
    "    print(f\"---- Using SparkExpectation from local codebase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "14d01585-7422-4f35-acdf-802297d1767b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CREATE SPARK SESSION AND DATABASE\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Example\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "74b9d5a8-468f-4610-bac1-c95fa0fd2bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show(truncate=False)\n",
    "\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0a64a095-30ed-496a-9b83-6acfbdf5d8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE {CONFIG['catalog']}.{CONFIG['schema']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"] ]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"----- No tables to drop\")\n",
    "\n",
    "# Drop global and local temp views matching pattern\n",
    "\n",
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"----- No views to drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "21e1f179-7704-48a4-b313-00871e3ce303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting Started with Spark Expectations: Simple Example\n",
    "\n",
    "## 1. Sample Source Dataset\n",
    "# initialize simple Pandas DataFrame and convert it to a Spark DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "## 2. Define Simple `row_dq` Rules\n",
    "# Create a rules DataFrame with a few simple data quality rules\n",
    "\n",
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_not_null\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age IS NOT NULL\",\n",
    "        \"action_if_failed\": \"warn\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Age must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_adult\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age < 20\",\n",
    "        \"action_if_failed\": \"ignore\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Age must be less than 20\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "        {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"email_not_null\",\n",
    "        \"column_name\": \"email\",\n",
    "        \"expectation\": \"email IS NOT NULL\",\n",
    "        \"action_if_failed\": \"drop\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Email must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    }\n",
    "\n",
    "    \n",
    "]\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "rules_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "\n",
    "rules_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecfb45b-fae8-4459-9531-b7cdf99c2ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notification Configurations for PagerDuty\n",
    "\n",
    "This is where you can set the integration key with the following ways:\n",
    "- setting the integration key manually\n",
    "- using databricks secret storage (assuming you have secrets stored within your notebook)\n",
    "- using cerberus secrets storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a40bdd-9ff4-461c-9e6b-5180bf7531aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure streaming and notification configuration\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "from dbruntime.databricks_repl_context import get_context\n",
    "\n",
    "# This is a dictionary that can be used to configure Spark Expectations behavior and override default settings.\n",
    "stats_streaming_config_dict = {\n",
    "    user_config.se_enable_streaming: False,\n",
    "}\n",
    "\n",
    "dbx_workspace_id = get_context().workspaceId\n",
    "dbx_workspace_url = get_context().browserHostName\n",
    "\n",
    "user_conf_dict = {\n",
    "    user_config.se_notifications_enable_pagerduty: True,\n",
    "    # PagerDuty Configuration - where you supply the integration key for your PD service\n",
    "    user_config.se_notifications_pagerduty_integration_key: integration_key,\n",
    "    user_config.se_notifications_pagerduty_webhook_url: \"https://events.pagerduty.com/v2/enqueue\",\n",
    "\n",
    "    # Fill in your cbs_url + cbs_sdb_path on where your secret is stored.\n",
    "    # user_config.secret_type: \"cerberus\",\n",
    "    # user_config.cbs_url: \"https://cerberus.com\",\n",
    "    # user_config.cbs_sdb_path: \"app/your/sdb/path\",\n",
    "\n",
    "    # Fill in your workspace url + secret scope for Databricks.\n",
    "    # user_config.secret_type: \"databricks\",\n",
    "    # user_config.dbx_workspace_url: dbx_workspace_url,\n",
    "    # user_config.dbx_secret_scope: \"pagerduty\",\n",
    "\n",
    "    # Notification triggers\n",
    "    user_config.se_notifications_on_start: False,\n",
    "    user_config.se_notifications_on_completion: False,\n",
    "    user_config.se_notifications_on_fail: False,\n",
    "    user_config.se_notifications_on_error_drop_exceeds_threshold_breach: False,\n",
    "    user_config.se_notifications_on_rules_action_if_failed_set_ignore: False,\n",
    "    user_config.se_notifications_on_error_drop_threshold: 1,\n",
    "}\n",
    "\n",
    "display(user_conf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6618e1-a708-4d82-b3cc-819d9c68a269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 3. Run Spark Expectations\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from spark_expectations.core import load_configurations\n",
    "\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "\n",
    "\n",
    "writer = WrappedDataFrameWriter().mode(\"overwrite\").format(\"delta\")\n",
    "\n",
    "\n",
    "# Initialize Default Config \n",
    "load_configurations(spark) \n",
    "\n",
    "\"\"\"\n",
    "This class implements/supports running the data quality rules on a dataframe returned by a function\n",
    "\n",
    "Args:\n",
    "    product_id: Name of the product\n",
    "    rules_df: DataFrame which contains the rules. User is responsible for reading\n",
    "        the rules_table in which ever system it is\n",
    "    stats_table: Name of the table where the stats/audit-info need to be written\n",
    "    debugger: Mark it as \"True\" if the debugger mode need to be enabled, by default is False\n",
    "    stats_streaming_options: Provide options to override the defaults, while writing into the stats streaming table\n",
    "\"\"\"\n",
    "se = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "#  Initialize input data\n",
    "data = [\n",
    "    {\"id\": 1, \"age\": 19,   \"email\": \"alice@example.com\"},\n",
    "    {\"id\": 2, \"age\": 17,   \"email\": \"bob@example.com\"},\n",
    "    {\"id\": 3, \"age\": None, \"email\": \"charlie@example.com\"},\n",
    "    {\"id\": 4, \"age\": 40,   \"email\": \"mike@example.com\"},\n",
    "    {\"id\": 5, \"age\": None, \"email\": \"ron@example.com\"},\n",
    "    {\"id\": 6, \"age\": 35,   \"email\": None},\n",
    "]\n",
    "input_df = spark.createDataFrame(pd.DataFrame(data))\n",
    "input_df.show(truncate=False)\n",
    "\n",
    "\"\"\"\n",
    "This decorator helps to wrap a function which returns dataframe and apply dataframe rules on it\n",
    "\n",
    "Args:\n",
    "    target_table: Name of the table where the final dataframe need to be written\n",
    "    write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n",
    "    write_to_temp_table: Mark it as \"True\" if the input dataframe need to be written to the temp table to break\n",
    "                        the spark plan\n",
    "    user_conf: Provide options to override the defaults, while writing into the stats streaming table\n",
    "    target_table_view: This view is created after the _row_dq process to run the target agg_dq and query_dq.\n",
    "        If value is not provided, defaulted to {target_table}_view\n",
    "    target_and_error_table_writer: Provide the writer to write the target and error table,\n",
    "        this will take precedence over the class level writer\n",
    "\n",
    "Returns:\n",
    "    Any: Returns a function which applied the expectations on dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@se.with_expectations(\n",
    "    target_table=CONFIG[\"target_table\"],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=True,\n",
    "    user_conf=user_conf_dict,\n",
    ")\n",
    "def get_dataset():\n",
    "    _df_source: DataFrame = input_df\n",
    "    _df_source.createOrReplaceTempView(CONFIG[\"in_memory_source\"])\n",
    "    return _df_source\n",
    "\n",
    "\n",
    "# This will run the DQ checks and raise if any \"fail\" rules are violated\n",
    "get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a066639-70a0-4def-82bc-b6959a16b810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "Checkout `Stats` and `Error` table to see spark-expectations execution results\n",
    "\n",
    "## Notifications\n",
    "At this point, incidents created based on the configurations up above would have been sent out to your PagerDuty service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "284e42a5-a657-45a4-a5f5-79e681b9c1be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_stats_table = f\"SELECT * FROM {CONFIG['stats_table']}\"\n",
    "spark.sql(query_stats_table).show(truncate=False)\n",
    "\n",
    "query_error_table = f\"SELECT * FROM {CONFIG['target_table']}_error\"\n",
    "spark.sql(query_error_table).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "904431c0-a27a-4589-84f7-aabd66d3263c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_target_table = f\"\"\"\n",
    "SELECT *\n",
    "FROM {CONFIG['target_table']} \n",
    "ORDER BY meta_dq_run_id, id\n",
    "\"\"\"\n",
    "\n",
    "final_data_set_df = spark.sql(query_target_table)\n",
    "\n",
    "if final_data_set_df is not None:\n",
    "    final_data_set_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9abe74e-b17d-412d-add2-a7bb0a6d7a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['in_memory_source']}\").collect()[0]['count']\n",
    "output_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['target_table']}\").collect()[0]['count']\n",
    "\n",
    "# Find missing rows in target_table that are present in in_memory_source\n",
    "removed_rows_df = spark.sql(f\"\"\"\n",
    "SELECT s.*\n",
    "FROM {CONFIG['in_memory_source']} s\n",
    "LEFT ANTI JOIN {CONFIG['target_table']} t\n",
    "ON s.id = t.id\n",
    "\"\"\")\n",
    "\n",
    "removed_rows_count = removed_rows_df.count()\n",
    "\n",
    "comparison_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"input\", input_count),\n",
    "        (\"output\", output_count),\n",
    "        (\"removed_records\", removed_rows_count)\n",
    "    ],\n",
    "    [\"table\", \"record_count\"]\n",
    ")\n",
    "\n",
    "comparison_df.show()\n",
    "\n",
    "if removed_rows_count > 0:\n",
    "    removed_rows_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7066481261502431,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_expectation_basic_pagerduty_notification",
   "widgets": {
    "catalog": {
     "currentValue": "development",
     "nuid": "637671b6-02a6-4a44-8284-c0caa98f22c4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "catalog",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "git_branch_or_commit": {
     "currentValue": "main",
     "nuid": "258eef8c-fe48-4025-b131-4a94e75c0e7f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": "",
      "name": "git_branch_or_commit",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "main",
      "label": "",
      "name": "git_branch_or_commit",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "git_org": {
     "currentValue": "Nike-Inc",
     "nuid": "10598b8d-6baf-429b-86a2-13a1b2593d73",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "git_org",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "git_org",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "library_source": {
     "currentValue": "git",
     "nuid": "6d928c35-b7e9-4176-90a9-f26fb906e136",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "git",
      "label": "",
      "name": "library_source",
      "options": {
       "choices": [
        "git",
        "pypi"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "git",
      "label": "",
      "name": "library_source",
      "options": {
       "autoCreated": false,
       "choices": [
        "git",
        "pypi"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "schema": {
     "currentValue": "team_the_league",
     "nuid": "c592e37b-40b6-47ee-8ec2-f901d19962cc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "schema",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
