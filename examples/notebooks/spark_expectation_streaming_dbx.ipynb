{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1e3662-caa0-43f6-9058-cdf1a9ae1826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3b1408-db69-4e94-a4be-c201e1bcea53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Expectations - Streaming Write Guide\n",
    "\n",
    "<div style=\"color:red; font-weight:bold; border:2px solid red; padding:8px;\">\n",
    "‚ö†Ô∏è ALERT: Notebook meant to be ran in Databricks !\n",
    "</div>\n",
    "\n",
    "* Please read through the [Spark Expectation Documentation](https://engineering.nike.com/spark-expectations) and [STREAMWRITE_GUIDE.md](../../STREAMWRITE_GUIDE.md) before proceeding with this demo\n",
    "\n",
    "This notebook demonstrates how to use Spark Expectations with **streaming DataFrames**:\n",
    "- Automatic detection of streaming DataFrames\n",
    "- Proper checkpoint location configuration\n",
    "- Streaming query management\n",
    "- Best practices for production streaming workloads\n",
    "\n",
    "#### Widgets\n",
    "* `catalog`, `schema` - define where output tables are going to be created\n",
    "  * Tables are going to be prefixed with logged in DBX user\n",
    "* `checkpoint_path` - dedicated checkpoint location for streaming queries (REQUIRED for production)\n",
    "* `library_source` combo box defines library url(git branch or pypi) from where to pull library\n",
    "  * `pypi` (installs latest published version available in PyPi)\n",
    "  * `git` (installs library from specified git branch)\n",
    "    * Set `git_branch` input field to match git branch (example `main`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe471f46-0611-4ee0-899e-4df87c412c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# below code to create widgets of database, stats_table_name and table_name\n",
    "dbutils.widgets.text(\"user\", \"user\")\n",
    "dbutils.widgets.text(\"git_org\", \"Nike-Inc\") #name of widget is 'database', default value is 'demo_db'\n",
    "dbutils.widgets.text(\"catalog\", \"development\")\n",
    "dbutils.widgets.text(\"schema\", \"default\")\n",
    "dbutils.widgets.text(\"library_source\", \"git\")\n",
    "dbutils.widgets.text(\"git_branch_or_commit\", \"main\")\n",
    "dbutils.widgets.text(\"override_version\", \"False\")\n",
    "# dbutils.widgets.dropdown(\"database\", \"default\", [database[0] for database in spark.catalog.listDatabases()]) -- limit 1024\n",
    "dbutils.widgets.text(\"in_memory_source\", \"se_rg_source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8c4c07-30f2-490e-9a0e-ad9cfa11a5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialize notebook config\n",
    "This step will read widget values needed to configure Spark-Expectation library installation and define schema, catalog and table naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1907b0d-459f-4dbc-a4ff-b100fcc52f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "logged_in_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "logged_in_user = logged_in_user.split('@')[0]\n",
    "\n",
    "user = re.sub(r'[^a-zA-Z]', '', logged_in_user).lower()\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "library = dbutils.widgets.get(\"library_source\")\n",
    "org = dbutils.widgets.get(\"git_org\")\n",
    "branch_or_commit = dbutils.widgets.get(\"git_branch_or_commit\")\n",
    "checkpoint_base_path = dbutils.widgets.get(\"checkpoint_path\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": catalog,\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_streaming_product\",\n",
    "    \"in_memory_source\": f\"se_{user}_streaming_source\",\n",
    "    \"rules_table\": f\"{catalog}.{schema}.se_{user}_streaming_rules\",\n",
    "    \"stats_table\": f\"{catalog}.{schema}.se_{user}_streaming_stats\",\n",
    "    \"target_table\": f\"{catalog}.{schema}.se_{user}_streaming_target\",\n",
    "    \"checkpoint_path\": checkpoint_base_path if checkpoint_base_path else f\"/tmp/checkpoints/se_{user}_streaming\",\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])\n",
    "display(config_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa977f49-6d7f-4731-b5bd-91ace3f60525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install Required Libraries\n",
    "* Spark Expectations\n",
    "* Jinja2 (required for using custom templates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8630431d-01eb-4ec3-8262-ff221c3f4b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG[\"library\"] == \"pypi\":\n",
    "    print(\"-----INSTALLING SPARK-EXPECTATIONS from PyPI\")\n",
    "    %pip install spark-expectations\n",
    "elif CONFIG[\"library\"] == \"git\":\n",
    "    print(f\"-----INSTALLING SPARK-EXPECTATIONS from Git Org/User {CONFIG['org']}, Branch/Commit {CONFIG['branch_or_commit']}\")\n",
    "    giturl = f\"git+https://github.com/{CONFIG['org']}/spark-expectations.git@{CONFIG['branch_or_commit']}\"\n",
    "    %pip install --force-reinstall {giturl}\n",
    "\n",
    "print(\"-----INSTALLING Jinja2 template library\")\n",
    "%pip install jinja2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e884569-e523-4afa-bdc9-b9444a2e2f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display Spark Expectations installed version\n",
    "from importlib.metadata import version\n",
    "print(f\"---- Current SparkExpectation Version: {version('spark-expectations')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbf6990c-26c4-4c08-a482-940c74802c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cleanup\n",
    "Removing previously created user prefixed tables and checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a23953-8ac9-47a8-9975-4cb31b9b333c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}_streaming*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE CATALOG {CONFIG['catalog']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"]]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"----- No tables to drop\")\n",
    "\n",
    "# Drop views matching pattern\n",
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"----- No views to drop\")\n",
    "\n",
    "# Clean up checkpoint directory\n",
    "print(f\"\\nCleaning up checkpoint directory: {CONFIG['checkpoint_path']}\")\n",
    "dbutils.fs.rm(CONFIG['checkpoint_path'], True)\n",
    "print(\"Checkpoint directory cleaned\")\n",
    "\n",
    "# NOTE: PERSIST TABLE is not supported on serverless compute, so do not use it here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e4e2146-cb22-46f0-bed2-42d4bcba56c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Streaming Data Quality with Spark Expectations\n",
    "\n",
    "### Steps:\n",
    "1. Create a streaming source (using `rate` source for demo)\n",
    "2. Define data quality rules\n",
    "3. Configure Spark Expectations with streaming options\n",
    "4. Apply DQ checks on streaming DataFrame\n",
    "5. Monitor and manage streaming queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f59c5229-3892-4fd4-b7d1-99c7b5c34158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Create Streaming Source\n",
    "\n",
    "For this demo, we'll use Spark's built-in `rate` source which generates streaming data.\n",
    "In production, you would typically read from Kafka, Kinesis, or other streaming sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62597335-bba1-46c3-8f8f-7149bc5609a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, when, lit\n",
    "\n",
    "# Create a streaming DataFrame using rate source\n",
    "# This generates rows with columns: timestamp, value\n",
    "streaming_source = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", \"5\")  # Generate 5 rows per second\n",
    "    .option(\"numPartitions\", \"2\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Transform the streaming data to add meaningful columns for DQ checks\n",
    "streaming_df = (\n",
    "    streaming_source\n",
    "    .withColumn(\"id\", col(\"value\"))\n",
    "    .withColumn(\"age\", (col(\"value\") % 50) + 10)  # Age between 10-59\n",
    "    .withColumn(\n",
    "        \"email\",\n",
    "        when(col(\"value\") % 10 == 0, lit(None))  # Every 10th record has null email\n",
    "        .otherwise(expr(\"concat('user', value, '@example.com')\"))\n",
    "    )\n",
    "    .withColumn(\"name\", expr(\"concat('User_', value)\"))\n",
    "    .select(\"id\", \"age\", \"email\", \"name\", \"timestamp\")\n",
    ")\n",
    "\n",
    "print(\"‚úì Streaming source created\")\n",
    "print(f\"  Is streaming: {streaming_df.isStreaming}\")\n",
    "print(\"  Schema:\")\n",
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e4df78-19c5-4ffc-9ef0-9137845eee17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Define Data Quality Rules\n",
    "\n",
    "Create rules to validate the streaming data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55adbac4-5ad6-466c-89a7-d1b16141a88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_not_null\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age IS NOT NULL\",\n",
    "        \"action_if_failed\": \"ignore\",  # For streaming, use 'ignore' to keep processing\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Age must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": False,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_range_valid\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age BETWEEN 10 AND 100\",\n",
    "        \"action_if_failed\": \"ignore\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Age must be between 10 and 100\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": False,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"high\",\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"email_not_null\",\n",
    "        \"column_name\": \"email\",\n",
    "        \"expectation\": \"email IS NOT NULL\",\n",
    "        \"action_if_failed\": \"drop\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Email must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": False,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"email_format_valid\",\n",
    "        \"column_name\": \"email\",\n",
    "        \"expectation\": \"email LIKE '%@%.%'\",\n",
    "        \"action_if_failed\": \"ignore\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Email must be in valid format\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": False,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"low\",\n",
    "    },\n",
    "]\n",
    "\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "rules_df.write.mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "\n",
    "print(\"‚úì Data quality rules created\")\n",
    "display(rules_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "725afb4f-79f1-4858-b095-9bbae08f2f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### For agg dq\n",
    "import pandas as pd\n",
    "\n",
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"agg_dq\",\n",
    "        \"rule\": \"data_existing\",\n",
    "        \"column_name\": \"sales\",\n",
    "        \"expectation\": \"count(*) > 0\",\n",
    "        \"action_if_failed\": \"fail\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Data should be present\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"agg_dq\",\n",
    "        \"rule\": \"min_sales\",\n",
    "        \"column_name\": \"sales\",\n",
    "        \"expectation\": \"min(sales)>1000\",\n",
    "        \"action_if_failed\": \"warn\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Minimum sales should be greater than 1000\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "rules_df.write.mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "\n",
    "print(\"‚úì Data quality rules created\")\n",
    "display(rules_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c350922-7256-4c06-998c-2f494ab79b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the pd integration key\n",
    "pd_integration_key = dbutils.secrets.get(scope=\"pagerduty_test\", key=\"pd_integration_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f8f8f8-c852-4f8c-853e-9f5d62463fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure streaming and notification configuration\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "\n",
    "user_conf_dict = {\n",
    "    # Enable Slack notifications\n",
    "    user_config.se_notifications_enable_slack: True,\n",
    "    # Slack webhook URL (replace with your actual webhook URL)\n",
    "    user_config.se_notifications_slack_webhook_url: \"slack_webhook\",\n",
    "\n",
    "    # # pagerduty notifications flags\n",
    "    user_config.se_notifications_enable_pagerduty: True,\n",
    "    user_config.se_notifications_pagerduty_integration_key: pd_integration_key,\n",
    "    user_config.se_notifications_pagerduty_webhook_url: \"pagerduty_webhook\",\n",
    "\n",
    "    # # email flags\n",
    "    user_config.se_notifications_enable_email: True,\n",
    "    user_config.se_notifications_email_smtp_host: \"email_smtp_host\",\n",
    "    user_config.se_notifications_email_smtp_port: 25,\n",
    "    user_config.se_notifications_email_from: \"email_from\",\n",
    "    user_config.se_notifications_email_to_other_mail_id: \"email_to\",\n",
    "    user_config.se_notifications_email_subject: \"StreamWriter Notifications Integration Testing\",\n",
    "    \n",
    "    # Notification triggers\n",
    "    user_config.se_notifications_on_start: True,\n",
    "    user_config.se_notifications_on_completion: True,\n",
    "    user_config.se_notifications_on_fail: True,\n",
    "    user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n",
    "    user_config.se_notifications_on_rules_action_if_failed_set_ignore: True,\n",
    "    user_config.se_notifications_on_error_drop_threshold: 15,\n",
    "    user_config.se_notifications_min_priority_slack: \"low\",\n",
    "\n",
    "    # Other required configurations\n",
    "    user_config.se_enable_error_table: True,\n",
    "    user_config.se_dq_rules_params: {\n",
    "        \"env\": \"dev\",\n",
    "        \"table\": \"demo_table_slack_rg\",\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "display(user_conf_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd219be3-5807-4931-bf75-4433e9c3cc33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Configure Spark Expectations for Streaming\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: For streaming workloads, you MUST configure:\n",
    "1. `se_enable_streaming` = False (this is for stats streaming to Kafka, not data streaming)\n",
    "2. Checkpoint location in the target table writer configuration\n",
    "3. Output mode (append/update/complete)\n",
    "4. Processing trigger (how often to process micro-batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e4e654c-afc3-4ad0-87fd-02c8835321be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972bc0f3-5019-4912-8d89-0914231fbe78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from spark_expectations.core import load_configurations\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    "    WrappedDataFrameStreamWriter\n",
    ")\n",
    "\n",
    "# Initialize default Spark Expectations configuration\n",
    "try:\n",
    "    load_configurations(spark)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not set some Spark configs due to Spark Connect limitations. Details: {e}\")\n",
    "    print(\"If you need full Spark Expectations functionality, use a classic Databricks cluster.\")\n",
    "\n",
    "# Configure writer for TARGET and ERROR tables (STREAMING)\n",
    "# This writer will be used for the target table which will be streaming\n",
    "target_writer_streaming_config = {\n",
    "    \"outputMode\": \"append\",  # Use append for streaming writes\n",
    "    \"format\": \"delta\",\n",
    "    \"queryName\": f\"se_{CONFIG['user']}_streaming_target_query\",\n",
    "    \"trigger\": {\"processingTime\": \"10 seconds\"},  # Process every 10 seconds\n",
    "    \"options\": {\n",
    "        # üî• CRITICAL: Always set checkpointLocation for production streaming\n",
    "        \"checkpointLocation\": f\"{CONFIG['checkpoint_path']}/target\",\n",
    "        \"maxFilesPerTrigger\": \"100\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create writer with streaming configuration\n",
    "target_writer = WrappedDataFrameStreamWriter().format(\"delta\").option(\"checkpointLocation\", f\"{CONFIG['checkpoint_path']}/target\")\n",
    "\n",
    "# Configure writer for STATS table (BATCH)\n",
    "# Stats are written in batch mode, even when processing streaming data\n",
    "stats_writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
    "\n",
    "# Streaming stats options (for Kafka - disabled in this example)\n",
    "stats_streaming_config_dict = {\n",
    "    user_config.se_enable_streaming: False  # Disable stats streaming to Kafka\n",
    "}\n",
    "\n",
    "# Initialize Spark Expectations\n",
    "se = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=stats_writer,\n",
    "    target_and_error_table_writer=target_writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "print(\"‚úì Spark Expectations configured for streaming\")\n",
    "print(f\"  Checkpoint location: {CONFIG['checkpoint_path']}\")\n",
    "print(f\"  Output mode: {target_writer_streaming_config['outputMode']}\")\n",
    "print(f\"  Processing trigger: {target_writer_streaming_config['trigger']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5a43d5-c6df-43eb-8914-4206be2aac2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Warning\n",
    "> **Note:** Spark Connect restricts setting custom Spark configurations, which may limit the functionality of Spark Expectations. For full support, it is recommended to use a classic Databricks cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "044f2022-c82f-41b7-adcd-143b5449351d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Apply Data Quality Checks to Streaming Data\n",
    "\n",
    "The `@se.with_expectations` decorator automatically detects streaming DataFrames and uses the appropriate write method.\n",
    "\n",
    "**Key Points:**\n",
    "- The decorator will use `writeStream` instead of `write` when it detects a streaming DataFrame\n",
    "- The target table writer configuration (with checkpoint location) will be used\n",
    "- The streaming query will be started and managed automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b16576-2159-4442-9596-d15e0d9e0bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "@se.with_expectations(\n",
    "    target_table=CONFIG[\"target_table\"],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=False,  # Not needed for streaming\n",
    "    user_conf=user_conf_dict,\n",
    "    target_and_error_table_writer=target_writer,\n",
    "    #target_table_writer_config=target_writer_streaming_config,  # üî• Pass streaming config\n",
    ")\n",
    "def process_streaming_data():\n",
    "    \"\"\"\n",
    "    This function returns the streaming DataFrame.\n",
    "    Spark Expectations will:\n",
    "    1. Apply all data quality rules\n",
    "    2. Write validated data to target table using streaming write\n",
    "    3. Write error records to error table\n",
    "    4. Write stats to stats table\n",
    "    \"\"\"\n",
    "    _df_source: DataFrame = streaming_df\n",
    "    _df_source.createOrReplaceTempView(CONFIG[\"in_memory_source\"])\n",
    "    return _df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece7e82c-a22c-4313-8923-3c52298fffe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = process_streaming_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c633c6a-bcf2-4299-b8ba-d463bd44f6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if hasattr(query, 'isActive') and query.isActive:\n",
    "    print(f\" Streaming query started: {query.name}\")\n",
    "    print(f\"   Query ID: {query.id}\")\n",
    "    print(\"   Running for 30 seconds...\")\n",
    "    \n",
    "    # Monitor for 30 seconds\n",
    "    for i in range(15):  # Increased to allow more records (up to 75 seconds of data)\n",
    "        if query.isActive:\n",
    "            progress = query.lastProgress\n",
    "            if progress:\n",
    "                batch_id = progress.get('batchId', 'N/A')\n",
    "                print(f\" Processing batch: {batch_id}\")\n",
    "            time.sleep(5)  # Enabled sleep to allow time between batches\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    query.stop()\n",
    "    print(\"Streaming query stopped successfully\")\n",
    "else:\n",
    "    print(\"Processed as batch DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a6caed6-f9e4-44b5-a169-a6bb832bebe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Monitor Streaming Query\n",
    "\n",
    "Check the status and progress of the streaming query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94686cb2-85cd-49f0-9db3-9f9380c68019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Let the stream run for a bit to process some data\n",
    "print(\"‚è≥ Letting stream run for 30 seconds to process data...\")\n",
    "time.sleep(30)\n",
    "\n",
    "if query and query.isActive:\n",
    "    # Get streaming query progress\n",
    "    progress = query.lastProgress\n",
    "    \n",
    "    if progress:\n",
    "        print(\"\\nüìä Streaming Query Progress:\")\n",
    "        print(f\"  Batch ID: {progress.get('batchId', 'N/A')}\")\n",
    "        print(f\"  Input Rows/Second: {progress.get('inputRowsPerSecond', 0)}\")\n",
    "        print(f\"  Processed Rows/Second: {progress.get('processedRowsPerSecond', 0)}\")\n",
    "        print(f\"  Batch Duration: {progress.get('batchDuration', 0)} ms\")\n",
    "        print(f\"  Timestamp: {progress.get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Show sources\n",
    "        sources = progress.get('sources', [])\n",
    "        if sources:\n",
    "            print(f\"\\n  Sources:\")\n",
    "            for source in sources:\n",
    "                print(f\"    - Description: {source.get('description', 'N/A')}\")\n",
    "                print(f\"      Input Rows: {source.get('numInputRows', 0)}\")\n",
    "                print(f\"      Processing Rate: {source.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No progress information available yet. Stream may be initializing.\")\n",
    "else:\n",
    "    print(\"‚ùå Streaming query is not active\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "906580f3-35b9-46ab-a7e1-1656d2e5cb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. View Results\n",
    "\n",
    "Check the target, error, and stats tables to see the results of data quality checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4894646-18d5-4473-b722-071c4af2b668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View records in target table\n",
    "print(\"üìã Target Table (validated records):\")\n",
    "target_df = spark.sql(f\"SELECT * FROM {CONFIG['target_table']} ORDER BY id DESC LIMIT 20\")\n",
    "display(target_df)\n",
    "\n",
    "# Count records\n",
    "total_count = spark.sql(f\"SELECT COUNT(*) as count FROM {CONFIG['target_table']}\").collect()[0]['count']\n",
    "print(f\"\\nTotal records in target table: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b50349-2faa-4dbe-9ffe-9c0841d72b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View error records\n",
    "error_table = f\"{CONFIG['target_table']}_error\"\n",
    "print(\"üö® Error Table (failed validation records):\")\n",
    "\n",
    "try:\n",
    "    error_df = spark.sql(f\"SELECT * FROM {error_table} ORDER BY id DESC LIMIT 20\")\n",
    "    error_count = spark.sql(f\"SELECT COUNT(*) as count FROM {error_table}\").collect()[0]['count']\n",
    "    \n",
    "    print(f\"Total error records: {error_count}\")\n",
    "    if error_count > 0:\n",
    "        display(error_df)\n",
    "    else:\n",
    "        print(\"No error records found (all records passed validation)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error table may not exist yet or has no records: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9386c7-02bf-45cd-bde0-8ff3ba9d166c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. View Data Quality Results in Detail\n",
    "\n",
    "Analyze which rules passed/failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71338a3-adaf-482c-8a37-8540348575d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get latest stats\n",
    "latest_stats = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        product_id,\n",
    "        table_name,\n",
    "        input_count,\n",
    "        error_count,\n",
    "        output_count,\n",
    "        ROUND(success_percentage, 2) as success_pct,\n",
    "        ROUND(error_percentage, 2) as error_pct,\n",
    "        row_dq_res_summary,\n",
    "        dq_status.row_dq as row_dq_status,\n",
    "        meta_dq_run_date_time\n",
    "    FROM {CONFIG['stats_table']}\n",
    "    ORDER BY meta_dq_run_date_time DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìà Latest Data Quality Summary:\")\n",
    "display(latest_stats)\n",
    "\n",
    "# Show row DQ summary\n",
    "print(\"\\nüîç Row-level DQ Results Summary:\")\n",
    "row_dq_summary = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        explode(row_dq_res_summary) as summary_item,\n",
    "        meta_dq_run_date_time\n",
    "    FROM {CONFIG['stats_table']}\n",
    "    ORDER BY meta_dq_run_date_time DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "if row_dq_summary.count() > 0:\n",
    "    expanded_summary = row_dq_summary.select(\n",
    "        col(\"summary_item.rule\").alias(\"rule\"),\n",
    "        col(\"summary_item.column_name\").alias(\"column\"),\n",
    "        col(\"summary_item.description\").alias(\"description\"),\n",
    "        col(\"summary_item.failed_row_count\").alias(\"failed_count\"),\n",
    "        col(\"summary_item.action_if_failed\").alias(\"action\"),\n",
    "        col(\"summary_item.tag\").alias(\"tag\")\n",
    "    )\n",
    "    display(expanded_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b453530f-8bb1-491a-9b68-8a30d847b883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Monitor Active Streaming Queries\n",
    "\n",
    "View all active streaming queries in the session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed2cdc3-e836-4ef9-83dd-383ead31f3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all active streaming queries\n",
    "print(\"üîÑ Active Streaming Queries:\")\n",
    "active_streams = spark.streams.active\n",
    "\n",
    "if active_streams:\n",
    "    for stream in active_streams:\n",
    "        print(f\"\\n  Query ID: {stream.id}\")\n",
    "        print(f\"  Query Name: {stream.name}\")\n",
    "        print(f\"  Status: {'Active' if stream.isActive else 'Inactive'}\")\n",
    "        print(f\"  Recent Progress: {stream.recentProgress[-1] if stream.recentProgress else 'N/A'}\")\n",
    "else:\n",
    "    print(\"  No active streaming queries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f531d0a-e724-419e-ad28-994138674709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9. Stop Streaming Query\n",
    "\n",
    "Gracefully stop the streaming query when done using the SparkExpectationsWriter method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2962faa-1de5-48ed-ad8b-05bc1c7a4a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from spark_expectations.sinks.utils.writer import SparkExpectationsWriter\n",
    "\n",
    "# Get the writer instance from Spark Expectations context\n",
    "# Note: In production, you would typically have access to the writer instance\n",
    "# For this demo, we'll use the streaming query directly\n",
    "\n",
    "# Stop the specific streaming query gracefully using the main streaming_query variable\n",
    "if streaming_query and streaming_query.isActive:\n",
    "    print(f\"üõë Stopping streaming query: {streaming_query.name}\")\n",
    "    print(f\"  Query ID: {streaming_query.id}\")\n",
    "    \n",
    "    # Stop with a 30 second timeout\n",
    "    streaming_query.stop()\n",
    "    print(\"‚úì Streaming query stopped successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Streaming query is not active or already stopped\")\n",
    "\n",
    "# Verify the query is stopped\n",
    "if streaming_query:\n",
    "    print(f\"\\nQuery Status: {'Active' if streaming_query.isActive else 'Stopped'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aec8810-e223-45f9-bc47-d9056f93e917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stop All Active Streaming Queries\n",
    "\n",
    "Stop all active streaming queries in the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9577d9-4a8a-4722-9053-74ca29376fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop all active streaming queries\n",
    "print(\"üõë Stopping all active streaming queries...\")\n",
    "active_streams = spark.streams.active\n",
    "\n",
    "if active_streams:\n",
    "    for stream in active_streams:\n",
    "        if stream.isActive:\n",
    "            print(f\"\\n  Stopping: {stream.name} (ID: {stream.id})\")\n",
    "            try:\n",
    "                stream.stop()\n",
    "                print(f\"  ‚úì Stopped successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error stopping stream: {e}\")\n",
    "    \n",
    "    # Wait a moment for cleanup\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Verify all streams are stopped\n",
    "    remaining_streams = spark.streams.active\n",
    "    print(f\"\\n‚úì All streaming queries stopped\")\n",
    "    print(f\"  Active streaming queries remaining: {len(remaining_streams)}\")\n",
    "else:\n",
    "    print(\"  No active streaming queries to stop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441ff32a-8d71-40df-ab7b-ca3a6ccca4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways for Streaming with Spark Expectations\n",
    "\n",
    "### ‚úÖ Best Practices:\n",
    "\n",
    "1. **Always Configure Checkpoint Locations**\n",
    "   - Required for fault tolerance and exactly-once processing\n",
    "   - Use dedicated, persistent storage (HDFS, S3, ADLS)\n",
    "   - Never reuse checkpoint paths between different streaming jobs\n",
    "\n",
    "2. **Choose Appropriate Actions**\n",
    "   - Use `action_if_failed: ignore` for most streaming rules\n",
    "   - Avoid `drop` action which can cause data loss\n",
    "   - Failed records are logged in error table for analysis\n",
    "\n",
    "3. **Monitor Your Streams**\n",
    "   - Regularly check streaming query status\n",
    "   - Monitor stats table for data quality trends\n",
    "   - Set up alerts for error rate thresholds\n",
    "\n",
    "4. **Configure Triggers Appropriately**\n",
    "   - `processingTime`: For micro-batch processing (e.g., \"10 seconds\")\n",
    "   - `once`: For one-time processing of available data\n",
    "   - `continuous`: For low-latency streaming (experimental)\n",
    "\n",
    "5. **Output Modes**\n",
    "   - `append`: Most common for streaming (only new rows)\n",
    "   - `complete`: Entire result table (requires aggregations)\n",
    "   - `update`: Only updated rows (for aggregations)\n",
    "\n",
    "6. **Graceful Shutdown**\n",
    "   - Always stop streaming queries gracefully using `stop()` method\n",
    "   - Use timeout parameter for controlled shutdown\n",
    "   - Check for active queries before stopping\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- [STREAMWRITE_GUIDE.md](../../STREAMWRITE_GUIDE.md) - Comprehensive streaming guide\n",
    "- [Spark Expectations Documentation](https://engineering.nike.com/spark-expectations)\n",
    "- Spark Structured Streaming Programming Guide\n",
    "\n",
    "### üîë Key Differences from Batch Processing:\n",
    "\n",
    "| Aspect | Batch | Streaming |\n",
    "|--------|-------|-----------|\n",
    "| Write Method | `df.write` | `df.writeStream` |\n",
    "| Checkpoint | Not required | **Required** for production |\n",
    "| Action on Failure | `drop`, `fail`, `ignore` | Prefer `ignore` |\n",
    "| Query Management | N/A | Must monitor and stop queries |\n",
    "| Stats Writing | Batch | Batch (even for streaming data) |\n",
    "| Target Writing | Batch | Streaming with checkpoint |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202cc605-0fec-4708-be2a-87541779b6ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "327c50b8-387d-4f68-8023-1636b69f6724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View stats table\n",
    "print(\"üìä Stats Table (data quality metrics):\")\n",
    "stats_df = spark.sql(f\"SELECT * FROM {CONFIG['stats_table']} ORDER BY meta_dq_run_date_time DESC LIMIT 10\")\n",
    "display(stats_df.select(\n",
    "    \"product_id\",\n",
    "    \"table_name\",\n",
    "    \"input_count\",\n",
    "    \"error_count\",\n",
    "    \"output_count\",\n",
    "    \"success_percentage\",\n",
    "    \"error_percentage\",\n",
    "    \"dq_status\",\n",
    "    \"meta_dq_run_id\",\n",
    "    \"meta_dq_run_date_time\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c249f78-8551-4c1b-bea6-cd9f9f28565c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30cc9535-bcf2-4f53-85b8-ae44433a9636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_expectation_streaming_dbx",
   "widgets": {
    "catalog": {
     "currentValue": "development",
     "nuid": "f1beef73-6d67-40e6-b485-4e6a4d413b2b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "development",
      "label": null,
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "development",
      "label": null,
      "name": "catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "checkpoint_path": {
     "currentValue": "/Volumes/development/team_the_league/demo_volume",
     "nuid": "d4433cb3-2a83-4bbc-a107-139c16defe6d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/development/team_league/demors",
      "label": "",
      "name": "checkpoint_path",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "/Volumes/development/team_league/demors",
      "label": "",
      "name": "checkpoint_path",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "git_branch_or_commit": {
     "currentValue": "feature-TLE-902.1",
     "nuid": "f1cc713c-5891-4037-9c73-ce24425d7041",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": null,
      "name": "git_branch_or_commit",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "main",
      "label": null,
      "name": "git_branch_or_commit",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "git_org": {
     "currentValue": "Nike-Inc",
     "nuid": "98b28482-4712-4de8-90ee-06ec4e57f324",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Nike-Inc",
      "label": null,
      "name": "git_org",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "Nike-Inc",
      "label": null,
      "name": "git_org",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "in_memory_source": {
     "currentValue": "se_rg_source",
     "nuid": "b094daa1-f811-46de-9747-02112e586455",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "se_rg_source",
      "label": null,
      "name": "in_memory_source",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "se_rg_source",
      "label": null,
      "name": "in_memory_source",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "library_source": {
     "currentValue": "git",
     "nuid": "befeeb60-d2d2-4d1a-a334-69646392c85d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "git",
      "label": null,
      "name": "library_source",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "git",
      "label": null,
      "name": "library_source",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "override_version": {
     "currentValue": "True",
     "nuid": "1dfd3464-9f79-4111-ab7f-924230012f65",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": null,
      "name": "override_version",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "False",
      "label": null,
      "name": "override_version",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema": {
     "currentValue": "team_the_league",
     "nuid": "3567a8c2-8164-47e0-ae15-d0a9f0f150d6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "default",
      "label": null,
      "name": "schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "user": {
     "currentValue": "rgula",
     "nuid": "c1727a22-6458-486f-a53f-962af2e308a8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "user",
      "label": null,
      "name": "user",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "user",
      "label": null,
      "name": "user",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
