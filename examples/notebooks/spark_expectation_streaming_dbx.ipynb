{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spark Expectations - Streaming Write Guide\n",
        "\n",
        "<div style=\"color:red; font-weight:bold; border:2px solid red; padding:8px;\">\n",
        "⚠️ ALERT: Notebook meant to be ran in Databricks !\n",
        "</div>\n",
        "\n",
        "* Please read through the [Spark Expectation Documentation](https://engineering.nike.com/spark-expectations) and [STREAMWRITE_GUIDE.md](../../STREAMWRITE_GUIDE.md) before proceeding with this demo\n",
        "\n",
        "This notebook demonstrates how to use Spark Expectations with **streaming DataFrames**:\n",
        "- Automatic detection of streaming DataFrames\n",
        "- Proper checkpoint location configuration\n",
        "- Streaming query management\n",
        "- Best practices for production streaming workloads\n",
        "\n",
        "#### Widgets\n",
        "* `catalog`, `schema` - define where output tables are going to be created\n",
        "  * Tables are going to be prefixed with logged in DBX user\n",
        "* `checkpoint_path` - dedicated checkpoint location for streaming queries (REQUIRED for production)\n",
        "* `library_source` combo box defines library url(git branch or pypi) from where to pull library\n",
        "  * `pypi` (installs latest published version available in PyPi)\n",
        "  * `git` (installs library from specified git branch)\n",
        "    * Set `git_branch` input field to match git branch (example `main`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize notebook config\n",
        "This step will read widget values needed to configure Spark-Expectation library installation and define schema, catalog and table naming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "logged_in_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
        "logged_in_user = logged_in_user.split('@')[0]\n",
        "\n",
        "user = re.sub(r'[^a-zA-Z]', '', logged_in_user).lower()\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "library = dbutils.widgets.get(\"library_source\")\n",
        "org = dbutils.widgets.get(\"git_org\")\n",
        "branch_or_commit = dbutils.widgets.get(\"git_branch_or_commit\")\n",
        "checkpoint_base_path = dbutils.widgets.get(\"checkpoint_path\")\n",
        "\n",
        "CONFIG = {\n",
        "    \"owner\": user,\n",
        "    \"catalog\": catalog,\n",
        "    \"schema\": schema,\n",
        "    \"user\": user,\n",
        "    \"product_id\": f\"se_{user}_streaming_product\",\n",
        "    \"in_memory_source\": f\"se_{user}_streaming_source\",\n",
        "    \"rules_table\": f\"{catalog}.{schema}.se_{user}_streaming_rules\",\n",
        "    \"stats_table\": f\"{catalog}.{schema}.se_{user}_streaming_stats\",\n",
        "    \"target_table\": f\"{catalog}.{schema}.se_{user}_streaming_target\",\n",
        "    \"checkpoint_path\": checkpoint_base_path if checkpoint_base_path else f\"/tmp/checkpoints/se_{user}_streaming\",\n",
        "    \"library\": library,\n",
        "    \"org\": org,\n",
        "    \"branch_or_commit\": branch_or_commit\n",
        "}\n",
        "\n",
        "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])\n",
        "display(config_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Required Libraries\n",
        "* Spark Expectations\n",
        "* Jinja2 (required for using custom templates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"library\"] == \"pypi\":\n",
        "    print(\"-----INSTALLING SPARK-EXPECTATIONS from PyPI\")\n",
        "    %pip install spark-expectations\n",
        "elif CONFIG[\"library\"] == \"git\":\n",
        "    print(f\"-----INSTALLING SPARK-EXPECTATIONS from Git Org/User {CONFIG['org']}, Branch/Commit {CONFIG['branch_or_commit']}\")\n",
        "    giturl = f\"git+https://github.com/{CONFIG['org']}/spark-expectations.git@{CONFIG['branch_or_commit']}\"\n",
        "    %pip install --force-reinstall {giturl}\n",
        "\n",
        "print(\"-----INSTALLING Jinja2 template library\")\n",
        "%pip install jinja2\n",
        "\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display Spark Expectations installed version\n",
        "from importlib.metadata import version\n",
        "print(f\"---- Current SparkExpectation Version: {version('spark-expectations')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleanup\n",
        "Removing previously created user prefixed tables and checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
        "pattern = f\"se_{CONFIG['user']}_streaming*\"\n",
        "\n",
        "# Set the current catalog\n",
        "spark.sql(f\"USE CATALOG {CONFIG['catalog']}\")\n",
        "\n",
        "# Drop tables matching pattern\n",
        "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
        "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"]]\n",
        "\n",
        "if tables_to_drop:\n",
        "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
        "    for row in tables_to_drop:\n",
        "        table_name = row[\"tableName\"]\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
        "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
        "else:\n",
        "    print(\"----- No tables to drop\")\n",
        "\n",
        "# Drop views matching pattern\n",
        "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
        "views_to_drop = views_df.collect()\n",
        "\n",
        "if views_to_drop:\n",
        "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
        "    for row in views_to_drop:\n",
        "        view_name = row[\"viewName\"]\n",
        "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
        "        print(f\"Dropped view: {view_name}\")\n",
        "else:\n",
        "    print(\"----- No views to drop\")\n",
        "\n",
        "# Clean up checkpoint directory\n",
        "print(f\"\\nCleaning up checkpoint directory: {CONFIG['checkpoint_path']}\")\n",
        "dbutils.fs.rm(CONFIG['checkpoint_path'], True)\n",
        "print(\"Checkpoint directory cleaned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming Data Quality with Spark Expectations\n",
        "\n",
        "### Steps:\n",
        "1. Create a streaming source (using `rate` source for demo)\n",
        "2. Define data quality rules\n",
        "3. Configure Spark Expectations with streaming options\n",
        "4. Apply DQ checks on streaming DataFrame\n",
        "5. Monitor and manage streaming queries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Create Streaming Source\n",
        "\n",
        "For this demo, we'll use Spark's built-in `rate` source which generates streaming data.\n",
        "In production, you would typically read from Kafka, Kinesis, or other streaming sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, expr, when, lit\n",
        "\n",
        "# Create a streaming DataFrame using rate source\n",
        "# This generates rows with columns: timestamp, value\n",
        "streaming_source = (\n",
        "    spark.readStream\n",
        "    .format(\"rate\")\n",
        "    .option(\"rowsPerSecond\", \"5\")  # Generate 5 rows per second\n",
        "    .option(\"numPartitions\", \"2\")\n",
        "    .load()\n",
        ")\n",
        "\n",
        "# Transform the streaming data to add meaningful columns for DQ checks\n",
        "streaming_df = (\n",
        "    streaming_source\n",
        "    .withColumn(\"id\", col(\"value\"))\n",
        "    .withColumn(\"age\", (col(\"value\") % 50) + 10)  # Age between 10-59\n",
        "    .withColumn(\n",
        "        \"email\",\n",
        "        when(col(\"value\") % 10 == 0, lit(None))  # Every 10th record has null email\n",
        "        .otherwise(expr(\"concat('user', value, '@example.com')\"))\n",
        "    )\n",
        "    .withColumn(\"name\", expr(\"concat('User_', value)\"))\n",
        "    .select(\"id\", \"age\", \"email\", \"name\", \"timestamp\")\n",
        ")\n",
        "\n",
        "print(\"✓ Streaming source created\")\n",
        "print(f\"  Is streaming: {streaming_df.isStreaming}\")\n",
        "print(\"  Schema:\")\n",
        "streaming_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Define Data Quality Rules\n",
        "\n",
        "Create rules to validate the streaming data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "rules_data = [\n",
        "    {\n",
        "        \"product_id\": CONFIG[\"product_id\"],\n",
        "        \"table_name\": CONFIG[\"target_table\"],\n",
        "        \"rule_type\": \"row_dq\",\n",
        "        \"rule\": \"age_not_null\",\n",
        "        \"column_name\": \"age\",\n",
        "        \"expectation\": \"age IS NOT NULL\",\n",
        "        \"action_if_failed\": \"ignore\",  # For streaming, use 'ignore' to keep processing\n",
        "        \"tag\": \"completeness\",\n",
        "        \"description\": \"Age must not be null\",\n",
        "        \"enable_for_source_dq_validation\": True,\n",
        "        \"enable_for_target_dq_validation\": False,\n",
        "        \"is_active\": True,\n",
        "        \"enable_error_drop_alert\": False,\n",
        "        \"error_drop_threshold\": 0,\n",
        "        \"priority\": \"medium\",\n",
        "    },\n",
        "    {\n",
        "        \"product_id\": CONFIG[\"product_id\"],\n",
        "        \"table_name\": CONFIG[\"target_table\"],\n",
        "        \"rule_type\": \"row_dq\",\n",
        "        \"rule\": \"age_range_valid\",\n",
        "        \"column_name\": \"age\",\n",
        "        \"expectation\": \"age BETWEEN 10 AND 100\",\n",
        "        \"action_if_failed\": \"ignore\",\n",
        "        \"tag\": \"validity\",\n",
        "        \"description\": \"Age must be between 10 and 100\",\n",
        "        \"enable_for_source_dq_validation\": True,\n",
        "        \"enable_for_target_dq_validation\": False,\n",
        "        \"is_active\": True,\n",
        "        \"enable_error_drop_alert\": False,\n",
        "        \"error_drop_threshold\": 0,\n",
        "        \"priority\": \"high\",\n",
        "    },\n",
        "    {\n",
        "        \"product_id\": CONFIG[\"product_id\"],\n",
        "        \"table_name\": CONFIG[\"target_table\"],\n",
        "        \"rule_type\": \"row_dq\",\n",
        "        \"rule\": \"email_not_null\",\n",
        "        \"column_name\": \"email\",\n",
        "        \"expectation\": \"email IS NOT NULL\",\n",
        "        \"action_if_failed\": \"ignore\",\n",
        "        \"tag\": \"completeness\",\n",
        "        \"description\": \"Email must not be null\",\n",
        "        \"enable_for_source_dq_validation\": True,\n",
        "        \"enable_for_target_dq_validation\": False,\n",
        "        \"is_active\": True,\n",
        "        \"enable_error_drop_alert\": False,\n",
        "        \"error_drop_threshold\": 0,\n",
        "        \"priority\": \"medium\",\n",
        "    },\n",
        "    {\n",
        "        \"product_id\": CONFIG[\"product_id\"],\n",
        "        \"table_name\": CONFIG[\"target_table\"],\n",
        "        \"rule_type\": \"row_dq\",\n",
        "        \"rule\": \"email_format_valid\",\n",
        "        \"column_name\": \"email\",\n",
        "        \"expectation\": \"email LIKE '%@%.%'\",\n",
        "        \"action_if_failed\": \"ignore\",\n",
        "        \"tag\": \"validity\",\n",
        "        \"description\": \"Email must be in valid format\",\n",
        "        \"enable_for_source_dq_validation\": True,\n",
        "        \"enable_for_target_dq_validation\": False,\n",
        "        \"is_active\": True,\n",
        "        \"enable_error_drop_alert\": False,\n",
        "        \"error_drop_threshold\": 0,\n",
        "        \"priority\": \"low\",\n",
        "    },\n",
        "]\n",
        "\n",
        "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
        "rules_df.write.mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
        "\n",
        "print(\"✓ Data quality rules created\")\n",
        "display(rules_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Configure Spark Expectations for Streaming\n",
        "\n",
        "⚠️ **IMPORTANT**: For streaming workloads, you MUST configure:\n",
        "1. `se_enable_streaming` = False (this is for stats streaming to Kafka, not data streaming)\n",
        "2. Checkpoint location in the target table writer configuration\n",
        "3. Output mode (append/update/complete)\n",
        "4. Processing trigger (how often to process micro-batches)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spark_expectations.core import load_configurations\n",
        "from spark_expectations.config.user_config import Constants as user_config\n",
        "from spark_expectations.core.expectations import (\n",
        "    SparkExpectations,\n",
        "    WrappedDataFrameWriter,\n",
        ")\n",
        "\n",
        "# Initialize default Spark Expectations configuration\n",
        "load_configurations(spark)\n",
        "\n",
        "# Configure writer for TARGET and ERROR tables (STREAMING)\n",
        "# This writer will be used for the target table which will be streaming\n",
        "target_writer_streaming_config = {\n",
        "    \"outputMode\": \"append\",  # Use append for streaming writes\n",
        "    \"format\": \"delta\",\n",
        "    \"queryName\": f\"se_{CONFIG['user']}_streaming_target_query\",\n",
        "    \"trigger\": {\"processingTime\": \"10 seconds\"},  # Process every 10 seconds\n",
        "    \"options\": {\n",
        "        # 🔥 CRITICAL: Always set checkpointLocation for production streaming\n",
        "        \"checkpointLocation\": f\"{CONFIG['checkpoint_path']}/target\",\n",
        "        \"maxFilesPerTrigger\": \"100\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create writer with streaming configuration\n",
        "target_writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
        "\n",
        "# Configure writer for STATS table (BATCH)\n",
        "# Stats are written in batch mode, even when processing streaming data\n",
        "stats_writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
        "\n",
        "# Streaming stats options (for Kafka - disabled in this example)\n",
        "stats_streaming_config_dict = {\n",
        "    user_config.se_enable_streaming: False  # Disable stats streaming to Kafka\n",
        "}\n",
        "\n",
        "# Initialize Spark Expectations\n",
        "se = SparkExpectations(\n",
        "    product_id=CONFIG[\"product_id\"],\n",
        "    rules_df=rules_df,\n",
        "    stats_table=CONFIG[\"stats_table\"],\n",
        "    stats_table_writer=stats_writer,\n",
        "    target_and_error_table_writer=target_writer,\n",
        "    stats_streaming_options=stats_streaming_config_dict,\n",
        ")\n",
        "\n",
        "print(\"✓ Spark Expectations configured for streaming\")\n",
        "print(f\"  Checkpoint location: {CONFIG['checkpoint_path']}\")\n",
        "print(f\"  Output mode: {target_writer_streaming_config['outputMode']}\")\n",
        "print(f\"  Processing trigger: {target_writer_streaming_config['trigger']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Apply Data Quality Checks to Streaming Data\n",
        "\n",
        "The `@se.with_expectations` decorator automatically detects streaming DataFrames and uses the appropriate write method.\n",
        "\n",
        "**Key Points:**\n",
        "- The decorator will use `writeStream` instead of `write` when it detects a streaming DataFrame\n",
        "- The target table writer configuration (with checkpoint location) will be used\n",
        "- The streaming query will be started and managed automatically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# User configuration for notifications (can be extended with Slack, email, etc.)\n",
        "notification_conf = {}\n",
        "\n",
        "@se.with_expectations(\n",
        "    target_table=CONFIG[\"target_table\"],\n",
        "    write_to_table=True,\n",
        "    write_to_temp_table=False,  # Not needed for streaming\n",
        "    user_conf=notification_conf,\n",
        "    target_and_error_table_writer=target_writer,\n",
        "    target_table_writer_config=target_writer_streaming_config,  # 🔥 Pass streaming config\n",
        ")\n",
        "def process_streaming_data():\n",
        "    \"\"\"\n",
        "    This function returns the streaming DataFrame.\n",
        "    Spark Expectations will:\n",
        "    1. Apply all data quality rules\n",
        "    2. Write validated data to target table using streaming write\n",
        "    3. Write error records to error table\n",
        "    4. Write stats to stats table\n",
        "    \"\"\"\n",
        "    return streaming_df\n",
        "\n",
        "# Start the streaming query with data quality checks\n",
        "print(\"🚀 Starting streaming query with Spark Expectations...\")\n",
        "streaming_query = process_streaming_data()\n",
        "\n",
        "print(\"\\n✓ Streaming query started successfully!\")\n",
        "print(f\"  Query ID: {streaming_query.id if streaming_query else 'N/A'}\")\n",
        "print(f\"  Query Name: {streaming_query.name if streaming_query else 'N/A'}\")\n",
        "print(f\"  Is Active: {streaming_query.isActive if streaming_query else False}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Monitor Streaming Query\n",
        "\n",
        "Check the status and progress of the streaming query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Let the stream run for a bit to process some data\n",
        "print(\"⏳ Letting stream run for 30 seconds to process data...\")\n",
        "time.sleep(30)\n",
        "\n",
        "if streaming_query and streaming_query.isActive:\n",
        "    # Get streaming query progress\n",
        "    progress = streaming_query.lastProgress\n",
        "    \n",
        "    if progress:\n",
        "        print(\"\\n📊 Streaming Query Progress:\")\n",
        "        print(f\"  Batch ID: {progress.get('batchId', 'N/A')}\")\n",
        "        print(f\"  Input Rows/Second: {progress.get('inputRowsPerSecond', 0)}\")\n",
        "        print(f\"  Processed Rows/Second: {progress.get('processedRowsPerSecond', 0)}\")\n",
        "        print(f\"  Batch Duration: {progress.get('batchDuration', 0)} ms\")\n",
        "        print(f\"  Timestamp: {progress.get('timestamp', 'N/A')}\")\n",
        "        \n",
        "        # Show sources\n",
        "        sources = progress.get('sources', [])\n",
        "        if sources:\n",
        "            print(f\"\\n  Sources:\")\n",
        "            for source in sources:\n",
        "                print(f\"    - Description: {source.get('description', 'N/A')}\")\n",
        "                print(f\"      Input Rows: {source.get('numInputRows', 0)}\")\n",
        "                print(f\"      Processing Rate: {source.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
        "    else:\n",
        "        print(\"⚠️ No progress information available yet. Stream may be initializing.\")\n",
        "else:\n",
        "    print(\"❌ Streaming query is not active\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. View Results\n",
        "\n",
        "Check the target, error, and stats tables to see the results of data quality checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View records in target table\n",
        "print(\"📋 Target Table (validated records):\")\n",
        "target_df = spark.sql(f\"SELECT * FROM {CONFIG['target_table']} ORDER BY id DESC LIMIT 20\")\n",
        "display(target_df)\n",
        "\n",
        "# Count records\n",
        "total_count = spark.sql(f\"SELECT COUNT(*) as count FROM {CONFIG['target_table']}\").collect()[0]['count']\n",
        "print(f\"\\nTotal records in target table: {total_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View error records\n",
        "error_table = f\"{CONFIG['target_table']}_error\"\n",
        "print(\"🚨 Error Table (failed validation records):\")\n",
        "\n",
        "try:\n",
        "    error_df = spark.sql(f\"SELECT * FROM {error_table} ORDER BY id DESC LIMIT 20\")\n",
        "    error_count = spark.sql(f\"SELECT COUNT(*) as count FROM {error_table}\").collect()[0]['count']\n",
        "    \n",
        "    print(f\"Total error records: {error_count}\")\n",
        "    if error_count > 0:\n",
        "        display(error_df)\n",
        "    else:\n",
        "        print(\"No error records found (all records passed validation)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error table may not exist yet or has no records: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. View Data Quality Results in Detail\n",
        "\n",
        "Analyze which rules passed/failed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get latest stats\n",
        "latest_stats = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        product_id,\n",
        "        table_name,\n",
        "        input_count,\n",
        "        error_count,\n",
        "        output_count,\n",
        "        ROUND(success_percentage, 2) as success_pct,\n",
        "        ROUND(error_percentage, 2) as error_pct,\n",
        "        row_dq_res_summary,\n",
        "        dq_status.row_dq as row_dq_status,\n",
        "        meta_dq_run_date_time\n",
        "    FROM {CONFIG['stats_table']}\n",
        "    ORDER BY meta_dq_run_date_time DESC\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "print(\"📈 Latest Data Quality Summary:\")\n",
        "display(latest_stats)\n",
        "\n",
        "# Show row DQ summary\n",
        "print(\"\\n🔍 Row-level DQ Results Summary:\")\n",
        "row_dq_summary = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        explode(row_dq_res_summary) as summary_item,\n",
        "        meta_dq_run_date_time\n",
        "    FROM {CONFIG['stats_table']}\n",
        "    ORDER BY meta_dq_run_date_time DESC\n",
        "    LIMIT 1\n",
        "\"\"\")\n",
        "\n",
        "if row_dq_summary.count() > 0:\n",
        "    expanded_summary = row_dq_summary.select(\n",
        "        col(\"summary_item.rule\").alias(\"rule\"),\n",
        "        col(\"summary_item.column_name\").alias(\"column\"),\n",
        "        col(\"summary_item.description\").alias(\"description\"),\n",
        "        col(\"summary_item.failed_row_count\").alias(\"failed_count\"),\n",
        "        col(\"summary_item.action_if_failed\").alias(\"action\"),\n",
        "        col(\"summary_item.tag\").alias(\"tag\")\n",
        "    )\n",
        "    display(expanded_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Monitor Active Streaming Queries\n",
        "\n",
        "View all active streaming queries in the session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all active streaming queries\n",
        "print(\"🔄 Active Streaming Queries:\")\n",
        "active_streams = spark.streams.active\n",
        "\n",
        "if active_streams:\n",
        "    for stream in active_streams:\n",
        "        print(f\"\\n  Query ID: {stream.id}\")\n",
        "        print(f\"  Query Name: {stream.name}\")\n",
        "        print(f\"  Status: {'Active' if stream.isActive else 'Inactive'}\")\n",
        "        print(f\"  Recent Progress: {stream.recentProgress[-1] if stream.recentProgress else 'N/A'}\")\n",
        "else:\n",
        "    print(\"  No active streaming queries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Stop Streaming Query\n",
        "\n",
        "Gracefully stop the streaming query when done using the SparkExpectationsWriter method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spark_expectations.sinks.utils.writer import SparkExpectationsWriter\n",
        "\n",
        "# Get the writer instance from Spark Expectations context\n",
        "# Note: In production, you would typically have access to the writer instance\n",
        "# For this demo, we'll use the streaming query directly\n",
        "\n",
        "# Stop the specific streaming query gracefully using the main streaming_query variable\n",
        "if streaming_query and streaming_query.isActive:\n",
        "    print(f\"🛑 Stopping streaming query: {streaming_query.name}\")\n",
        "    print(f\"  Query ID: {streaming_query.id}\")\n",
        "    \n",
        "    # Stop with a 30 second timeout\n",
        "    streaming_query.stop()\n",
        "    print(\"✓ Streaming query stopped successfully\")\n",
        "else:\n",
        "    print(\"⚠️ Streaming query is not active or already stopped\")\n",
        "\n",
        "# Verify the query is stopped\n",
        "if streaming_query:\n",
        "    print(f\"\\nQuery Status: {'Active' if streaming_query.isActive else 'Stopped'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stop All Active Streaming Queries\n",
        "\n",
        "Stop all active streaming queries in the current session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop all active streaming queries\n",
        "print(\"🛑 Stopping all active streaming queries...\")\n",
        "active_streams = spark.streams.active\n",
        "\n",
        "if active_streams:\n",
        "    for stream in active_streams:\n",
        "        if stream.isActive:\n",
        "            print(f\"\\n  Stopping: {stream.name} (ID: {stream.id})\")\n",
        "            try:\n",
        "                stream.stop()\n",
        "                print(f\"  ✓ Stopped successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Error stopping stream: {e}\")\n",
        "    \n",
        "    # Wait a moment for cleanup\n",
        "    import time\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # Verify all streams are stopped\n",
        "    remaining_streams = spark.streams.active\n",
        "    print(f\"\\n✓ All streaming queries stopped\")\n",
        "    print(f\"  Active streaming queries remaining: {len(remaining_streams)}\")\n",
        "else:\n",
        "    print(\"  No active streaming queries to stop\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways for Streaming with Spark Expectations\n",
        "\n",
        "### ✅ Best Practices:\n",
        "\n",
        "1. **Always Configure Checkpoint Locations**\n",
        "   - Required for fault tolerance and exactly-once processing\n",
        "   - Use dedicated, persistent storage (HDFS, S3, ADLS)\n",
        "   - Never reuse checkpoint paths between different streaming jobs\n",
        "\n",
        "2. **Choose Appropriate Actions**\n",
        "   - Use `action_if_failed: ignore` for most streaming rules\n",
        "   - Avoid `drop` action which can cause data loss\n",
        "   - Failed records are logged in error table for analysis\n",
        "\n",
        "3. **Monitor Your Streams**\n",
        "   - Regularly check streaming query status\n",
        "   - Monitor stats table for data quality trends\n",
        "   - Set up alerts for error rate thresholds\n",
        "\n",
        "4. **Configure Triggers Appropriately**\n",
        "   - `processingTime`: For micro-batch processing (e.g., \"10 seconds\")\n",
        "   - `once`: For one-time processing of available data\n",
        "   - `continuous`: For low-latency streaming (experimental)\n",
        "\n",
        "5. **Output Modes**\n",
        "   - `append`: Most common for streaming (only new rows)\n",
        "   - `complete`: Entire result table (requires aggregations)\n",
        "   - `update`: Only updated rows (for aggregations)\n",
        "\n",
        "6. **Graceful Shutdown**\n",
        "   - Always stop streaming queries gracefully using `stop()` method\n",
        "   - Use timeout parameter for controlled shutdown\n",
        "   - Check for active queries before stopping\n",
        "\n",
        "### 📚 Additional Resources:\n",
        "- [STREAMWRITE_GUIDE.md](../../STREAMWRITE_GUIDE.md) - Comprehensive streaming guide\n",
        "- [Spark Expectations Documentation](https://engineering.nike.com/spark-expectations)\n",
        "- Spark Structured Streaming Programming Guide\n",
        "\n",
        "### 🔑 Key Differences from Batch Processing:\n",
        "\n",
        "| Aspect | Batch | Streaming |\n",
        "|--------|-------|-----------|\n",
        "| Write Method | `df.write` | `df.writeStream` |\n",
        "| Checkpoint | Not required | **Required** for production |\n",
        "| Action on Failure | `drop`, `fail`, `ignore` | Prefer `ignore` |\n",
        "| Query Management | N/A | Must monitor and stop queries |\n",
        "| Stats Writing | Batch | Batch (even for streaming data) |\n",
        "| Target Writing | Batch | Streaming with checkpoint |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View stats table\n",
        "print(\"📊 Stats Table (data quality metrics):\")\n",
        "stats_df = spark.sql(f\"SELECT * FROM {CONFIG['stats_table']} ORDER BY meta_dq_run_date_time DESC LIMIT 10\")\n",
        "display(stats_df.select(\n",
        "    \"product_id\",\n",
        "    \"table_name\",\n",
        "    \"input_count\",\n",
        "    \"error_count\",\n",
        "    \"output_count\",\n",
        "    \"success_percentage\",\n",
        "    \"error_percentage\",\n",
        "    \"dq_status\",\n",
        "    \"meta_dq_run_id\",\n",
        "    \"meta_dq_run_date_time\"\n",
        "))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
