{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc224046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE INPUT WIDGETS FOR CONFIGURATION\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "widget_user = widgets.Text(\n",
    "    value='testuser',\n",
    "    placeholder='Type something',\n",
    "    description='user: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_git_org = widgets.Text(\n",
    "    value='Nike-Inc',\n",
    "    placeholder='Type something',\n",
    "    description='git_org ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_catalog = widgets.Text(\n",
    "    value='spark_catalog',\n",
    "    placeholder='Type something',\n",
    "    description='catalog:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_schema = widgets.Text(\n",
    "    value='default',\n",
    "    placeholder='Type something',\n",
    "    description='schema:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_library_source = widgets.Combobox(\n",
    "    placeholder='Choose source',\n",
    "    options=['pypi', 'git'],\n",
    "    description='library_source:',\n",
    "    ensure_option=True,\n",
    "    value='git',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_git_branch_or_commit = widgets.Text(\n",
    "    value='main',\n",
    "    placeholder='Type branch name or commit hash',\n",
    "    description='git_branch_or_commit:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "widget_override_version = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Override SE version',\n",
    "    disabled=False,\n",
    "    style={'description_width': '30px'}\n",
    ")\n",
    "\n",
    "hbox = widgets.HBox([\n",
    "    widget_user,\n",
    "    widget_catalog, \n",
    "    widget_schema,\n",
    "    widget_override_version, \n",
    "    widget_library_source, \n",
    "    widget_git_org,\n",
    "    widget_git_branch_or_commit\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4aa3737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535e820049f74813a93da48c29cd67a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='testuser', description='user: ', placeholder='Type something', style=TextStyle(descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display widgets\n",
    "display(hbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263fe506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: testuser\n",
      "Catalog: spark_catalog\n",
      "Schema: default\n",
      "Override SE Version: False\n",
      "Library Source: git\n",
      "Git Organization: Nike-Inc\n",
      "Branch/Commit: main\n"
     ]
    }
   ],
   "source": [
    "# Extract configuration values from widgets\n",
    "user = re.sub(r'[^a-zA-Z]', '', widget_user.value).lower()\n",
    "catalog = widget_catalog.value\n",
    "schema = widget_schema.value\n",
    "override_se_version = widget_override_version.value\n",
    "library = widget_library_source.value\n",
    "org = widget_git_org.value\n",
    "branch_or_commit = widget_git_branch_or_commit.value\n",
    "\n",
    "print(f\"User: {user}\")\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")\n",
    "print(f\"Override SE Version: {override_se_version}\")\n",
    "print(f\"Library Source: {library}\")\n",
    "print(f\"Git Organization: {org}\")\n",
    "print(f\"Branch/Commit: {branch_or_commit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1874a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owner</td>\n",
       "      <td>testuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>catalog</td>\n",
       "      <td>spark_catalog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schema</td>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user</td>\n",
       "      <td>testuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product_id</td>\n",
       "      <td>se_testuser_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rules_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stats_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_stats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customers_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>orders_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>products_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>override_se_version</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>library</td>\n",
       "      <td>git</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>org</td>\n",
       "      <td>Nike-Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>branch_or_commit</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Key                                        Value\n",
       "0                 owner                                     testuser\n",
       "1               catalog                                spark_catalog\n",
       "2                schema                                      default\n",
       "3                  user                                     testuser\n",
       "4            product_id                          se_testuser_product\n",
       "5           rules_table      spark_catalog.default.se_testuser_rules\n",
       "6           stats_table      spark_catalog.default.se_testuser_stats\n",
       "7       customers_table  spark_catalog.default.se_testuser_customers\n",
       "8          orders_table     spark_catalog.default.se_testuser_orders\n",
       "9        products_table   spark_catalog.default.se_testuser_products\n",
       "10  override_se_version                                        False\n",
       "11              library                                          git\n",
       "12                  org                                     Nike-Inc\n",
       "13     branch_or_commit                                         main"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build configuration dictionary\n",
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": catalog,\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_product\",\n",
    "    \"rules_table\": f\"{catalog}.{schema}.se_{user}_rules\",\n",
    "    \"stats_table\": f\"{catalog}.{schema}.se_{user}_stats\",\n",
    "    \"customers_table\": f\"{catalog}.{schema}.se_{user}_customers\",\n",
    "    \"orders_table\": f\"{catalog}.{schema}.se_{user}_orders\",\n",
    "    \"products_table\": f\"{catalog}.{schema}.se_{user}_products\",\n",
    "    \"override_se_version\": override_se_version,\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89380ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Current SparkExpectation Version: 2.7.1.dev3+gf06eabfa6\n"
     ]
    }
   ],
   "source": [
    "# Display current Spark Expectations version\n",
    "from importlib.metadata import version\n",
    "print(f\"---- Current SparkExpectation Version: {version('spark-expectations')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3919ba13",
   "metadata": {},
   "source": [
    "### Setting up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dc61a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d85ffbcf-7f6a-44c6-af2c-002455512d55;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 110ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d85ffbcf-7f6a-44c6-af2c-002455512d55\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/11/14 17:33:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session created successfully\n"
     ]
    }
   ],
   "source": [
    "# CREATE SPARK SESSION\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Expectations - DQ Pro Rules Test\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark session created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82adc77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show existing databases and tables\n",
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show(truncate=False)\n",
    "\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477697b3",
   "metadata": {},
   "source": [
    "### Cleanup Existing Tables and Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c825b4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No tables to drop\n"
     ]
    }
   ],
   "source": [
    "# Clean up existing tables and views from previous runs\n",
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE {CONFIG['catalog']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"]]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"ðŸ§¹ Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"   âœ“ Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"âœ… No tables to drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a19c4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No views to drop\n"
     ]
    }
   ],
   "source": [
    "# Drop views matching pattern\n",
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"ðŸ§¹ Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"   âœ“ Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"âœ… No views to drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148a463",
   "metadata": {},
   "source": [
    "### Load Rules from YAML File\n",
    "Now let's load the comprehensive rules from `rules_all_types.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152e524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading rules from: /app/notebooks/resources/rules_all_types.yaml\n",
      "ðŸ“‹ Loaded 23 rules from YAML file\n"
     ]
    }
   ],
   "source": [
    "# Load rules from YAML file\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Path to the rules file\n",
    "rules_file_path = \"/app/notebooks/resources/rules_all_types.yaml\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(rules_file_path):\n",
    "    print(f\"âŒ Rules file not found at: {rules_file_path}\")\n",
    "    print(\"Available files in resources:\")\n",
    "    for file in os.listdir(\"/app/notebooks/resources\"):\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(f\"âœ… Loading rules from: {rules_file_path}\")\n",
    "    \n",
    "    with open(rules_file_path, 'r') as file:\n",
    "        rules_yaml = yaml.safe_load(file)\n",
    "    \n",
    "    print(f\"ðŸ“‹ Loaded {len(rules_yaml)} rules from YAML file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d50e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted 23 rules to DataFrame format\n"
     ]
    }
   ],
   "source": [
    "# Convert YAML rules to DataFrame format\n",
    "rules_data = []\n",
    "\n",
    "for rule_key, rule_value in rules_yaml.items():\n",
    "    rule_dict = {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": rule_value.get(\"table_name\", \"\"),\n",
    "        \"rule_type\": rule_value.get(\"rule_type\", \"\"),\n",
    "        \"rule\": rule_value.get(\"rule\", \"\"),\n",
    "        \"column_name\": rule_value.get(\"column_name\", \"\"),\n",
    "        \"expectation\": rule_value.get(\"expectation\", \"\"),\n",
    "        \"action_if_failed\": rule_value.get(\"action_if_failed\", \"\"),\n",
    "        \"tag\": rule_value.get(\"tag\", \"\"),\n",
    "        \"description\": rule_value.get(\"description\", \"\"),\n",
    "        \"enable_for_source_dq_validation\": rule_value.get(\"enable_for_source_dq_validation\", True),\n",
    "        \"enable_for_target_dq_validation\": rule_value.get(\"enable_for_target_dq_validation\", True),\n",
    "        \"is_active\": rule_value.get(\"is_active\", True),\n",
    "        \"enable_error_drop_alert\": rule_value.get(\"enable_error_drop_alert\", False),\n",
    "        \"error_drop_threshold\": rule_value.get(\"error_drop_threshold\", 0),\n",
    "        \"enable_querydq_custom_output\": rule_value.get(\"enable_querydq_custom_output\", False),\n",
    "        \"query_dq_delimiter\": rule_value.get(\"query_dq_delimiter\", None),\n",
    "        \"priority\": rule_value.get(\"priority\", \"medium\")\n",
    "    }\n",
    "    \n",
    "    # Update table names to use our config\n",
    "    if \"customers\" in rule_dict[\"table_name\"]:\n",
    "        rule_dict[\"table_name\"] = CONFIG[\"customers_table\"]\n",
    "    elif \"orders\" in rule_dict[\"table_name\"]:\n",
    "        rule_dict[\"table_name\"] = CONFIG[\"orders_table\"]\n",
    "    elif \"products\" in rule_dict[\"table_name\"]:\n",
    "        rule_dict[\"table_name\"] = CONFIG[\"products_table\"]\n",
    "    \n",
    "    rules_data.append(rule_dict)\n",
    "\n",
    "print(f\"âœ… Converted {len(rules_data)} rules to DataFrame format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94dfd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Rules Summary by Type:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|rule_type|count|\n",
      "+---------+-----+\n",
      "|   row_dq|   12|\n",
      "|   agg_dq|    7|\n",
      "| query_dq|    4|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "ðŸ“Š Rules Summary by Action:\n",
      "+----------------+-----+\n",
      "|action_if_failed|count|\n",
      "+----------------+-----+\n",
      "|          ignore|    6|\n",
      "|            drop|   11|\n",
      "|            fail|    6|\n",
      "+----------------+-----+\n",
      "\n",
      "\n",
      "ðŸ“Š Rules Summary by Tag:\n",
      "+------------+-----+\n",
      "|         tag|count|\n",
      "+------------+-----+\n",
      "| consistency|    6|\n",
      "|    validity|    3|\n",
      "|  uniqueness|    1|\n",
      "|    accuracy|    7|\n",
      "|  timeliness|    1|\n",
      "|completeness|    5|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create rules DataFrame and display summary\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "\n",
    "# Show summary of rules by type\n",
    "print(\"ðŸ“Š Rules Summary by Type:\")\n",
    "rules_df.groupBy(\"rule_type\").count().show()\n",
    "\n",
    "print(\"\\nðŸ“Š Rules Summary by Action:\")\n",
    "rules_df.groupBy(\"action_if_failed\").count().show()\n",
    "\n",
    "print(\"\\nðŸ“Š Rules Summary by Tag:\")\n",
    "rules_df.groupBy(\"tag\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd535d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ ALL DQ Rules loaded from yaml file:\n",
      "+-------------------+-------------------------------------------+---------+-----------------------+------------+--------------------------------------------------------------------------+----------------+------------+----------------------------------------------+-------------------------------+-------------------------------+---------+-----------------------+--------------------+----------------------------+------------------+--------+\n",
      "|product_id         |table_name                                 |rule_type|rule                   |column_name |expectation                                                               |action_if_failed|tag         |description                                   |enable_for_source_dq_validation|enable_for_target_dq_validation|is_active|enable_error_drop_alert|error_drop_threshold|enable_querydq_custom_output|query_dq_delimiter|priority|\n",
      "+-------------------+-------------------------------------------+---------+-----------------------+------------+--------------------------------------------------------------------------+----------------+------------+----------------------------------------------+-------------------------------+-------------------------------+---------+-----------------------+--------------------+----------------------------+------------------+--------+\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_customers|row_dq   |customer_id_not_null   |customer_id |customer_id IS NOT NULL                                                   |drop            |completeness|Customer ID must not be null                  |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_customers|row_dq   |email_valid_format     |email       |email rlike '^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'            |drop            |validity    |Email must be in valid format                 |true                           |true                           |true     |true                   |5                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_customers|row_dq   |email_not_empty        |email       |email IS NOT NULL AND email != ''                                         |drop            |completeness|Email cannot be null or empty                 |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |row_dq   |sales_greater_than_zero|sales       |sales > 0                                                                 |ignore          |accuracy    |Sales value must be positive                  |false                          |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |row_dq   |discount_range         |discount    |discount BETWEEN 0 AND 1                                                  |drop            |accuracy    |Discount must be between 0 and 1              |true                           |true                           |true     |true                   |10                  |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |row_dq   |quantity_positive      |quantity    |quantity > 0                                                              |drop            |accuracy    |Quantity must be positive                     |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |row_dq   |order_date_not_future  |order_date  |order_date <= current_timestamp()                                         |drop            |timeliness  |Order date cannot be in future                |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |row_dq   |ship_date_after_order  |ship_date   |ship_date >= order_date                                                   |drop            |consistency |Ship date must be after or equal to order date|true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |row_dq   |unique_order_id        |order_id    |count(*) over(partition by order_id order by 1) = 1                       |drop            |uniqueness  |Order ID must be unique                       |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |row_dq   |ship_mode_valid_values |ship_mode   |lower(trim(ship_mode)) in('first class', 'second class', 'standard class')|drop            |validity    |Ship mode must be in predefined list          |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_products |row_dq   |product_name_not_empty |product_name|product_name IS NOT NULL AND length(trim(product_name)) > 0               |drop            |completeness|Product name cannot be empty                  |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_products |row_dq   |product_price_positive |price       |price > 0                                                                 |drop            |accuracy    |Product price must be positive                |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |agg_dq   |total_sales_range      |sales       |sum(sales) BETWEEN 100000 AND 10000000                                    |fail            |consistency |Total sales must be within expected range     |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |agg_dq   |average_sales_check    |sales       |avg(sales) BETWEEN 50 AND 5000                                            |ignore          |consistency |Average sale must be reasonable               |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |agg_dq   |median_sales           |sales       |percentile_approx(sales, 0.5) BETWEEN 100 AND 1000                        |ignore          |consistency |Median sales should be between $100 and $1000 |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |agg_dq   |row_count_minimum      |*           |count(*) > 1000                                                           |fail            |completeness|Must have at least 1000 orders                |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |agg_dq   |distinct_customers     |customer_id |count(distinct customer_id) BETWEEN 100 AND 50000                         |ignore          |accuracy    |Distinct customer count should be in range    |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |agg_dq   |distinct_products      |product_id  |count(distinct product_id) BETWEEN 50 AND 10000                           |ignore          |accuracy    |Distinct products should be in range          |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |agg_dq   |max_discount           |discount    |max(discount) <= 0.5                                                      |fail            |accuracy    |Maximum discount should not exceed 50%        |true                           |true                           |true     |false                  |0                   |false                       |NULL              |medium  |\n",
      "|se_testuser_product|spark_catalog.default.se_testuser_orders   |query_dq |order_count_check      |*           |(select count(*) from order_source) > 10                                  |fail            |completeness|Order count must exceed threshold             |true                           |true                           |true     |false                  |0                   |true                        |NULL              |medium  |\n",
      "+-------------------+-------------------------------------------+---------+-----------------------+------------+--------------------------------------------------------------------------+----------------+------------+----------------------------------------------+-------------------------------+-------------------------------+---------+-----------------------+--------------------+----------------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all DQ PRO rules \n",
    "print(\"ðŸ“‹ ALL DQ Rules loaded from yaml file:\")\n",
    "rules_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d57761a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 04:26:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "ERROR:root:Exception while sending command.                       (23 + 2) / 50]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o105.saveAsTable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save rules to Delta table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrules_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrules_table\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Rules saved to table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrules_table\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Verify the table was created\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o105.saveAsTable"
     ]
    }
   ],
   "source": [
    "# Save rules to Delta table\n",
    "rules_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "print(f\"âœ… Rules saved to table: {CONFIG['rules_table']}\")\n",
    "\n",
    "# Verify the table was created\n",
    "spark.sql(f\"SELECT COUNT(*) as rule_count FROM {CONFIG['rules_table']}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e6277",
   "metadata": {},
   "source": [
    "### Create Sample Data for Testing\n",
    "We'll create realistic sample data for customers, orders, and products tables to test all our rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive sample data for CUSTOMERS table\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "customers_data = []\n",
    "for i in range(1, 101):  # Create 100 customers\n",
    "    customer_id = i\n",
    "    \n",
    "    # Mix of valid and invalid emails\n",
    "    if i % 10 == 0:  # 10% invalid emails\n",
    "        email = f\"invalid_email_{i}\"\n",
    "    elif i % 15 == 0:  # Some null emails\n",
    "        email = None\n",
    "    elif i % 20 == 0:  # Some empty emails\n",
    "        email = \"\"\n",
    "    else:\n",
    "        email = f\"customer{i}@example.com\"\n",
    "    \n",
    "    first_name = f\"FirstName{i}\" if i % 25 != 0 else None  # Some null first names\n",
    "    last_name = f\"LastName{i}\" if i % 30 != 0 else \"\"  # Some empty last names\n",
    "    phone = f\"555-{i:04d}\" if i % 12 != 0 else None  # Some null phones\n",
    "    \n",
    "    customers_data.append({\n",
    "        \"customer_id\": customer_id,\n",
    "        \"email\": email,\n",
    "        \"first_name\": first_name,\n",
    "        \"last_name\": last_name,\n",
    "        \"phone\": phone,\n",
    "        \"registration_date\": datetime.now() - timedelta(days=random.randint(1, 365))\n",
    "    })\n",
    "\n",
    "customers_df = spark.createDataFrame(pd.DataFrame(customers_data))\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['customers_table'])\n",
    "print(f\"âœ… Created {customers_df.count()} customer records in {CONFIG['customers_table']}\")\n",
    "customers_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive sample data for PRODUCTS table\n",
    "\n",
    "products_data = []\n",
    "categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home & Garden\", \"Sports\", \"Toys\"]\n",
    "\n",
    "for i in range(1, 201):  # Create 200 products\n",
    "    product_id = i\n",
    "    product_name = f\"Product {i}\" if i % 20 != 0 else None  # Some null product names\n",
    "    \n",
    "    # Mix of valid and invalid prices\n",
    "    if i % 15 == 0:  # Some negative prices (invalid)\n",
    "        price = -10.99\n",
    "    elif i % 25 == 0:  # Some zero prices (invalid)\n",
    "        price = 0\n",
    "    else:\n",
    "        price = round(random.uniform(10, 500), 2)\n",
    "    \n",
    "    category = random.choice(categories) if i % 30 != 0 else None  # Some null categories\n",
    "    stock_quantity = random.randint(0, 100)\n",
    "    \n",
    "    products_data.append({\n",
    "        \"product_id\": product_id,\n",
    "        \"product_name\": product_name,\n",
    "        \"category\": category,\n",
    "        \"price\": price,\n",
    "        \"stock_quantity\": stock_quantity\n",
    "    })\n",
    "\n",
    "products_df = spark.createDataFrame(pd.DataFrame(products_data))\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['products_table'])\n",
    "print(f\"âœ… Created {products_df.count()} product records in {CONFIG['products_table']}\")\n",
    "products_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive sample data for ORDERS table\n",
    "\n",
    "orders_data = []\n",
    "\n",
    "for i in range(1, 1501):  # Create 1500 orders\n",
    "    order_id = i\n",
    "    customer_id = random.randint(1, 100)\n",
    "    product_id = random.randint(1, 200)\n",
    "    \n",
    "    # Mix of valid and invalid sales amounts\n",
    "    if i % 18 == 0:  # Some negative sales (invalid)\n",
    "        sales = -50.0\n",
    "    elif i % 22 == 0:  # Some zero sales (invalid)\n",
    "        sales = 0.0\n",
    "    else:\n",
    "        sales = round(random.uniform(50, 5000), 2)\n",
    "    \n",
    "    # Mix of valid and invalid quantities\n",
    "    if i % 16 == 0:  # Some negative quantities (invalid)\n",
    "        quantity = -1\n",
    "    elif i % 24 == 0:  # Some zero quantities (invalid)\n",
    "        quantity = 0\n",
    "    else:\n",
    "        quantity = random.randint(1, 10)\n",
    "    \n",
    "    # Mix of valid and invalid discounts\n",
    "    if i % 14 == 0:  # Some discounts > 1 (invalid)\n",
    "        discount = 1.2\n",
    "    elif i % 28 == 0:  # Some negative discounts (invalid)\n",
    "        discount = -0.1\n",
    "    else:\n",
    "        discount = round(random.uniform(0, 0.3), 2)\n",
    "    \n",
    "    order_date = datetime.now() - timedelta(days=random.randint(1, 180))\n",
    "    \n",
    "    # Ship date logic - some invalid (before order date or in future)\n",
    "    if i % 11 == 0:  # Some ship dates before order date (invalid)\n",
    "        ship_date = order_date - timedelta(days=random.randint(1, 5))\n",
    "    elif i % 19 == 0:  # Some ship dates in future (valid, just shipped)\n",
    "        ship_date = order_date + timedelta(days=random.randint(1, 3))\n",
    "    else:\n",
    "        ship_date = order_date + timedelta(days=random.randint(1, 10))\n",
    "    \n",
    "    # Ship mode - mix of valid and invalid values\n",
    "    valid_ship_modes = [\"First Class\", \"Second Class\", \"Standard Class\"]\n",
    "    if i % 13 == 0:  # Some invalid ship modes\n",
    "        ship_mode = \"Invalid Mode\"\n",
    "    else:\n",
    "        ship_mode = random.choice(valid_ship_modes)\n",
    "    \n",
    "    revenue = sales * quantity * (1 - discount)\n",
    "    \n",
    "    orders_data.append({\n",
    "        \"order_id\": order_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"product_id\": product_id,\n",
    "        \"sales\": sales,\n",
    "        \"quantity\": quantity,\n",
    "        \"discount\": discount,\n",
    "        \"revenue\": revenue,\n",
    "        \"order_date\": order_date,\n",
    "        \"ship_date\": ship_date,\n",
    "        \"ship_mode\": ship_mode\n",
    "    })\n",
    "\n",
    "orders_df = spark.createDataFrame(pd.DataFrame(orders_data))\n",
    "orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['orders_table'])\n",
    "print(f\"âœ… Created {orders_df.count()} order records in {CONFIG['orders_table']}\")\n",
    "orders_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data quality issues summary\n",
    "print(\"ðŸ” Data Quality Issues Introduced for Testing:\\n\")\n",
    "\n",
    "print(\"CUSTOMERS Table:\")\n",
    "print(f\"  â€¢ Null emails: {customers_df.filter('email IS NULL').count()}\")\n",
    "print(f\"  â€¢ Empty emails: {customers_df.filter('email = \\\"\\\"').count()}\")\n",
    "print(f\"  â€¢ Invalid email format: ~{customers_df.filter('email NOT LIKE \\\"%@%\\\"').count()}\")\n",
    "print(f\"  â€¢ Null first names: {customers_df.filter('first_name IS NULL').count()}\")\n",
    "\n",
    "print(\"\\nPRODUCTS Table:\")\n",
    "print(f\"  â€¢ Null product names: {products_df.filter('product_name IS NULL').count()}\")\n",
    "print(f\"  â€¢ Invalid prices (<=0): {products_df.filter('price <= 0').count()}\")\n",
    "print(f\"  â€¢ Null categories: {products_df.filter('category IS NULL').count()}\")\n",
    "\n",
    "print(\"\\nORDERS Table:\")\n",
    "print(f\"  â€¢ Invalid sales (<=0): {orders_df.filter('sales <= 0').count()}\")\n",
    "print(f\"  â€¢ Invalid quantities (<=0): {orders_df.filter('quantity <= 0').count()}\")\n",
    "print(f\"  â€¢ Invalid discounts (<0 or >1): {orders_df.filter('discount < 0 OR discount > 1').count()}\")\n",
    "print(f\"  â€¢ Ship date before order date: {orders_df.filter('ship_date < order_date').count()}\")\n",
    "print(f\"  â€¢ Invalid ship modes: {orders_df.filter('LOWER(TRIM(ship_mode)) NOT IN (\\\"first class\\\", \\\"second class\\\", \\\"standard class\\\")').count()}\")\n",
    "\n",
    "print(\"\\nâœ… Sample data created with intentional quality issues for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c86f8c",
   "metadata": {},
   "source": [
    "### Create Temporary Views for Query DQ Rules\n",
    "Query DQ rules need temporary views for complex validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views for query_dq rules testing\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW order_source AS SELECT * FROM {CONFIG['orders_table']}\")\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW order_target AS SELECT * FROM {CONFIG['orders_table']}\")\n",
    "\n",
    "print(\"âœ… Created temporary views: order_source, order_target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a40bf",
   "metadata": {},
   "source": [
    "### Initialize Spark Expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512559ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spark Expectations configuration\n",
    "from spark_expectations.core import load_configurations\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "\n",
    "# Initialize default config\n",
    "load_configurations(spark)\n",
    "\n",
    "# Configure writer\n",
    "writer = WrappedDataFrameWriter().mode(\"overwrite\").format(\"delta\")\n",
    "\n",
    "# Stats streaming configuration\n",
    "stats_streaming_config_dict = {user_config.se_enable_streaming: False}\n",
    "\n",
    "print(\"âœ… Spark Expectations configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5180b63",
   "metadata": {},
   "source": [
    "### Test ROW_DQ Rules on Customers Table\n",
    "Testing row-level data quality rules with actions: drop, ignore, warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rules for customers table and row_dq type\n",
    "customers_rules_df = rules_df.filter(\n",
    "    (rules_df.table_name == CONFIG['customers_table']) & \n",
    "    (rules_df.rule_type == 'row_dq')\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“‹ Testing {customers_rules_df.count()} ROW_DQ rules on CUSTOMERS table:\")\n",
    "customers_rules_df.select(\"rule\", \"column_name\", \"expectation\", \"action_if_failed\", \"description\").show(truncate=False)\n",
    "\n",
    "# Initialize Spark Expectations for customers\n",
    "se_customers = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=customers_rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "# Apply expectations\n",
    "@se_customers.with_expectations(\n",
    "    target_table=CONFIG['customers_table'],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=False,\n",
    "    user_conf={},\n",
    "    target_table_view=\"customers_final\"\n",
    ")\n",
    "def process_customers():\n",
    "    return spark.table(CONFIG['customers_table'])\n",
    "\n",
    "# Execute\n",
    "result_customers_df = process_customers()\n",
    "print(f\"\\nâœ… Processed customers table. Result count: {result_customers_df.count()}\")\n",
    "result_customers_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce980ef",
   "metadata": {},
   "source": [
    "### Test ROW_DQ Rules on Orders Table\n",
    "Testing row-level rules on orders with multiple validation scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rules for orders table and row_dq type\n",
    "orders_row_rules_df = rules_df.filter(\n",
    "    (rules_df.table_name == CONFIG['orders_table']) & \n",
    "    (rules_df.rule_type == 'row_dq')\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“‹ Testing {orders_row_rules_df.count()} ROW_DQ rules on ORDERS table:\")\n",
    "orders_row_rules_df.select(\"rule\", \"column_name\", \"expectation\", \"action_if_failed\", \"description\").show(truncate=False)\n",
    "\n",
    "# Initialize Spark Expectations for orders\n",
    "se_orders_row = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=orders_row_rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "# Apply expectations\n",
    "@se_orders_row.with_expectations(\n",
    "    target_table=CONFIG['orders_table'],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=False,\n",
    "    user_conf={},\n",
    "    target_table_view=\"orders_final\"\n",
    ")\n",
    "def process_orders_row():\n",
    "    return spark.table(CONFIG['orders_table'])\n",
    "\n",
    "# Execute\n",
    "result_orders_row_df = process_orders_row()\n",
    "print(f\"\\nâœ… Processed orders table with ROW_DQ rules. Result count: {result_orders_row_df.count()}\")\n",
    "result_orders_row_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0c2df",
   "metadata": {},
   "source": [
    "### Test ROW_DQ Rules on Products Table\n",
    "Testing row-level rules on products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rules for products table and row_dq type\n",
    "products_row_rules_df = rules_df.filter(\n",
    "    (rules_df.table_name == CONFIG['products_table']) & \n",
    "    (rules_df.rule_type == 'row_dq')\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“‹ Testing {products_row_rules_df.count()} ROW_DQ rules on PRODUCTS table:\")\n",
    "products_row_rules_df.select(\"rule\", \"column_name\", \"expectation\", \"action_if_failed\", \"description\").show(truncate=False)\n",
    "\n",
    "# Initialize Spark Expectations for products\n",
    "se_products = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=products_row_rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "# Apply expectations\n",
    "@se_products.with_expectations(\n",
    "    target_table=CONFIG['products_table'],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=False,\n",
    "    user_conf={},\n",
    "    target_table_view=\"products_final\"\n",
    ")\n",
    "def process_products():\n",
    "    return spark.table(CONFIG['products_table'])\n",
    "\n",
    "# Execute\n",
    "result_products_df = process_products()\n",
    "print(f\"\\nâœ… Processed products table. Result count: {result_products_df.count()}\")\n",
    "result_products_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb951d8",
   "metadata": {},
   "source": [
    "### Test AGG_DQ Rules on Orders Table\n",
    "Testing aggregation-level data quality rules (sum, avg, count, min, max, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rules for orders table and agg_dq type\n",
    "orders_agg_rules_df = rules_df.filter(\n",
    "    (rules_df.table_name == CONFIG['orders_table']) & \n",
    "    (rules_df.rule_type == 'agg_dq')\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“‹ Testing {orders_agg_rules_df.count()} AGG_DQ rules on ORDERS table:\")\n",
    "orders_agg_rules_df.select(\"rule\", \"column_name\", \"expectation\", \"action_if_failed\", \"description\").show(truncate=False)\n",
    "\n",
    "# Initialize Spark Expectations for aggregation rules\n",
    "se_orders_agg = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=orders_agg_rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "# Apply expectations\n",
    "@se_orders_agg.with_expectations(\n",
    "    target_table=CONFIG['orders_table'],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=False,\n",
    "    user_conf={},\n",
    "    target_table_view=\"orders_agg_final\"\n",
    ")\n",
    "def process_orders_agg():\n",
    "    return spark.table(CONFIG['orders_table'])\n",
    "\n",
    "# Execute\n",
    "result_orders_agg_df = process_orders_agg()\n",
    "print(f\"\\nâœ… Processed orders table with AGG_DQ rules. Result count: {result_orders_agg_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952bf14",
   "metadata": {},
   "source": [
    "### Test QUERY_DQ Rules\n",
    "Testing custom query-based data quality rules for complex validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rules for query_dq type\n",
    "query_rules_df = rules_df.filter(rules_df.rule_type == 'query_dq')\n",
    "\n",
    "print(f\"ðŸ“‹ Testing {query_rules_df.count()} QUERY_DQ rules:\")\n",
    "query_rules_df.select(\"rule\", \"expectation\", \"action_if_failed\", \"enable_querydq_custom_output\", \"query_dq_delimiter\", \"description\").show(truncate=False)\n",
    "\n",
    "# Initialize Spark Expectations for query rules\n",
    "se_query = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=query_rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "# Apply expectations\n",
    "@se_query.with_expectations(\n",
    "    target_table=CONFIG['orders_table'],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=False,\n",
    "    user_conf={},\n",
    "    target_table_view=\"orders_query_final\"\n",
    ")\n",
    "def process_query_dq():\n",
    "    return spark.table(CONFIG['orders_table'])\n",
    "\n",
    "# Execute\n",
    "result_query_df = process_query_dq()\n",
    "print(f\"\\nâœ… Processed QUERY_DQ rules. Result count: {result_query_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370922cb",
   "metadata": {},
   "source": [
    "### View Statistics and Results\n",
    "Let's examine the statistics table to see how all our rules performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display statistics\n",
    "stats_df = spark.table(CONFIG['stats_table'])\n",
    "print(f\"ðŸ“Š Total statistics records: {stats_df.count()}\")\n",
    "\n",
    "# Show latest statistics\n",
    "stats_df.select(\n",
    "    \"rule_type\",\n",
    "    \"rule\",\n",
    "    \"action_if_failed\",\n",
    "    \"row_count\",\n",
    "    \"error_count\",\n",
    "    \"success_percentage\",\n",
    "    \"execution_status\"\n",
    ").orderBy(\"rule_type\", \"rule\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a85369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by rule type\n",
    "print(\"ðŸ“ˆ Summary by Rule Type:\")\n",
    "stats_df.groupBy(\"rule_type\", \"execution_status\").count().show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Summary by Action Type:\")\n",
    "stats_df.groupBy(\"action_if_failed\", \"execution_status\").count().show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Summary by Tag:\")\n",
    "stats_df.groupBy(\"tag\").agg({\"error_count\": \"sum\", \"row_count\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d375c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rules that failed\n",
    "print(\"âŒ Failed Rules:\")\n",
    "failed_stats = stats_df.filter(\"execution_status = 'Failed'\")\n",
    "if failed_stats.count() > 0:\n",
    "    failed_stats.select(\"rule\", \"rule_type\", \"action_if_failed\", \"error_count\", \"description\").show(truncate=False)\n",
    "else:\n",
    "    print(\"   No failed rules!\")\n",
    "\n",
    "print(\"\\nâš ï¸ Rules with Errors (but passed):\")\n",
    "error_stats = stats_df.filter(\"error_count > 0 AND execution_status = 'Passed'\")\n",
    "if error_stats.count() > 0:\n",
    "    error_stats.select(\"rule\", \"rule_type\", \"action_if_failed\", \"error_count\", \"success_percentage\").show(truncate=False)\n",
    "else:\n",
    "    print(\"   No rules with errors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76bac4",
   "metadata": {},
   "source": [
    "### Test Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive test summary\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ¯ DQ PRO RULES LOAD TEST - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_rules = stats_df.count()\n",
    "passed_rules = stats_df.filter(\"execution_status = 'Passed'\").count()\n",
    "failed_rules = stats_df.filter(\"execution_status = 'Failed'\").count()\n",
    "total_errors = stats_df.agg({\"error_count\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Statistics:\")\n",
    "print(f\"   â€¢ Total Rules Tested: {total_rules}\")\n",
    "print(f\"   â€¢ Rules Passed: {passed_rules}\")\n",
    "print(f\"   â€¢ Rules Failed: {failed_rules}\")\n",
    "print(f\"   â€¢ Total Errors Detected: {total_errors}\")\n",
    "print(f\"   â€¢ Success Rate: {(passed_rules/total_rules*100):.2f}%\")\n",
    "\n",
    "row_dq_count = stats_df.filter(\"rule_type = 'row_dq'\").count()\n",
    "agg_dq_count = stats_df.filter(\"rule_type = 'agg_dq'\").count()\n",
    "query_dq_count = stats_df.filter(\"rule_type = 'query_dq'\").count()\n",
    "\n",
    "print(f\"\\nðŸ“‹ Rules by Type:\")\n",
    "print(f\"   â€¢ ROW_DQ Rules: {row_dq_count}\")\n",
    "print(f\"   â€¢ AGG_DQ Rules: {agg_dq_count}\")\n",
    "print(f\"   â€¢ QUERY_DQ Rules: {query_dq_count}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Tables Created:\")\n",
    "print(f\"   â€¢ Rules Table: {CONFIG['rules_table']}\")\n",
    "print(f\"   â€¢ Stats Table: {CONFIG['stats_table']}\")\n",
    "print(f\"   â€¢ Customers Table: {CONFIG['customers_table']}\")\n",
    "print(f\"   â€¢ Orders Table: {CONFIG['orders_table']}\")\n",
    "print(f\"   â€¢ Products Table: {CONFIG['products_table']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… All rule types have been tested successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ea014",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook has successfully demonstrated:\n",
    "\n",
    "1. âœ… **Loading rules from YAML** - Imported comprehensive rules from `rules_all_types.yaml`\n",
    "2. âœ… **ROW_DQ Rules** - Tested row-level validations across customers, orders, and products\n",
    "3. âœ… **AGG_DQ Rules** - Tested aggregation rules for data consistency and accuracy\n",
    "4. âœ… **QUERY_DQ Rules** - Tested complex query-based validations\n",
    "5. âœ… **Multiple Action Types** - Demonstrated drop, ignore, warn, and fail actions\n",
    "6. âœ… **Comprehensive Data Quality** - Covered completeness, validity, accuracy, uniqueness, consistency, and timeliness\n",
    "\n",
    "**Key Features Tested:**\n",
    "- `enable_querydq_custom_output` - Enabled for query DQ rules\n",
    "- `priority` - Set for all rules (high/medium/low)\n",
    "- `query_dq_delimiter` - Configured for complex query DQ rules with parameters\n",
    "\n",
    "**Next Steps:**\n",
    "- Review the statistics table for detailed execution results\n",
    "- Analyze error tables to see specific data quality violations\n",
    "- Adjust rules and thresholds based on your data quality requirements\n",
    "- Integrate with notification systems (email, Slack, Teams) for alerts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
