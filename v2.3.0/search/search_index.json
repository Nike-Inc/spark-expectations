{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Spark-Expectations","text":"<p>Taking inspiration from DLT - data quality expectations: Spark-Expectations is built, so that the data quality rules can  run using decorator pattern while the spark job is in flight and Additionally, the framework able to perform data  quality checks when the data is at rest.</p>"},{"location":"#features-of-spark-expectations","title":"Features Of Spark Expectations","text":"<p>Please find the spark-expectations flow and feature diagrams below</p> <p> </p> <p> </p>"},{"location":"#concept","title":"Concept","text":"<p>Most of the data quality tools do the data quality checks or data validation on a table at rest and provide metrics in  different forms. <code>While the existing tools are good to do profiling and provide metrics, below are the problems that we  commonly see</code> </p> <ul> <li>The existing tools do not perform any action or remove the malformed data in the original table </li> <li>Most existing frameworks do not offer the capability to perform both row and column level data quality checks  within a single tool.</li> <li>User have to manually check the provided metrics, and it becomes cumbersome to find the records which doesn't meet  the data quality standards</li> <li>Downstream users have to consume the same data with error, or they have to do additional computation to remove the  records that doesn't meet the standards</li> <li>Another process is required as a corrective action to rectify the errors in the data and lot of planning is usually  required for this activity</li> </ul> <p><code>Spark-Expectations solves all of the above problems by following the below principles</code></p> <ul> <li>Spark Expectations provides the ability to run both individual row-based and overall aggregated data quality rules  on both the source and validated data sets. In case a rules fails, the row-level error is recorded in the <code>_error</code> table  and a summarized report of all failed aggregated data quality rules is compiled in the <code>_stats</code> table</li> <li>All the records which fail one or more data quality rules, are by default quarantined in an <code>_error</code> table along with  the metadata on rules that failed, job information etc. This helps analysts or products to look at the error data easily  and work with the teams required to correct the data and reprocess it easily</li> <li>Aggregated Metrics are provided on the job level along with necessary metadata so that recalculation or compute is  avoided</li> <li>The data that doesn't meet the data quality contract or the standards is not written into the final table unless or otherwise specified. </li> <li>By default, frameworks have the capability to send notifications only upon failure, but they have the ability to  send notifications at the start, as well as upon completion</li> </ul> <p>There is a field in the rules table called action_if_failed, which determines what needs to be done if a rule fails</p> <ul> <li>Let's consider a hypothetical scenario, where we have 100 columns and with 200 row level data quality rules, 10 aggregation data quality rules and 5 query data quality rules  computed against. When the dq job is run, there are 10 rules that failed on a particular row and 4 aggregation rules fails- what determines if that row should end up in  final table or not? Below are the hierarchy of checks that happens?</li> <li>Among the row level 10 rules failed, if there is at least one rule which has an action_if_failed as fail -    then the job will be failed </li> <li>Among the 10 row level rules failed, if there is no rule that has an action_if_failed as fail, but at least    has one rule with action_if_failed as drop - then the record/row will be dropped</li> <li>Among the 10 row level rules failed, if no rule neither has fail nor drop as an action_if_failed - then    the record will be end up in the final table. Note that, this record would also exist in the <code>_error</code> table</li> <li>The aggregation and query dq rules have a setting called <code>action_if_failed</code> with two options: <code>fail</code> or <code>ignore</code>. If any of   the 10 aggregation rules and 5 query dq rules which failed has an action_if_failed_as_fail, then the metadata summary will be    recorded in the <code>_stats</code> table and the job will be considered a failure. However, if none of the failed rules    has an action_if_failed_as_fail, then summary of the aggregated rules' metadata will still be collected in the    <code>_stats</code> table for failed aggregated and  query dq rules.</li> </ul>"},{"location":"Observability_examples/","title":"Spark Expectation Observability Features","text":""},{"location":"Observability_examples/#overview","title":"Overview","text":"<p>This document provides an overview of the observability features available in Spark for data quality (DQ) checks using Delta Lake. Observability in this context refers to the ability to monitor, measure, and understand the state and performance of data quality rules applied to datasets.</p>"},{"location":"Observability_examples/#required-attributes-for-enabling-observability-in-sample-dq-delta","title":"Required Attributes for Enabling Observability in Sample DQ Delta","text":"<p>To enable observability in a sample DQ Delta setup, the following attributes are required:</p> <pre><code>    user_config.se_enable_obs_dq_report_result: True,\nuser_config.se_dq_obs_alert_flag: True,\nuser_config.se_dq_obs_default_email_template: \"\"\n#also user need to pass the smtp details for sending mail.\nuser_config.se_notifications_email_smtp_host: \"smtp.######.com\",\nuser_config.se_notifications_email_smtp_port: 587,\nuser_config.se_notifications_email_from: \"a*****.obs@nike.com\",\nuser_config.se_notifications_email_to_other_mail_id: \"abc@mail.com\"\nuser_config.se_notifications_smtp_password: \"************\"\n</code></pre>"},{"location":"Observability_examples/#currently-our-abservability-system-supports-2-flows","title":"Currently our abservability system supports 2 flows:","text":""},{"location":"Observability_examples/#flow-1-dq-report-generation","title":"Flow 1: DQ Report Generation","text":"<p>When the DQ report flag is enabled (true), the system generates a report table upon the successful completion of Spark expectations. This flow focuses solely on report generation without triggering any alerts.</p>"},{"location":"Observability_examples/#flow-2-dq-report-generation-with-email-alerts","title":"Flow 2: DQ Report Generation with Email Alerts","text":"<p>When both the DQ report flag and alert flag are enabled (true), the system performs two actions: 1. Generates the report table after Spark expectations are completed. 2. Sends an email alert to the user-provided email address, notifying them of the results.</p>"},{"location":"Observability_examples/#key-highlights","title":"Key Highlights:","text":"<ul> <li>Flow 1: Report generation only (DQ flag = true).</li> <li>Flow 2: Report generation + email alerts (DQ flag = true, Alert flag = true).</li> <li>We dont need to create any report table it will be autogenerated along with the flow.</li> </ul> <p>The report table is derived from the Query DQ Output Table and the Detailed Table.  It is designed to calculate key metrics, numerical summaries, and other analytical insights. To ensure consistency and accuracy in the report table, users must adhere to predefined standards when writing queries for the **Query DQ Output Table. Below is the example of how the rules we can configure- <pre><code>RULES_DATA= \"\"\"\n(\"your_product\", \"dq_spark_dev.customer_order\", \"row_dq\", \"sales_greater_than_zero\", \"sales\", \"sales &gt; 2\", \"ignore\", \"accuracy\", \"sales value should be greater than zero\", false, true, true, false, 0,null, null)    ,(\"your_product\", \"dq_spark_{env}.customer_order\", \"row_dq\", \"discount_threshold\", \"discount\", \"discount*100 &lt; 60\",\"drop\", \"validity\", \"discount should be less than 40\", true, true, true, false, 0,null, null)\n,(\"your_product\", \"dq_spark_{env}.customer_order\", \"row_dq\", \"ship_mode_in_set\", \"ship_mode\", \"lower(trim(ship_mode)) in('second class', 'standard class', 'standard class')\", \"drop\", \"validity\", \"ship_mode mode belongs in the sets\", true, true, true, false, 0,null, null)\n,(\"your_product\", \"dq_spark_{env}.customer_order\", \"row_dq\", \"profit_threshold\", \"profit\", \"profit&gt;0\", \"ignore\", \"validity\", \"profit threshold should be greater tahn 0\", false, true, false, true, 0,null, null)\n,(\"your_product\", \"dq_spark_dev.customer_order\", \"query_dq\", \"product_missing_count_threshold\", \"column_name\", \"((select count(*) from ({source_f1}) a) - (select count(*) from ({target_f1}) b) ) &gt; 3@source_f1@SELECT DISTINCT product_id, order_id, order_date, COUNT(*) AS count FROM order_source GROUP BY product_id, order_id, order_date@target_f1@SELECT DISTINCT product_id, order_id, order_date, COUNT(*) AS count FROM order_target GROUP BY product_id, order_id, order_date\", \"ignore\", \"validity\", \"row count threshold\", true, false, true, false, 0,null, true)\n\"\"\"\n</code></pre></p>"},{"location":"Observability_examples/#template-options-for-report-table-rendering","title":"Template Options for Report Table Rendering","text":"<p>Users have two options for generating the report table:</p> <ol> <li>Custom Template:</li> <li>If the user provides a custom template through the specified attribute, the system will use this template to render the report table.</li> <li> <p>user_config.se_dq_obs_default_email_template: \"\"</p> </li> <li> <p>Default Jinja Template:</p> </li> <li>If no custom template is provided by the user, the system will automatically fall back to the default jinja template* for rendering the report table.</li> </ol>"},{"location":"Observability_examples/#sample-for-the-alert-received-in-the-mail","title":"Sample for the alert received in the mail","text":""},{"location":"bigquery/","title":"BigQuery","text":""},{"location":"bigquery/#example-write-to-delta","title":"Example - Write to Delta","text":"<p>Setup SparkSession for BigQuery to test in your local environment. Configure accordingly for higher environments. Refer to Examples in base_setup.py and bigquery.py</p> spark_session<pre><code>from pyspark.sql import SparkSession\nbuilder = (\nSparkSession.builder.config(\n\"spark.jars.packages\",\n\"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.30.0\",\n)\n)\nspark = builder.getOrCreate()\nspark._jsc.hadoopConfiguration().set(\n\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\"\n)\nspark.conf.set(\"viewsEnabled\", \"true\")\nspark.conf.set(\"materializationDataset\", \"&lt;temp_dataset&gt;\")\n</code></pre> <p>Below is the configuration that can be used to run SparkExpectations and write to Delta Lake</p> iceberg_write<pre><code>import os\nfrom pyspark.sql import DataFrame\nfrom spark_expectations.core.expectations import (\nSparkExpectations,\nWrappedDataFrameWriter,\n)\nfrom spark_expectations.config.user_config import Constants as user_config\nos.environ[\n\"GOOGLE_APPLICATION_CREDENTIALS\"\n] = \"path_to_your_json_credential_file\"  # This is needed for spark write to bigquery\nwriter = (\nWrappedDataFrameWriter().mode(\"overwrite\")\n.format(\"bigquery\")\n.option(\"createDisposition\", \"CREATE_IF_NEEDED\")\n.option(\"writeMethod\", \"direct\")\n)\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your_product\",\nrules_df=spark.read.format(\"bigquery\").load(\n\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;rules_table&gt;\"\n),\nstats_table=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;stats_table&gt;\",\nstats_table_writer=writer,\ntarget_and_error_table_writer=writer,\ndebugger=False,\nstats_streaming_options={user_config.se_enable_streaming: False}\n)\n#if smtp server needs to be authenticated, password can be passed directly with user config or set in a secure way like cerberus or databricks secret\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\",\nuser_config.cbs_url: \"htpps://cerberus.example.com\",\nuser_config.cbs_sdb_path: \"\",\nuser_config.cbs_smtp_password: \"\",\n# user_config.secret_type: \"databricks\",\n# user_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\",\n# user_config.dbx_secret_scope: \"your_secret_scope\",\n# user_config.dbx_smtp_password: \"your_password\",\n}\n# Commented fields are optional or required when notifications are enabled\nuser_conf = {\nuser_config.se_notifications_enable_email: False,\n# user_config.se_notifications_enable_smtp_server_auth: False,\n# user_config.se_notifications_enable_custom_email_body: True,\n# user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n# user_config.se_notifications_email_smtp_port: 25,\n# user_config.se_notifications_smtp_password: \"your_password\",\n# user_config.se_notifications_smtp_creds_dict: smtp_creds_dict,\n# user_config.se_notifications_email_from: \"\",\n# user_config.se_notifications_email_to_other_mail_id: \"\",\n# user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n# user_config.se_notifications_email_custom_body: \"Custom statistics: 'product_id': {}\",\nuser_config.se_notifications_enable_slack: False,\n# user_config.se_notifications_slack_webhook_url: \"\",\n# user_config.se_notifications_on_start: True,\n# user_config.se_notifications_on_completion: True,\n# user_config.se_notifications_on_fail: True,\n# user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n# user_config.se_notifications_on_error_drop_threshold: 15,\n# user_config.se_enable_error_table: True,\n# user_config.enable_query_dq_detailed_result: True,\n# user_config.enable_agg_dq_detailed_result: True,\n# user_config.se_dq_rules_params: { \"env\": \"local\", \"table\": \"product\", },\n}\n@se.with_expectations(\ntarget_table=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_name&gt;\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_view_name&gt;\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\nreturn _df_order\n</code></pre>"},{"location":"delta/","title":"Delta","text":""},{"location":"delta/#example-write-to-delta","title":"Example - Write to Delta","text":"<p>Setup SparkSession for Delta Lake to test in your local environment. Configure accordingly for higher environments. Refer to Examples in base_setup.py and  delta.py</p> spark_session<pre><code>from pyspark.sql import SparkSession\nbuilder = (\nSparkSession.builder.config(\n\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"\n)\n.config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\")\n.config(\n\"spark.sql.catalog.spark_catalog\",\n\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n)\n.config(\"spark.sql.warehouse.dir\", \"/tmp/hive/warehouse\")\n.config(\"spark.driver.extraJavaOptions\", \"-Dderby.system.home=/tmp/derby\")\n.config(\"spark.jars.ivy\", \"/tmp/ivy2\")\n)\nspark = builder.getOrCreate()\n</code></pre> <p>Below is the configuration that can be used to run SparkExpectations and write to Delta Lake</p> delta_write<pre><code>import os\nfrom pyspark.sql import DataFrame\nfrom spark_expectations.core.expectations import (\nSparkExpectations,\nWrappedDataFrameWriter,\n)\nfrom spark_expectations.config.user_config import Constants as user_config\nwriter = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your_product\",\nrules_df=spark.table(\"dq_spark_local.dq_rules\"),\nstats_table=\"dq_spark_local.dq_stats\",\nstats_table_writer=writer,\ntarget_and_error_table_writer=writer,\ndebugger=False,\nstats_streaming_options={user_config.se_enable_streaming: False}\n)\n#if smtp server needs to be authenticated, password can be passed directly with user config or set in a secure way like cerberus or databricks secret\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\",\nuser_config.cbs_url: \"htpps://cerberus.example.com\",\nuser_config.cbs_sdb_path: \"\",\nuser_config.cbs_smtp_password: \"\",\n# user_config.secret_type: \"databricks\",\n# user_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\",\n# user_config.dbx_secret_scope: \"your_secret_scope\",\n# user_config.dbx_smtp_password: \"your_password\",\n}\n# Commented fields are optional or required when notifications are enabled\nuser_conf = {\nuser_config.se_notifications_enable_email: False,\n# user_config.se_notifications_enable_smtp_server_auth: False,\n# user_config.se_notifications_enable_custom_email_body: True,\n# user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n# user_config.se_notifications_email_smtp_port: 25,\n# user_config.se_notifications_smtp_password: \"your_password\",\n# user_config.se_notifications_smtp_creds_dict: smtp_creds_dict,\n# user_config.se_notifications_email_from: \"\",\n# user_config.se_notifications_email_to_other_mail_id: \"\",\n# user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n# user_config.se_notifications_email_custom_body: \"Custom statistics: 'product_id': {}\",\nuser_config.se_notifications_enable_slack: False,\n# user_config.se_notifications_slack_webhook_url: \"\",\n# user_config.se_notifications_on_start: True,\n# user_config.se_notifications_on_completion: True,\n# user_config.se_notifications_on_fail: True,\n# user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n# user_config.se_notifications_on_error_drop_threshold: 15,\n# user_config.se_enable_error_table: True,\n# user_config.enable_query_dq_detailed_result: True,\n# user_config.enable_agg_dq_detailed_result: True,\n# user_config.se_dq_rules_params: { \"env\": \"local\", \"table\": \"product\", },\n}\n@se.with_expectations(\ntarget_table=\"dq_spark_local.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\nreturn _df_order\n</code></pre>"},{"location":"examples/","title":"Understand Args","text":""},{"location":"examples/#configurations","title":"Configurations","text":"<p>In order to establish the global configuration parameter for DQ Spark Expectations, you must define and complete the required fields within a variable. This involves creating a variable and ensuring that all the necessary information is provided in the appropriate fields.</p> <pre><code>from spark_expectations.config.user_config import Constants as user_config\nse_user_conf = {\nuser_config.se_notifications_enable_email: False,  # (1)!\nuser_config.se_notifications_enable_smtp_server_auth: False, # (2)!\nuser_config.se_notifications_enable_custom_email_body: False, # (3)\nuser_config.se_notifications_email_smtp_host: \"mailhost.com\",  # (4)!\nuser_config.se_notifications_email_smtp_port: 25,  # (5)!\nuser_config.se_notifications_smtp_password: \"your_password\",# (6)!\n# user_config.se_notifications_smtp_creds_dict: {\n#     user_config.secret_type: \"cerberus\",\n#     user_config.cbs_url: \"https://prod.cerberus.nikecloud.com\",\n#     user_config.cbs_sdb_path: \"your_sdb_path\",\n#     user_config.cbs_smtp_password: \"your_smtp_password\",\n# }, # (7)!\nuser_config.se_notifications_email_from: \"&lt;sender_email_id&gt;\",  # (8)!\nuser_config.se_notifications_email_to_other_mail_id: \"&lt;receiver_email_id's&gt;\",  # (9)!\nuser_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",  # (10)!\nuser_config.se_notifications_email_custom_body: \"custom stats: 'product_id': {}\", # (11)!\nuser_config.se_notifications_enable_slack: True,  # (12)!\nuser_config.se_notifications_slack_webhook_url: \"&lt;slack-webhook-url&gt;\",  # (13)!\nuser_config.se_notifications_on_start: True,  # (14)!\nuser_config.se_notifications_on_completion: True,  # (15)!\nuser_config.se_notifications_on_fail: True,  # (16)!\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,  # (17)!\nuser_config.se_notifications_on_rules_action_if_failed_set_ignore: True,  # (18)! \nuser_config.se_notifications_on_error_drop_threshold: 15,  # (19)!\nuser_config.se_enable_error_table: True,  # (20)!\nuser_config.enable_query_dq_detailed_result: True, # (21)!\nuser_config.enable_agg_dq_detailed_result: True, # (22)!\nuser_config.querydq_output_custom_table_name: \"&lt;catalog.schema.table-name&gt;\", #23\nuser_config.se_dq_rules_params: {\n\"env\": \"local\",\n\"table\": \"product\",\n}, # (24)!\n}\n}\n</code></pre> <ol> <li>The <code>user_config.se_notifications_enable_email</code> parameter, which controls whether notifications are sent via email, is set to false by default</li> <li>The <code>user_config.se_notifications_enable_smtp_server_auth</code> optional parameter, which controls whether SMTP server authentication is enabled, is set to false by default</li> <li>The <code>user_config.se_notifications_enable_custom_email_body</code> optional parameter, which controls whether custom email body is enabled, is set to false by default</li> <li>The <code>user_config.se_notifications_email_smtp_host</code> parameter is set to \"mailhost.com\" by default and is used to specify the email SMTP domain host</li> <li>The <code>user_config.se_notifications_email_smtp_port</code> parameter, which accepts a port number, is set to \"25\" by default</li> <li>The <code>user_config.se_notifications_smtp_password</code> parameter is used to specify the password for the SMTP server (if smtp_server requires authentication either this parameter or <code>user_config.se_notifications_smtp_creds_dict</code> should be set)</li> <li>The <code>user_config.se_notifications_smtp_creds_dict</code> parameter is used to specify the credentials for the SMTP server (if smtp_server requires authentication either this parameter or <code>user_config.se_notifications_smtp_password</code> should be set)</li> <li>The <code>user_config.se_notifications_email_from</code> parameter is used to specify the email ID that will trigger the email notification</li> <li>The <code>user_config.se_notifications_email_to_other_mail_id</code> parameter accepts a list of recipient email IDs</li> <li>The <code>user_config.se_notifications_email_subject</code> parameter captures the subject line of the email</li> <li>The <code>user_config.se_notifications_email_custom_body</code> optional parameter, captures the custom email body, need to be compliant with certain syntax</li> <li>The <code>user_config.se_notifications_enable_slack</code> parameter, which controls whether notifications are sent via slack, is set to false by default </li> <li>The <code>user_config/se_notifications_slack_webhook_url</code> parameter accepts the webhook URL of a Slack channel for sending notifications </li> <li>When <code>user_config.se_notifications_on_start</code> parameter set to <code>True</code> enables notification on start of the spark-expectations, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_completion</code> parameter set to <code>True</code> enables notification on completion of spark-expectations framework, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_fail</code> parameter set to <code>True</code> enables notification on failure of spark-expectations data quality framework, variable by default set to <code>True</code></li> <li>When <code>user_config.se_notifications_on_error_drop_exceeds_threshold_breach</code> parameter set to <code>True</code> enables notification when error threshold reaches above the configured value </li> <li>When <code>user_config.se_notifications_on_rules_action_if_failed_set_ignore</code> parameter set to <code>True</code> enables notification when rules action is set to ignore if failed </li> <li>The <code>user_config.se_notifications_on_error_drop_threshold</code> parameter captures error drop threshold value </li> <li>The <code>user_config.se_enable_error_table</code> parameter, which controls whether error data to load into error table, is set to true by default </li> <li>When <code>user_config.enable_query_dq_detailed_result</code> parameter set to <code>True</code>, enables the option to cature the query_dq detailed stats to detailed_stats table. By default set to <code>False</code></li> <li>When <code>user_config.enable_agg_dq_detailed_result</code> parameter set to <code>True</code>, enables the option to cature the agg_dq detailed stats to detailed_stats table. By default set to <code>False</code></li> <li>The <code>user_config.querydq_output_custom_table_name</code> parameter is used to specify the name of the custom query_dq output table which captures the output of the alias queries passed in the query dq expectation. Default is _custom_output  <li>The <code>user_config.se_dq_rules_params</code> parameter, which are required to dynamically update dq rules</li> <p>In case of SMTP server authentication, the password can be passed directly with the user config or set in a secure way like Cerberus or Databricks secret. If it is preferred to use Cerberus for secure password storage, the <code>user_config.se_notifications_smtp_creds_dict</code> parameter can be used to specify the credentials for the SMTP server in the following way: <pre><code>from spark_expectations.config.user_config import Constants as user_config\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\", # (1)!\nuser_config.cbs_url: \"https://prod.cerberus.nikecloud.com\", # (2)!\nuser_config.cbs_sdb_path: \"your_sdb_path\", # (3)!\nuser_config.cbs_smtp_password: \"your_smtp_password\", # (4)!\n}\n</code></pre> 1. The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) 2. The <code>user_config.cbs_url</code> used to pass Cerberus URL  3. The <code>user_config.cbs_sdb_path</code> captures Cerberus secure data store path  4. The <code>user_config.cbs_smtp_password</code> captures key for smtp_password in the Cerberus sdb</p> <p>Similarly, if it is preferred to use Databricks for secure password storage, the <code>user_config.se_notifications_smtp_creds_dict</code> parameter can be used to specify the credentials for the SMTP server in the following way: <pre><code>from spark_expectations.config.user_config import Constants as user_config\nsmtp_creds_dict = {\nuser_config.secret_type: \"databricks\", # (1)!\nuser_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\", # (2)!\nuser_config.dbx_secret_scope: \"your_secret_scope\", # (3)!\nuser_config.dbx_smtp_password: \"your_password\", # (4)!\n}\n</code></pre> 1. The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) 2. The <code>user_config.dbx_workspace_url</code> used to pass Databricks workspace in the format <code>https://&lt;workspace_name&gt;.cloud.databricks.com</code> 3. The <code>user_config.dbx_secret_scope</code> captures name of the secret scope 4. The <code>user_config.dbx_smtp_password</code> captures secret key for smtp password in the Databricks secret scope</p> <pre><code>### Spark Expectations Initialization \nFor all the below examples the below import and SparkExpectations class instantiation is mandatory\nWhen store for sensitive details is Databricks secret scope,construct config dictionary for authentication of Kafka and \navoid duplicate construction every time your project is initialized, you can create a dictionary with the following keys and their appropriate values. \nThis dictionary can be placed in the __init__.py file of your project or declared as a global variable.\n```python\nfrom typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.dbx_workspace_url  : \"https://workspace.cloud.databricks.com\", # (3)!\nuser_config.dbx_secret_scope: \"sole_common_prod\", # (4)!\nuser_config.dbx_kafka_server_url: \"se_streaming_server_url_secret_key\", # (5)!\nuser_config.dbx_secret_token_url: \"se_streaming_auth_secret_token_url_key\", # (6)!\nuser_config.dbx_secret_app_name: \"se_streaming_auth_secret_appid_key\", # (7)!\nuser_config.dbx_secret_token: \"se_streaming_auth_secret_token_key\", # (8)!\nuser_config.dbx_topic_name: \"se_streaming_topic_name\", # (9)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.dbx_workspace_url</code> used to pass Databricks workspace in the format <code>https://&lt;workspace_name&gt;.cloud.databricks.com</code></li> <li>The <code>user_config.dbx_secret_scope</code> captures name of the secret scope</li> <li>The <code>user_config.dbx_kafka_server_url</code> captures secret key for the Kafka URL</li> <li>The <code>user_config.dbx_secret_token_url</code> captures secret key for the Kafka authentication app URL</li> <li>The <code>user_config.dbx_secret_app_name</code> captures secret key for the Kafka authentication app name</li> <li>The <code>user_config.dbx_secret_token</code> captures secret key for the Kafka authentication app secret token</li> <li>The <code>user_config.dbx_topic_name</code> captures secret key for the Kafka topic name</li> </ol> <p>Similarly when sensitive store is Cerberus: </p> <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.cbs_url  : \"https://&lt;url&gt;.cerberus.com\", # (3)!\nuser_config.cbs_sdb_path: \"cerberus_sdb_path\", # (4)!\nuser_config.cbs_kafka_server_url: \"se_streaming_server_url_secret_sdb_path\", # (5)!\nuser_config.cbs_secret_token_url: \"se_streaming_auth_secret_token_url_sdb_path\", # (6)!\nuser_config.cbs_secret_app_name: \"se_streaming_auth_secret_appid_sdb_path\", # (7)!\nuser_config.cbs_secret_token: \"se_streaming_auth_secret_token_sdb_path\", # (8)!\nuser_config.cbs_topic_name: \"se_streaming_topic_name_sdb_path\", # (9)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.cbs_url</code> used to pass Cerberus URL</li> <li>The <code>user_config.cbs_sdb_path</code> captures Cerberus secure data store path</li> <li>The <code>user_config.cbs_kafka_server_url</code> captures path where Kafka URL stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_secret_token_url</code> captures path where Kafka authentication app stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_secret_app_name</code> captures path where Kafka authentication app name stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_secret_token</code> captures path where Kafka authentication app name secret token stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_topic_name</code>  captures path where Kafka topic name stored in the Cerberus sdb</li> </ol> <p>You can disable the streaming functionality by setting the <code>user_config.se_enable_streaming</code> parameter to <code>False</code> </p> <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: False, # (1)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> </ol> <pre><code>from spark_expectations.core.expectations import SparkExpectations\n# product_id should match with the \"product_id\" in the rules table\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your-products-id\", \nstats_streaming_options=stats_streaming_config_dict)  # (1)!\n</code></pre> <ol> <li>Instantiate <code>SparkExpectations</code> class which has all the required functions for running data quality rules</li> </ol>"},{"location":"examples/#example-1","title":"Example 1","text":"<pre><code>from spark_expectations.core.expectations import SparkExpectations, WrappedDataFrameWriter\nwriter = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")  # (1)!\nse = SparkExpectations(  # (10)!\nproduct_id=\"your_product\",  # (11)!\nrules_df=spark.table(\"dq_spark_local.dq_rules\"),  # (12)!\nstats_table=\"dq_spark_local.dq_stats\",  # (13)!\nstats_table_writer=writer,  # (14)!\ntarget_and_error_table_writer=writer,  # (15)!\ndebugger=False,  # (16)!\n# stats_streaming_options={user_config.se_enable_streaming: False},  # (17)!\n)\n@se.with_expectations(  # (2)!\nwrite_to_table=True,  # (3)!\nwrite_to_temp_table=True,  # (4)!\nuser_conf=se_user_conf,  # (5)!\ntarget_table_view=\"order\",  # (6)!\ntarget_and_error_table_writer=writer,  # (7)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")  # (8)!\nreturn _df_order  # (9)!\n</code></pre> <ol> <li>The <code>WrappedDataFrameWriter</code> class is used to wrap the <code>DataFrameWriter</code> class and add additional functionality to it</li> <li>The <code>@se.with_expectations</code> decorator is used to run the data quality rules</li> <li>The <code>write_to_table</code> parameter is used to write the final data into the table. By default, it is False. This is optional, if you just want to run the data quality checks. A good example will be a staging table or temporary view.</li> <li>The <code>write_to_temp_table</code> parameter is used to write the input dataframe into the temp table, so that it breaks the spark plan and might speed up the job in cases of complex dataframe lineage</li> <li>The <code>user_conf</code> parameter is utilized to gather all the configurations that are associated with notifications. There are four types of notifications: notification_on_start, notification_on_completion, notification_on_fail and notification_on_error_threshold_breach.    Enable notifications for all four stages by setting the values to <code>True</code>. By default, all four stages are set to <code>False</code></li> <li>The <code>target_table_view</code> parameter is used to provide the name of a view that represents the target validated dataset for implementation of <code>query_dq</code> on the clean dataset from <code>row_dq</code></li> <li>The <code>target_and_error_table_writer</code> parameter is used to write the final data into the table. By default, it is False. This is optional, if you just want to run the data quality checks. A good example will be a staging table or temporary view.</li> <li>View registration can be utilized when implementing <code>query_dq</code> expectations.</li> <li>Returning a dataframe is mandatory for the <code>spark_expectations</code> to work, if we do not return a dataframe - then an exception will be raised</li> <li>Instantiate <code>SparkExpectations</code> class which has all the required functions for running data quality rules</li> <li>The <code>product_id</code> parameter is used to specify the product ID of the data quality rules. This has to be a unique value</li> <li>The <code>rules_df</code> parameter is used to specify the dataframe that contains the data quality rules</li> <li>The <code>stats_table</code> parameter is used to specify the table name where the statistics will be written into</li> <li>The <code>stats_table_writer</code> takes in the configuration that need to be used to write the stats table using pyspark</li> <li>The <code>target_and_error_table_writer</code> takes in the configuration that need to be used to write the target and error table using pyspark</li> <li>The <code>debugger</code> parameter is used to enable the debugger mode</li> <li>The <code>stats_streaming_options</code> parameter is used to specify the configurations for streaming statistics into Kafka. To not use Kafka, uncomment this.</li> </ol>"},{"location":"iceberg/","title":"Iceberg","text":""},{"location":"iceberg/#example-write-to-delta","title":"Example - Write to Delta","text":"<p>Setup SparkSession for iceberg to test in your local environment. Configure accordingly for higher environments. Refer to Examples in base_setup.py and iceberg.py</p> spark_session<pre><code>from pyspark.sql import SparkSession\nbuilder = (\nSparkSession.builder.config(\n\"spark.jars.packages\",\n\"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1\",\n)\n.config(\n\"spark.sql.extensions\",\n\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n)\n.config(\n\"spark.sql.catalog.spark_catalog\",\n\"org.apache.iceberg.spark.SparkSessionCatalog\",\n)\n.config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n.config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/tmp/hive/warehouse\")\n.config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n.config(\"spark.sql.catalog.local.type\", \"hadoop\")\n.config(\"spark.sql.catalog.local.warehouse\", \"/tmp/hive/warehouse\")\n)\nspark = builder.getOrCreate()\n</code></pre> <p>Below is the configuration that can be used to run SparkExpectations and write to Delta Lake</p> iceberg_write<pre><code>import os\nfrom pyspark.sql import DataFrame\nfrom spark_expectations.core.expectations import (\nSparkExpectations,\nWrappedDataFrameWriter,\n)\nfrom spark_expectations.config.user_config import Constants as user_config\nwriter = WrappedDataFrameWriter().mode(\"append\").format(\"iceberg\")\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your_product\",\nrules_df=spark.sql(\"select * from dq_spark_local.dq_rules\"),\nstats_table=\"dq_spark_local.dq_stats\",\nstats_table_writer=writer,\ntarget_and_error_table_writer=writer,\ndebugger=False,\nstats_streaming_options={user_config.se_enable_streaming: False},\n)\n#if smtp server needs to be authenticated, password can be passed directly with user config or set in a secure way like cerberus or databricks secret\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\",\nuser_config.cbs_url: \"htpps://cerberus.example.com\",\nuser_config.cbs_sdb_path: \"\",\nuser_config.cbs_smtp_password: \"\",\n# user_config.secret_type: \"databricks\",\n# user_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\",\n# user_config.dbx_secret_scope: \"your_secret_scope\",\n# user_config.dbx_smtp_password: \"your_password\",\n}\n# Commented fields are optional or required when notifications are enabled\nuser_conf = {\nuser_config.se_notifications_enable_email: False,\n# user_config.se_notifications_enable_smtp_server_auth: False,\n# user_config.se_notifications_enable_custom_email_body: True,\n# user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n# user_config.se_notifications_email_smtp_port: 25,\n# user_config.se_notifications_smtp_password: \"your_password\",\n# user_config.se_notifications_smtp_creds_dict: smtp_creds_dict,\n# user_config.se_notifications_email_from: \"\",\n# user_config.se_notifications_email_to_other_mail_id: \"\",\n# user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n# user_config.se_notifications_email_custom_body: \"Custom statistics: 'product_id': {}\",\nuser_config.se_notifications_enable_slack: False,\n# user_config.se_notifications_slack_webhook_url: \"\",\n# user_config.se_notifications_on_start: True,\n# user_config.se_notifications_on_completion: True,\n# user_config.se_notifications_on_fail: True,\n# user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n# user_config.se_notifications_on_error_drop_threshold: 15,\n# user_config.se_enable_error_table: True,\n# user_config.enable_query_dq_detailed_result: True,\n# user_config.enable_agg_dq_detailed_result: True,\n# user_config.se_dq_rules_params: { \"env\": \"local\", \"table\": \"product\", },\n}\n@se.with_expectations(\ntarget_table=\"dq_spark_local.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\nreturn _df_order\n</code></pre>"},{"location":"api/sample_dq_bigquery/","title":"Sample dq bigquery","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery-attributes","title":"Attributes","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.current_dir","title":"<code>spark_expectations.examples.sample_dq_bigquery.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.dic_job_info","title":"<code>spark_expectations.examples.sample_dq_bigquery.dic_job_info = {'job': 'job_name', 'Region': 'NA', 'Snapshot': '2024-04-15'}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.job_info","title":"<code>spark_expectations.examples.sample_dq_bigquery.job_info = str(dic_job_info)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.se","title":"<code>spark_expectations.examples.sample_dq_bigquery.se: SparkExpectations = SparkExpectations(product_id='your_product', rules_df=spark.read.format('bigquery').load('&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;rules_table&gt;'), stats_table='&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;stats_table&gt;', stats_table_writer=writer, target_and_error_table_writer=writer, debugger=False)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.spark","title":"<code>spark_expectations.examples.sample_dq_bigquery.spark = set_up_bigquery('&lt;temp_dataset&gt;')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.user_conf","title":"<code>spark_expectations.examples.sample_dq_bigquery.user_conf = {user_config.se_notifications_enable_email: False, user_config.se_notifications_email_smtp_host: 'mailhost.com', user_config.se_notifications_email_smtp_port: 25, user_config.se_notifications_email_from: '', user_config.se_notifications_email_to_other_mail_id: '', user_config.se_notifications_email_subject: 'spark expectations - data quality - notifications', user_config.se_notifications_enable_slack: False, user_config.se_notifications_slack_webhook_url: '', user_config.se_notifications_on_start: True, user_config.se_notifications_on_completion: True, user_config.se_notifications_on_fail: True, user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True, user_config.se_notifications_on_error_drop_threshold: 15, user_config.se_enable_query_dq_detailed_result: True, user_config.se_enable_agg_dq_detailed_result: True, user_config.se_enable_error_table: True, user_config.se_dq_rules_params: {'env': 'local', 'table': 'product'}, user_config.se_job_metadata: job_info}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.writer","title":"<code>spark_expectations.examples.sample_dq_bigquery.writer = WrappedDataFrameWriter().mode('overwrite').format('bigquery').option('createDisposition', 'CREATE_IF_NEEDED').option('writeMethod', 'direct')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery-classes","title":"Classes","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery-functions","title":"Functions","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.build_new","title":"<code>spark_expectations.examples.sample_dq_bigquery.build_new() -&gt; DataFrame</code>","text":"Source code in <code>spark_expectations/examples/sample_dq_bigquery.py</code> <pre><code>@se.with_expectations(\ntarget_table=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_name&gt;\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_view_name&gt;\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")\nreturn _df_order\n</code></pre>"},{"location":"api/sample_dq_delta/","title":"Sample dq delta","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta-attributes","title":"Attributes","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.current_dir","title":"<code>spark_expectations.examples.sample_dq_delta.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.dic_job_info","title":"<code>spark_expectations.examples.sample_dq_delta.dic_job_info = {'job': 'job_name', 'Region': 'NA', 'env': 'dev', 'Snapshot': '2024-04-15', 'data_object_name ': 'customer_order'}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.job_info","title":"<code>spark_expectations.examples.sample_dq_delta.job_info = str(dic_job_info)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.se","title":"<code>spark_expectations.examples.sample_dq_delta.se: SparkExpectations = SparkExpectations(product_id='your_product', rules_df=spark.table('dq_spark_dev.dq_rules'), stats_table='dq_spark_dev.dq_stats', stats_table_writer=writer, target_and_error_table_writer=writer, debugger=False)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.spark","title":"<code>spark_expectations.examples.sample_dq_delta.spark = set_up_delta()</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.user_conf","title":"<code>spark_expectations.examples.sample_dq_delta.user_conf = {user_config.se_notifications_smtp_password: 'w*******', user_config.se_notifications_smtp_creds_dict: {user_config.secret_type: 'cerberus', user_config.cbs_url: 'https://prod.cerberus.nikecloud.com', user_config.cbs_sdb_path: 'your_sdb_path', user_config.cbs_smtp_password: 'your_smtp_password'}, user_config.se_notifications_enable_smtp_server_auth: False, user_config.se_enable_obs_dq_report_result: False, user_config.se_dq_obs_alert_flag: False, user_config.se_dq_obs_default_email_template: '', user_config.se_notifications_enable_email: False, user_config.se_notifications_enable_custom_email_body: False, user_config.se_notifications_email_smtp_host: 'smtp.office365.com', user_config.se_notifications_email_smtp_port: 587, user_config.se_notifications_email_from: 'a.dsm.*****.com', user_config.se_notifications_email_to_other_mail_id: 'abc@mail.com', user_config.se_notifications_email_subject: 'spark expectations - data quality - notifications', user_config.se_notifications_email_custom_body: \"Spark Expectations Statistics for this dq run:\\n    'product_id': {},\\n    'table_name': {},\\n    'source_agg_dq_results': {}',\\n    'dq_status': {}\", user_config.se_notifications_enable_slack: False, user_config.se_notifications_slack_webhook_url: '', user_config.se_notifications_on_start: False, user_config.se_notifications_on_completion: False, user_config.se_notifications_on_fail: False, user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True, user_config.se_notifications_on_error_drop_threshold: 15, user_config.se_enable_query_dq_detailed_result: True, user_config.se_enable_agg_dq_detailed_result: True, user_config.se_enable_error_table: True, user_config.se_dq_rules_params: {'env': 'dev', 'table': 'product', 'data_object_name': 'customer_order', 'data_source': 'customer_source', 'data_layer': 'Integrated'}, user_config.se_job_metadata: job_info}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.writer","title":"<code>spark_expectations.examples.sample_dq_delta.writer = WrappedDataFrameWriter().mode('append').format('delta')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta-classes","title":"Classes","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta-functions","title":"Functions","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.build_new","title":"<code>spark_expectations.examples.sample_dq_delta.build_new() -&gt; DataFrame</code>","text":"Source code in <code>spark_expectations/examples/sample_dq_delta.py</code> <pre><code>@se.with_expectations(\ntarget_table=\"dq_spark_dev.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_s.csv\"))\n)\n_df_order_source.createOrReplaceTempView(\"order_source\")\n_df_order_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_t.csv\"))\n)\n_df_order_target.createOrReplaceTempView(\"order_target\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_source.createOrReplaceTempView(\"customer_source\")\n_df_customer_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_target.createOrReplaceTempView(\"customer_target\")\nreturn _df_order_source\n</code></pre>"},{"location":"api/sample_dq_iceberg/","title":"Sample dq iceberg","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg-attributes","title":"Attributes","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.current_dir","title":"<code>spark_expectations.examples.sample_dq_iceberg.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.dic_job_info","title":"<code>spark_expectations.examples.sample_dq_iceberg.dic_job_info = {'job': 'job_name', 'Region': 'NA', 'Snapshot': '2024-04-15'}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.job_info","title":"<code>spark_expectations.examples.sample_dq_iceberg.job_info = str(dic_job_info)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.se","title":"<code>spark_expectations.examples.sample_dq_iceberg.se: SparkExpectations = SparkExpectations(product_id='your_product', rules_df=spark.sql('select * from dq_spark_local.dq_rules'), stats_table='dq_spark_local.dq_stats', stats_table_writer=writer, target_and_error_table_writer=writer, debugger=False, stats_streaming_options={user_config.se_enable_streaming: False})</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.spark","title":"<code>spark_expectations.examples.sample_dq_iceberg.spark = set_up_iceberg()</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.user_conf","title":"<code>spark_expectations.examples.sample_dq_iceberg.user_conf = {user_config.se_notifications_enable_email: False, user_config.se_notifications_email_smtp_host: 'mailhost.com', user_config.se_notifications_email_smtp_port: 25, user_config.se_notifications_email_from: '', user_config.se_notifications_email_to_other_mail_id: '', user_config.se_notifications_email_subject: 'spark expectations - data quality - notifications', user_config.se_notifications_enable_slack: False, user_config.se_notifications_slack_webhook_url: '', user_config.se_notifications_on_start: True, user_config.se_notifications_on_completion: True, user_config.se_notifications_on_fail: True, user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True, user_config.se_notifications_on_error_drop_threshold: 15, user_config.se_enable_query_dq_detailed_result: True, user_config.se_enable_agg_dq_detailed_result: True, user_config.se_enable_error_table: True, user_config.se_dq_rules_params: {'env': 'local', 'table': 'product'}, user_config.se_job_metadata: job_info}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.writer","title":"<code>spark_expectations.examples.sample_dq_iceberg.writer = WrappedDataFrameWriter().mode('append').format('iceberg')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg-classes","title":"Classes","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg-functions","title":"Functions","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.build_new","title":"<code>spark_expectations.examples.sample_dq_iceberg.build_new() -&gt; DataFrame</code>","text":"Source code in <code>spark_expectations/examples/sample_dq_iceberg.py</code> <pre><code>@se.with_expectations(\ntarget_table=\"dq_spark_local.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_s.csv\"))\n)\n_df_order_source.createOrReplaceTempView(\"order_source\")\n_df_order_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_t.csv\"))\n)\n_df_order_target.createOrReplaceTempView(\"order_target\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_source.createOrReplaceTempView(\"customer_source\")\n_df_customer_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_target.createOrReplaceTempView(\"customer_target\")\nreturn _df_order_source\n</code></pre>"},{"location":"configurations/configure_rules/","title":"Configure Rules","text":""},{"location":"configurations/configure_rules/#configure-rules-in-the-catalogschemaproduct_rules","title":"Configure Rules in the <code>catalog</code>.<code>schema</code>.<code>{product}_rules</code>","text":"<p>Please find the data set which used for the data quality rules setup order.csv</p>"},{"location":"configurations/configure_rules/#example-of-row-aggregation-and-query-rules-for-data-quality","title":"Example Of Row, Aggregation And Query Rules For Data Quality","text":"<p>To perform row data quality checks for artificially order table, please set up rules using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description, enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active, enable_error_drop_alert, error_drop_threshold ,query_dq_delimiter,enable_querydq_custom_output) values\n--The row data quality has been set on customer_id when customer_id is null, drop respective row into error table \n--as \"action_if_failed\" tagged \"drop\"\n('apla_nd', '`catalog`.`schema`.customer_order',  'row_dq', 'customer_id_is_not_null', 'customer_id', 'customer_id is not null','drop', 'validity', 'customer_id should not be null', false, false, true,false, 0,null, null)\n--The row data quality has been set on sales when sales is less than zero, drop respective row into error table as \n--'action_if_failed' tagged \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'sales_greater_than_zero', 'sales', 'sales &gt; 0', 'drop', 'accuracy', 'sales value should be greater than zero', false, false, true,false, 0,null, null)\n--The row data quality has been set on discount when discount is less than 60, drop respective row into error table\n--and final table  as \"action_if_failed\" tagged 'ignore'\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'discount_threshold', 'discount', 'discount*100 &lt; 60',\n'ignore', 'validity', 'discount should be less than 40', false, false, true,false, 0,null, null)\n--The row data quality has been set on ship_mode when ship_mode not in (\"second class\", \"standard class\", \n--\"standard class\"), drop respective row into error table and fail the framework  as \"action_if_failed\" tagged \"fail\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'ship_mode_in_set', 'ship_mode', 'lower(trim(ship_mode))\nin('second class', 'standard class', 'standard class')', 'fail', 'validity', 'ship_mode mode belongs in the sets',\nfalse, false, true,false, 0,null, null)\n--The row data quality has been set on profit when profit is less than or equals to 0, drop respective row into \n--error table and final table as \"action_if_failed\" tagged \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'profit_threshold', 'profit', 'profit&gt;0', 'ignore', 'validity', 'profit threshold should be greater than 0', false, false, true,false, 0,null, null)\n--The rule has been established to identify and remove completely identical records in which rows repeat with the \n--same value more than once, while keeping one instance of the row. Any additional duplicated rows will be dropped \n--into error table as action_if_failed set to \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'complete_duplicate', 'All', 'row_number() \n over(partition by customer_id, order_id order by 1)=1', 'drop', 'uniqueness', 'drop complete duplicate records', false, false, true,false, 0,null, null)\n</code></pre> <p>Please set up rules for checking the quality of the columns in the artificial order table, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description,  enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active, enable_error_drop_alert, error_drop_threshold ,query_dq_delimiter,enable_querydq_custom_output) values\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales values falls below 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales', 'sales', 'sum(sales)&gt;10000', 'ignore', 'validity', 'sum of sales must be greater than 10000',  true, true, true,false, 0,null, null)\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales values falls between 1000 and 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales_range_type1', 'sales', 'sum(sales) between 1000 and 10000', 'ignore', 'validity', 'sum of sales must be between 1000 and 1000',  true, true, true)\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales value is greater than 1000 and less than 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales_range_type2', 'sales', 'sum(sales)&gt;1000 and sum(sales)&lt;10000', 'ignore', 'validity', 'sum of sales must be greater than 1000 and less than 10000',  true, true, true)\n--The aggregation rule is established on the 'ship_mode' column and the metadata of the rule will be captured in \n--the statistics table when distinct ship_mode greater than 3 and enabled for only source data set\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'distinct_of_ship_mode', 'ship_mode', 'count(distinct ship_mode)&lt;=3', 'ignore', 'validity', 'regex format validation for quantity', true, false, true,false, 0,null, null)\n-- The aggregation rule is established on the table count and the metadata of the rule will be captured in the \n--statistics table when distinct count greater than 10000 and fails the job as \"action_if_failed\" set to \"fail\" \n--and enabled only for validated dataset\n,('apla_nd', '`catalog`.`schema`..customer_order', 'agg_dq', 'row_count', '*', 'count(*)&gt;=10000', 'fail', 'validity',\n'distinct ship_mode must be less or equals to 3', false, true, true,false, 0,null, null)\n</code></pre> <p>Please set up rules for checking the quality of artificially order table by implementing query data quality option, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description, enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active, enable_error_drop_alert, error_drop_threshold ,query_dq_delimiter,enable_querydq_custom_output) values\n--The query dq rule is established to check product_id difference between two table if difference is more than 20% \n--from source table, the metadata of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_missing_count_threshold', '*', '((select count(distinct product_id) from {table}) - (select count(distinct product_id) from order))&gt;\n(select count(distinct product_id) from product)*0.2', 'ignore', 'validity', 'row count threshold difference must \nbe less than 20%', true, true, true,false, 0,null, null)\n--The query dq rule is established to check distinct product_id in the product table is less than 5, if not the \n--metadata of the rule will be captured in the statistics table along with fails the job as \"action_if_failed\" is \n--\"fail\" and enabled for source dataset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_category', '*', '(select count(distinct category) \nfrom {table}) &lt; 5', 'fail', 'validity', 'distinct product category must be less than 5', true, False, true,false, 0,null, null)\n--The query dq rule is established to check count of the dataset should be less than 10000 other wise the metadata \n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled only for target dataset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'row_count_in_order', '*', '(select count(*) from order)&lt;10000', 'ignore', 'accuracy', 'count of the row in order dataset must be less then 10000', false, true, true,false, 0,null, null)\n--The query dq rule is established to check count of the unique productid and orderid between order_source and order_target dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"true\". This will capture the source_f1 and target_f1 alias queries in a query_dq custom output table. \n--This custom table can be passed in the user config : user_config.querydq_output_custom_table_name: &lt;catalog.schema.table_name&gt;. \n--If this value is not passed, the query_dq custom output table will be created by appending \"custom_output\" as suffix to \n--the stats_table parameter passed with SparkExpectation class invoke. \"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries.\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"product_missing_count_threshold\", \"*\", \"((select count(*) from ({source_f1}) a) - (select count(*) from ({target_f1}) b) ) &lt; 3$source_f1$select distinct product_id,order_id \nfrom order_source$target_f1$select distinct product_id,order_id from order_target\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0,null, true)\n--The query dq rule is established to check count of the customer_id counts between customer_source and customer_target dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"true\". This will capture the source_f1 and target_f1 alias queries in a query_dq custom output table. \n--This custom table can be passed in the user config : user_config.querydq_output_custom_table_name: &lt;catalog.schema.table_name&gt;.\n-- The alias can be anything but if it is passed in the format source_&lt;key_name&gt; and target_&lt;key_name&gt;, the query_dq custom output table will capture the the source and target in the same row which will help in easy comparision. \n--If user_config.querydq_output_custom_table_name is not passed, the query_dq custom output table will be created by appending \"custom_output\" as suffix to the stats_table \n--parameter passed with SparkExpectation class invoke. \"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries.\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"customer_missing_count_threshold\",\"*\", \"((select count(*) from ({source_f1}) a join ({source_f2}) b on a.customer_id = b.customer_id) - (select count(*) \nfrom ({target_f1}) a join ({target_f2}) b on a.customer_id = b.customer_id)) &gt; ({target_f3})$source_f1$select customer_id, count(*) \nfrom customer_source group by customer_id$source_f2$select customer_id, \ncount(*) from order_source group by customer_id$target_f1$select customer_id, count(*) from customer_target \ngroup by customer_id$target_f2$select customer_id, count(*) from order_target group by customer_id$target_f3$select count(*) \nfrom order_source\", \"ignore\", \"validity\", \"customer count threshold\", true, true, true, false, 0,null, true)\n--The query dq rule is established to check count of the unique productid and orderid between order_source and order_target dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"false\". This will not capture the source_f1 alias query to query_dq custom output table. \n--\"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries. Default value is '$'. Her it is overrided with '@'\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"order_count_validity\", \"*\", \"({source_f1}) &gt; 10@source_f1@select count(*) \nfrom order_source\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0, \"@\", false)\n--The query dq rule is established to check count of the order_source dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"true\". Though it is \"true\" since there is no alias query in the expectaion, \n--there will not be any output to be captured in the query_dq custom output table. \n-- \"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries. Default value is '$'.\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"order_count_validity_check\", \"*\", \"(select count(*) \nfrom order_source) &gt; 10\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0, null, true)\n</code></pre>"},{"location":"configurations/databricks_setup_guide/","title":"Databricks setup guide","text":""},{"location":"configurations/databricks_setup_guide/#effortlessly-explore-spark-expectations-on-example-dataset-with-automated-setup-in-databricks","title":"Effortlessly Explore Spark Expectations on Example Dataset with Automated Setup in Databricks","text":"<p>This section provides instructions on how to set up a sample notebook in the Databricks environment to investigate, comprehend, and conduct a feasibility study on the Spark Expectations framework.</p>"},{"location":"configurations/databricks_setup_guide/#prerequisite","title":"Prerequisite:","text":"<ol> <li>Recommended Databricks run time environment for better experience - DBS 11.0 and above</li> <li>Please install the Kafka jar using the path <code>dbfs:/kafka-jars/databricks-shaded-strimzi-kafka-oauth-client-1.1.jar</code>, If the jar is not available in the dbfs location, please raise a ticket with Platform team to add the jar to your workspace</li> <li>Please follow the steps provided here to integrate and clone repo from git Databricks</li> <li>Please follow the steps to create the webhook-hook URL for team-specific channel here</li> </ol>"},{"location":"configurations/migration_versions_comparison/","title":"comparison","text":"<p>Please find the difference in the changes with different version, latest three versions changes are documented</p>"},{"location":"configurations/migration_versions_comparison/#modifications-made-to-the-version-during-implementation-or-integration","title":"Modifications made to the version during implementation or integration","text":"stage 0.8.0 1.0.0 1.2.0 rules table schema changes added additional two column  1.<code>enable_error_drop_alert(boolean)</code>  2.<code>error_drop_threshold(int)</code>  documentation found here Remains same Remains same rule table creation required yes - creation not required if you're upgrading from old version but schema changes required yes - creation not required if you're upgrading from old version but schema changes required Remains same. Additionally dq rules dynamically updates based on the parameter passed from externally stats table schema changes remains same Remains Same Remains same. Additionally all row dq rules stats get in row dq rules summary stats table creation required automated Remains Same Remains same notification config setting remains same Remains Same Remains same secret store and Kafka authentication details Create a dictionary that contains your secret configuration values and register in <code>__init__.py</code> for multiple usage, example Remains Same. You can disable streaming if needed, in SparkExpectations class Remains same spark expectations initialization create spark expectations class object using <code>SparkExpectations</code> by passing <code>product_id</code> and additional optional parameter <code>debugger</code>, <code>stats_streaming_options</code> example New arguments are added. Please follow this - example Remains same with_expectations decorator remains same New arguments are added. Please follow this - example Remains same WrappedDataFrameWriter Doesn't exist This is new and users need to provider the writer object to record the spark conf that need to be used while writing - example Remains same"},{"location":"configurations/rules/","title":"Rules","text":""},{"location":"configurations/rules/#different-types-of-expectations","title":"Different Types of Expectations","text":"<p>Please find the different types of possible expectations </p>"},{"location":"configurations/rules/#possible-row-data-quality-expectations","title":"Possible Row Data Quality Expectations","text":"rule_description category tag rule_expectation Expect that the values in the column should not be null/empty null_validation completeness <code>[col_name] is not null</code> Ensure that the primary key values are unique and not duplicated primary_key_validation uniqueness <code>count(*) over(partition by [primary_key_or_combination_of_primary_key] order by 1)=1</code> Perform a thorough check to make sure that there are no duplicate values, if there are duplicates preserve one row into target complete_duplicate_validation uniqueness <code>row_number() over(partition by [all_the_column_in_dataset_b_ comma_separated] order by 1)=1</code> Verify that the date values are in the correct format date_format_validation validity <code>to_date([date_col_name], '[mention_expected_date_format]') is not null</code> Verify that the date values are in the correct format using regex date_format_validation_with_regex validity <code>[date_col_name] rlike '[regex_format_of_date]'</code> Expect column value is date parseable expect_column_values_to_be_date_parseable validity <code>try_cast([date_col_name] as date)</code> Verify values in a column to conform to a specified regular expression pattern expect_column_values_to_match_regex validity <code>[col_name]  rlike '[regex_format]'</code> Verify values in a column to not conform to a specified regular expression pattern expect_column_values_to_not_match_regex validity <code>[col_name] not rlike '[regex_format]'</code> Verify values in a column to match regex in list expect_column_values_to_match_regex_list validity <code>[col_name] not rlike '[regex format1]' or [col_name] not rlike '[regex_format2]' or [col_name] not rlike '[regex_format3]'</code> Expect the values in a column to belong to a specified set expect_column_values_to_be_in_set accuracy <code>[col_name] in ([values_in_comma_separated])</code> Expect the values in a column not to belong to a specified set expect_column_values_to_be_not_in_set accuracy <code>[col_name] not in ([values_in_comma_separated])</code> Expect the values in a column to fall within a defined range expect_column_values_to_be_in_range accuracy <code>[col_name] between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be within a specified range expect_column_value_lengths_to_be_between accuracy <code>length([col_name]) between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be equal to a certain value expect_column_value_lengths_to_be_equal accuracy <code>length([col_name])=[threshold]</code> Expect values in the column to exceed a certain limit expect_column_value_to_be_greater_than accuracy <code>[col_name] &gt; [threshold_value]</code> Expect values in the column  not to exceed a certain limit expect_column_value_to_be_lesser_than accuracy <code>[col_name] &lt; [threshold_value]</code> Expect values in the column to be equal to or exceed a certain limit expect_column_value_greater_than_equal accuracy <code>[col_name] &gt;= [threshold_value]</code> Expect values in the column to be equal to or not exceed a certain limit expect_column_value_lesser_than_equal accuracy <code>[col_name] &lt;= [threshold_value]</code> Expect values in column A to be greater than values in column B expect_column_pair_values_A_to_be_greater_than_B accuracy <code>[col_A] &gt; [col_B]</code> Expect values in column A to be lesser than values in column B expect_column_pair_values_A_to_be_lesser_than_B accuracy <code>[col_A] &lt; [col_B]</code> Expect values in column A to be greater than or equals to values in column B expect_column_A_to_be_greater_than_B accuracy <code>[col_A] &gt;= [col_B]</code> Expect values in column A to be lesser than or equals to values in column B expect_column_A_to_be_lesser_than_or_equals_B accuracy <code>[col_A] &lt;= [col_B]</code> Expect the sum of values across multiple columns to be equal to a certain value expect_multicolumn_sum_to_equal accuracy <code>[col_1] + [col_2] + [col_3] = [threshold_value]</code> Expect sum of values in each category equals certain value expect_sum_of_value_in_subset_equal accuracy <code>sum([col_name]) over(partition by [category_col] order by 1)</code> Expect count of values in each category equals certain value expect_count_of_value_in_subset_equal accuracy <code>count(*) over(partition by [category_col] order by 1)</code> Expect distinct value in each category exceeds certain range expect_distinct_value_in_subset_exceeds accuracy <code>count(distinct [col_name]) over(partition by [category_col] order by 1)</code>"},{"location":"configurations/rules/#possible-aggregation-data-quality-expectations","title":"Possible Aggregation Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column that are present in a given list expect_column_distinct_values_to_be_in_set accuracy <code>array_intersect(collect_list(distinct [col_name]), Array($compare_values_string)) = Array($compare_values_string)</code> Expect the mean value of a column to fall within a specified range expect_column_mean_to_be_between consistency <code>avg([col_name]) between [lower_bound] and [upper_bound]</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>percentile_approx([column_name], 0.5) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>stddev([col_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>count(distinct [col_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>max([col_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_sum_to_be_between accuracy <code>min([col_name]) between [lower_bound] and [upper_bound]</code> Expect row count of the dataset fall within certain range expect_row_count_to_be_between accuracy <code>count(*) between [lower_bound] and [upper_bound]</code> Expect row count of the dataset fall within certain range expect_row_count_to_be_in_range accuracy <code>count(*) &gt;[lower_bound] and count(*) &lt; [upper_bound]</code>"},{"location":"configurations/rules/#possible-query-data-quality-expectations","title":"Possible Query Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column must be greater than threshold value expect_column_distinct_values_greater_than_threshold_value accuracy <code>(select count(distinct [col_name]) from [table_name]) &gt; [threshold_value]</code> Expect count between two table or view must be same expect_count_between_two_table_same consistency <code>(select count(*) from [table_a]) = (select count(*) from [table_b])</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>(select percentile_approx([column_name], 0.5) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>(select stddev([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>(select count(distinct [col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>(select max([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_min_to_be_between accuracy <code>(select min([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect referential integrity expect_referential_integrity_between_two_table_should_be_less_than_100 accuracy <code>( select * from [table_a] left join [table_b] on [condition] where [table_b.column] is null) select count(*) from refrentail_check) &lt; 100</code> Compare the source table and target table output (by default @ is the delimiter. can be overriden by query_dq_delimiter atttribute in rules table) customer_missing_count_threshold validity The_alias_within_the_curly_bracket_is_added_to_the_expectation_which_gets_resolved_at_compile_time_with_alias_values<code>((select count(*) from ({source_f1}) a join ({source_f2}) b on a.customer_id = b.customer_id) - (select count(*) from ({target_f1}) a join ({target_f2}) b on source_column = target_column)) &gt; ({target_f3})@source_f1@select column, count(*) from source_tbl group by column@source_f2@select column2, count(*) from table2 group by column2@target_f1@select column, count(*) from target_tbl  group by column@target_f2@select column2, count(*) from target_tbl2 group by column2@target_f3@select count(*) from source_tbl</code>"},{"location":"getting-started/setup/","title":"Setup","text":""},{"location":"getting-started/setup/#installation","title":"Installation","text":"<p>The library is available in the Python Package Index (PyPi) and can be installed in your environment using the below command or   add the library \"spark-expectations\" into the requirements.txt or poetry dependencies.</p> <pre><code>pip install -U spark-expectations\n</code></pre>"},{"location":"getting-started/setup/#required-tables","title":"Required Tables","text":"<p>There are two tables that need to be created for spark-expectations to run seamlessly and integrate with a spark job. The below SQL statements used three namespaces which works with Databricks Unity Catalog, but if you are using hive please update the namespaces accordingly and also provide necessary table metadata.</p>"},{"location":"getting-started/setup/#rules-table","title":"Rules Table","text":"<p>We need to create a rules tables which contains all the data quality rules. Please use the below template to create your rules table for your project.</p> <pre><code>create table if not exists `catalog`.`schema`.`{product}_rules` (\nproduct_id STRING,  -- (1)!\ntable_name STRING,  -- (2)!\nrule_type STRING,  -- (3)!\nrule STRING,  -- (4)!\ncolumn_name STRING,  -- (5)!\nexpectation STRING,  -- (6)!\naction_if_failed STRING,  -- (7)!\ntag STRING,  -- (8)!\ndescription STRING,  -- (9)!\nenable_for_source_dq_validation BOOLEAN,  -- (10)! \nenable_for_target_dq_validation BOOLEAN,  -- (11)!\nis_active BOOLEAN,  -- (12)!\nenable_error_drop_alert BOOLEAN,  -- (13)!\nerror_drop_threshold INT,  -- (14)!\nquery_dq_delimiter STRING,  -- (15)!\nenable_querydq_custom_output BOOLEAN,  -- (16)!\n);\n</code></pre> <ol> <li><code>product_id</code> A unique name at the level of dq rules execution</li> <li><code>table_name</code> The table for which the rule is being defined for</li> <li><code>rule_type</code> 3 different type of rules. They are 'row_dq', 'agg_dq' and 'query_dq'</li> <li><code>rule</code> Short description of the rule </li> <li><code>column_name</code> The column name for which the rule is defined for. This only applies for row_dq. For agg_dq and query_dq, use blank/empty value. </li> <li><code>expectation</code> Provide the DQ rule condition </li> <li><code>action_if_failed</code> There are 3 different types of actions. These are 'ignore', 'drop', and 'fail'.      Ignore: The rule is run and the output is logged. No action is performed regardless of whether the rule has succeeded or failed. Applies for all 3 rule types.      Drop: The rows that fail the rule get dropped from the dataset. Applies for only row_dq rule type.     Fail: job fails if the rule fails. Applies for all 3 rule types.</li> <li><code>tag</code> provide some tag name to dq rule example:  completeness, validity, uniqueness etc. </li> <li><code>description</code>  Long description for the rule</li> <li><code>enable_for_source_dq_validation</code> flag to run the agg rule</li> <li><code>enable_for_target_dq_validation</code> flag to run the query rule</li> <li><code>is_active</code> true or false to indicate if the rule is active or not. </li> <li><code>enable_error_drop_alert</code> true or false. This determines if an alert notification should be sent out if row(s) is(are) dropped from the data set</li> <li><code>error_drop_threshold</code> Threshold for the alert notification that gets triggered when row(s) is(are) dropped from the data set</li> <li><code>query_dq_delimiter</code> segregate custom queries delimiter ex: $, @ etc. By default it is @. Users can override it with any other delimiter based on the need. The same delimiter mentioned here has to be used in the custom query.</li> <li><code>enable_querydq_custom_output</code> required custom query output in separate table</li> </ol> <p>The Spark Expectation process consists of three phases: 1. When enable_for_source_dq_validation is true, execute agg_dq and query_dq on the source Dataframe 2. If the first step is successful, proceed to run row_dq 3. When enable_for_target_dq_validation is true, exeucte agg_dq and query_dq on the Dataframe resulting from row_dq</p>"},{"location":"getting-started/setup/#rule-type-for-rules","title":"Rule Type For Rules","text":"<p>The rules column has a column called \"rule_type\". It is important that this column should only accept one of  these three values - <code>[row_dq, agg_dq, query_dq]</code>. If other values are provided, the library may cause unforeseen errors. Please run the below command to add constraints to the above created rules table</p> <pre><code>ALTER TABLE `catalog`.`schema`.`{product}_rules` ADD CONSTRAINT rule_type_action CHECK (rule_type in ('row_dq', 'agg_dq', 'query_dq'));\n</code></pre>"},{"location":"getting-started/setup/#action-if-failed-for-row-aggregation-and-query-data-quality-rules","title":"Action If Failed For Row, Aggregation and Query Data Quality Rules","text":"<p>The rules column has a column called \"action_if_failed\". It is important that this column should only accept one of  these values - <code>[fail, drop or ignore]</code> for <code>'rule_type'='row_dq'</code> and <code>[fail, ignore]</code> for <code>'rule_type'='agg_dq' and 'rule_type'='query_dq'</code>.  If other values are provided, the library may cause unforeseen errors. Please run the below command to add constraints to the above created rules table</p> <pre><code>ALTER TABLE apla_nd_dq_rules ADD CONSTRAINT action CHECK ((rule_type = 'row_dq' and action_if_failed IN ('ignore', 'drop', 'fail')) or (rule_type = 'agg_dq' and action_if_failed in ('ignore', 'fail')) or (rule_type = 'query_dq' and action_if_failed in ('ignore', 'fail')));\n</code></pre>"},{"location":"getting-started/setup/#dq-stats-table","title":"DQ Stats Table","text":"<p>In order to collect the stats/metrics for each data quality job run, the spark-expectations job will automatically create the stats table if it does not exist. The below SQL statement can be used to create the table if you want to create it manually, but it is not recommended.</p> <pre><code>create table if not exists `catalog`.`schema`.`dq_stats` (\nproduct_id STRING,  -- (1)!\ntable_name STRING,  -- (2)!\ninput_count LONG,  -- (3)!\nerror_count LONG,  -- (4)!\noutput_count LONG,  -- (5)!\noutput_percentage FLOAT,  -- (6)!\nsuccess_percentage FLOAT,  -- (7)!\nerror_percentage FLOAT,  -- (8)!\nsource_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (9)!\nfinal_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (10)!\nsource_query_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (11)!\nfinal_query_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (12)!\nrow_dq_res_summary array&lt;map&lt;string, string&gt;&gt;,  -- (13)!\nrow_dq_error_threshold array&lt;map&lt;string, string&gt;&gt;,  -- (14)!\ndq_status map&lt;string, string&gt;,  -- (15)!\ndq_run_time map&lt;string, float&gt;,  -- (16)!\ndq_rules map&lt;string, map&lt;string,int&gt;&gt;,  -- (17)!\nmeta_dq_run_id STRING,  -- (18)!\nmeta_dq_run_date DATE,  -- (19)!\nmeta_dq_run_datetime TIMESTAMP,  -- (20)!\n);\n</code></pre> <ol> <li><code>product_id</code> A unique name at the level of dq rules execution</li> <li><code>table_name</code> The table for which the rule is being defined for</li> <li><code>input_count</code> total input row count of given dataframe</li> <li><code>error_count</code> total error count for all row_dq rules</li> <li><code>output_count</code> total count of records that passed the row_dq rules or configured to be ignored when they fail</li> <li><code>output_percentage</code> percentage of total count of records that passed the row_dq rules or configured to be ignored when they fail</li> <li><code>success_percentage</code> percentage of total count of records that passed the row_dq rules</li> <li><code>error_percentage</code> percentage of total count of records that failed the row_dq rules</li> <li><code>source_agg_dq_results</code> results for agg dq rules are stored</li> <li><code>final_agg_dq_results</code> results for agg dq rules are stored after row_dq rules executed</li> <li><code>source_query_dq_results</code> results for query dq rules are stored</li> <li><code>final_query_dq_results</code> results for query dq rules are stored after row_dq rules executed</li> <li><code>row_dq_res_summary</code> summary of row dq results are stored</li> <li><code>row_dq_error_threshold</code> threshold for rules defined in the rules table for row_dq rules</li> <li><code>dq_status</code>  stores the status of the rule execution.</li> <li><code>dq_run_time</code> time taken by the rules</li> <li><code>dq_rules</code> how many dq rules are executed in this run</li> <li><code>meta_dq_run_id</code> unique id generated for this run</li> <li><code>meta_dq_run_date</code> date on which rule is executed</li> <li><code>meta_dq_run_datetime</code> date and time on which rule is executed</li> </ol>"},{"location":"getting-started/setup/#dq-detailed-stats-table","title":"DQ Detailed Stats Table","text":"<p>This table provides detailed stats of all the expectations along with the status provided in the stats table in a relational format. This table need not be created. It gets auto created with \"_detailed \" to the dq stats table name. This is optional and only get's created if the config is set to have the detailed stats table. Below is the schema</p> <pre><code>create table if not exists `catalog`.`schema`.`dq_stats_detailed` (\nrun_id string,  -- (1)!    \nproduct_id string,  -- (2)!  \ntable_name string,  -- (3)!  \nrule_type string,  -- (4)!  \nrule string,  -- (5)!\nsource_expectations string,  -- (6)!\ntag string,  -- (7)!\ndescription string,  -- (8)!\nsource_dq_status string,  -- (9)!\nsource_dq_actual_outcome string,  -- (10)!\nsource_dq_expected_outcome string,  -- (11)!\nsource_dq_actual_row_count string,  -- (12)!\nsource_dq_error_row_count string,  -- (13)!\nsource_dq_row_count string,  -- (14)!\nsource_dq_start_time string,  -- (15)!\nsource_dq_end_time string,  -- (16)!\ntarget_expectations string,  -- (17)!\ntarget_dq_status string,  -- (18)!\ntarget_dq_actual_outcome string,  -- (19)!\ntarget_dq_expected_outcome string,  -- (20)!\ntarget_dq_actual_row_count string,  -- (21)!\ntarget_dq_error_row_count string,  -- (22)!\ntarget_dq_row_count string,  -- (23)!\ntarget_dq_start_time string,  -- (24)!\ntarget_dq_end_time string,  -- (25)!\ndq_date date,  -- (26)!\ndq_time string,  -- (27)!\ndq_job_metadata_info string,  -- (28)!\n);\n</code></pre> <ol> <li><code>run_id</code> Run Id for a specific run </li> <li><code>product_id</code> Unique product identifier </li> <li><code>table_name</code> The target table where the final data gets inserted</li> <li><code>rule_type</code> Either row/query/agg dq</li> <li><code>rule</code>  Rule name</li> <li><code>source_expectations</code> Actual Rule to be executed on the source dq</li> <li><code>tag</code> completeness,uniqueness,validity,accuracy,consistency,</li> <li><code>description</code> Description of the Rule</li> <li><code>source_dq_status</code> Status of the rule execution in the Source dq</li> <li><code>source_dq_actual_outcome</code> Actual outcome of the Source dq check</li> <li><code>source_dq_expected_outcome</code> Expected outcome of the Source dq check</li> <li><code>source_dq_actual_row_count</code> Number of rows of the source dq</li> <li><code>source_dq_error_row_count</code> Number of rows failed in the source dq</li> <li><code>source_dq_row_count</code> Number of rows of the source dq</li> <li><code>source_dq_start_time</code> source dq start timestamp</li> <li><code>source_dq_end_time</code> source dq end timestamp</li> <li><code>target_expectations</code> Actual Rule to be executed on the target dq</li> <li><code>target_dq_status</code> Status of the rule execution in the Target dq</li> <li><code>target_dq_actual_outcome</code> Actual outcome of the Target dq check</li> <li><code>target_dq_expected_outcome</code> Expected outcome of the Target dq check</li> <li><code>target_dq_actual_row_count</code> Number of rows of the target dq</li> <li><code>target_dq_error_row_count</code> Number of rows failed in the target dq</li> <li><code>target_dq_row_count</code> Number of rows of the target dq</li> <li><code>target_dq_start_time</code> target dq start timestamp</li> <li><code>target_dq_end_time</code> target dq end timestamp</li> <li><code>dq_date</code> Dq executed date</li> <li><code>dq_time</code> Dq executed timestamp</li> <li><code>dq_job_metadata_info</code> dq job metadata</li> </ol>"}]}