{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Spark-Expectations","text":"<p>Taking inspiration from DLT - data quality expectations: Spark-Expectations is built, so that the data quality rules can  run using decorator pattern while the spark job is in flight and Additionally, the framework able to perform data  quality checks when the data is at rest.</p>"},{"location":"#features-of-spark-expectations","title":"Features Of Spark Expectations","text":"<p>Please find the spark-expectations flow and feature diagrams below</p> <p> </p> <p> </p>"},{"location":"#rules","title":"Rules","text":"<p>Rules table holds most important building block that defines data quality expectations. Tool currently support three different types of Rules. </p> <p>Proceed to User Guide / Data Quality Rules for details how rules can be configured</p>"},{"location":"#output","title":"Output","text":"<p>Spark-Execution creates multiple tables to store output of the job.  Creation of those tables is managed by the library and certain tables can be toggled or are only created if certain data quality rules exist.</p> <p><code>Stats Table</code> is most important one and captures Data Quality metrics.  Check User Guide / Data Quality Metrics for more information about it but also other tables shown in SE diagram.</p>"},{"location":"#integrations","title":"Integrations","text":""},{"location":"#notifications","title":"Notifications","text":"<p>Spark Expectation provides way to enable different behavior and thresholds for data quality run notifications. Currently supported notifications are:</p> <ul> <li>Slack</li> <li>Email</li> <li>PagerDuty</li> </ul>"},{"location":"Observability_examples/","title":"Spark Expectations Observability Features","text":""},{"location":"Observability_examples/#overview","title":"Overview","text":"<p>This document provides an overview of the observability features available in Spark for data quality (DQ) checks using Delta Lake. Observability in this context refers to the ability to monitor, measure, and understand the state and performance of data quality rules applied to datasets.</p>"},{"location":"Observability_examples/#required-attributes-for-enabling-observability-in-sample-dq-delta","title":"Required Attributes for Enabling Observability in Sample DQ Delta","text":"<p>To enable observability in a sample DQ Delta setup, the following attributes are required:</p> <pre><code>    user_config.se_enable_obs_dq_report_result: True,\nuser_config.se_dq_obs_alert_flag: True,\nuser_config.se_dq_obs_default_email_template: \"\"\n#also user need to pass the smtp details for sending mail.\nuser_config.se_notifications_email_smtp_host: \"smtp.######.com\",\nuser_config.se_notifications_email_smtp_port: 587,\nuser_config.se_notifications_email_from: \"a*****.obs@nike.com\",\nuser_config.se_notifications_email_to_other_mail_id: \"abc@mail.com\"\nuser_config.se_notifications_smtp_password: \"************\"\n</code></pre>"},{"location":"Observability_examples/#currently-our-observability-system-supports-2-flows","title":"Currently our observability system supports 2 flows:","text":""},{"location":"Observability_examples/#flow-1-dq-report-generation","title":"Flow 1: DQ Report Generation","text":"<p>When the DQ report flag is enabled (true), the system generates a report table upon the successful completion of Spark expectations. This flow focuses solely on report generation without triggering any alerts.</p>"},{"location":"Observability_examples/#flow-2-dq-report-generation-with-email-alerts","title":"Flow 2: DQ Report Generation with Email Alerts","text":"<p>When both the DQ report flag and alert flag are enabled (true), the system performs two actions: 1. Generates the report table after Spark expectations are completed. 2. Sends an email alert to the user-provided email address, notifying them of the results.</p>"},{"location":"Observability_examples/#key-highlights","title":"Key Highlights:","text":"<ul> <li>Flow 1: Report generation only (DQ flag = true).</li> <li>Flow 2: Report generation + email alerts (DQ flag = true, Alert flag = true).</li> <li>We don't need to create any report table it will be autogenerated along with the flow.</li> </ul> <p>The report table is derived from the Query DQ Output Table and the Detailed Table.  It is designed to calculate key metrics, numerical summaries, and other analytical insights. To ensure consistency and accuracy in the report table, users must adhere to predefined standards when writing queries for the **Query DQ Output Table. Below is the example of how the rules we can configure- <pre><code>RULES_DATA= \"\"\"\n(\"your_product\", \"dq_spark_dev.customer_order\", \"row_dq\", \"sales_greater_than_zero\", \"sales\", \"sales &gt; 2\", \"ignore\", \"accuracy\", \"sales value should be greater than zero\", false, true, true, false, 0,null, null, \"medium\")    ,(\"your_product\", \"dq_spark_{env}.customer_order\", \"row_dq\", \"discount_threshold\", \"discount\", \"discount*100 &lt; 60\",\"drop\", \"validity\", \"discount should be less than 40\", true, true, true, false, 0,null, null, \"medium\")\n,(\"your_product\", \"dq_spark_{env}.customer_order\", \"row_dq\", \"ship_mode_in_set\", \"ship_mode\", \"lower(trim(ship_mode)) in('second class', 'standard class', 'standard class')\", \"drop\", \"validity\", \"ship_mode mode belongs in the sets\", true, true, true, false, 0,null, null, \"medium\")\n,(\"your_product\", \"dq_spark_{env}.customer_order\", \"row_dq\", \"profit_threshold\", \"profit\", \"profit&gt;0\", \"ignore\", \"validity\", \"profit threshold should be greater tahn 0\", false, true, false, true, 0,null, null, \"medium\")\n,(\"your_product\", \"dq_spark_dev.customer_order\", \"query_dq\", \"product_missing_count_threshold\", \"column_name\", \"((select count(*) from ({source_f1}) a) - (select count(*) from ({target_f1}) b) ) &gt; 3@source_f1@SELECT DISTINCT product_id, order_id, order_date, COUNT(*) AS count FROM order_source GROUP BY product_id, order_id, order_date@target_f1@SELECT DISTINCT product_id, order_id, order_date, COUNT(*) AS count FROM order_target GROUP BY product_id, order_id, order_date\", \"ignore\", \"validity\", \"row count threshold\", true, false, true, false, 0,null, true, \"medium\")\n\"\"\"\n</code></pre></p>"},{"location":"Observability_examples/#template-options-for-report-table-rendering","title":"Template Options for Report Table Rendering","text":"<p>Users have two options for generating the report table:</p> <ol> <li>Custom Template:</li> <li>If the user provides a custom template through the specified attribute, the system will use this template to render the report table.</li> <li> <p><code>user_config.se_dq_obs_default_email_template: \"\"</code></p> </li> <li> <p>Default Jinja Template:</p> </li> <li>If no custom template is provided by the user, the system will automatically fall back to the default jinja template (spark_expectations/config/templates/advanced_email_alert_template.jinja) for rendering the report table.</li> </ol>"},{"location":"Observability_examples/#sample-for-the-alert-received-in-the-mail","title":"Sample for the alert received in the mail","text":""},{"location":"bigquery/","title":"BigQuery","text":""},{"location":"bigquery/#example-write-to-delta","title":"Example - Write to Delta","text":"<p>Setup SparkSession for BigQuery to test in your local environment. Configure accordingly for higher environments. Refer to Examples in base_setup.py and bigquery.py</p> spark_session<pre><code>from pyspark.sql import SparkSession\nbuilder = (\nSparkSession.builder.config(\n\"spark.jars.packages\",\n\"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.30.0\",\n)\n)\nspark = builder.getOrCreate()\nspark._jsc.hadoopConfiguration().set(\n\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\"\n)\nspark.conf.set(\"viewsEnabled\", \"true\")\nspark.conf.set(\"materializationDataset\", \"&lt;temp_dataset&gt;\")\n</code></pre> <p>Below is the configuration that can be used to run SparkExpectations and write to Delta Lake</p> iceberg_write<pre><code>import os\nfrom pyspark.sql import DataFrame\nfrom spark_expectations.core.expectations import (\nSparkExpectations,\nWrappedDataFrameWriter,\n)\nfrom spark_expectations.config.user_config import Constants as user_config\nos.environ[\n\"GOOGLE_APPLICATION_CREDENTIALS\"\n] = \"path_to_your_json_credential_file\"  # This is needed for spark write to bigquery\nwriter = (\nWrappedDataFrameWriter().mode(\"overwrite\")\n.format(\"bigquery\")\n.option(\"createDisposition\", \"CREATE_IF_NEEDED\")\n.option(\"writeMethod\", \"direct\")\n)\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your_product\",\nrules_df=spark.read.format(\"bigquery\").load(\n\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;rules_table&gt;\"\n),\nstats_table=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;stats_table&gt;\",\nstats_table_writer=writer,\ntarget_and_error_table_writer=writer,\ndebugger=False,\nstats_streaming_options={user_config.se_enable_streaming: False}\n)\n#if smtp server needs to be authenticated, password can be passed directly with user config or set in a secure way like cerberus or databricks secret\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\",\nuser_config.cbs_url: \"https://cerberus.example.com\",\nuser_config.cbs_sdb_path: \"\",\nuser_config.cbs_smtp_password: \"\",\n# user_config.secret_type: \"databricks\",\n# user_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\",\n# user_config.dbx_secret_scope: \"your_secret_scope\",\n# user_config.dbx_smtp_password: \"your_password\",\n}\n# Commented fields are optional or required when notifications are enabled\nuser_conf = {\nuser_config.se_notifications_enable_email: False,\n# user_config.se_notifications_enable_smtp_server_auth: False,\n# user_config.se_notifications_enable_custom_email_body: True,\n# user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n# user_config.se_notifications_email_smtp_port: 25,\n# user_config.se_notifications_smtp_password: \"your_password\",\n# user_config.se_notifications_smtp_creds_dict: smtp_creds_dict,\n# user_config.se_notifications_email_from: \"\",\n# user_config.se_notifications_email_to_other_mail_id: \"\",\n# user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n# user_config.se_notifications_email_custom_body: \"Custom statistics: 'product_id': {}\",\nuser_config.se_notifications_enable_slack: False,\n# user_config.se_notifications_slack_webhook_url: \"\",\n# user_config.se_notifications_on_start: True,\n# user_config.se_notifications_on_completion: True,\n# user_config.se_notifications_on_fail: True,\n# user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n# user_config.se_notifications_on_error_drop_threshold: 15,\n# user_config.se_enable_error_table: True,\n# user_config.enable_query_dq_detailed_result: True,\n# user_config.enable_agg_dq_detailed_result: True,\n# user_config.se_dq_rules_params: { \"env\": \"local\", \"table\": \"product\", },\n}\n@se.with_expectations(\ntarget_table=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_name&gt;\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_view_name&gt;\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\nreturn _df_order\n</code></pre>"},{"location":"delta/","title":"Delta","text":""},{"location":"delta/#example-write-to-delta","title":"Example - Write to Delta","text":"<p>Setup SparkSession for Delta Lake to test in your local environment. Configure accordingly for higher environments. Refer to Examples in base_setup.py and  delta.py</p> spark_session<pre><code>from pyspark.sql import SparkSession\nbuilder = (\nSparkSession.builder.config(\n\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"\n)\n.config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\")\n.config(\n\"spark.sql.catalog.spark_catalog\",\n\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n)\n.config(\"spark.sql.warehouse.dir\", \"/tmp/hive/warehouse\")\n.config(\"spark.driver.extraJavaOptions\", \"-Dderby.system.home=/tmp/derby\")\n.config(\"spark.jars.ivy\", \"/tmp/ivy2\")\n)\nspark = builder.getOrCreate()\n</code></pre> <p>Below is the configuration that can be used to run SparkExpectations and write to Delta Lake</p> delta_write<pre><code>import os\nfrom pyspark.sql import DataFrame\nfrom spark_expectations.core.expectations import (\nSparkExpectations,\nWrappedDataFrameWriter,\n)\nfrom spark_expectations.config.user_config import Constants as user_config\nwriter = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your_product\",\nrules_df=spark.table(\"dq_spark_local.dq_rules\"),\nstats_table=\"dq_spark_local.dq_stats\",\nstats_table_writer=writer,\ntarget_and_error_table_writer=writer,\ndebugger=False,\nstats_streaming_options={user_config.se_enable_streaming: False}\n)\n#if smtp server needs to be authenticated, password can be passed directly with user config or set in a secure way like cerberus or databricks secret\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\",\nuser_config.cbs_url: \"https://cerberus.example.com\",\nuser_config.cbs_sdb_path: \"\",\nuser_config.cbs_smtp_password: \"\",\n# user_config.secret_type: \"databricks\",\n# user_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\",\n# user_config.dbx_secret_scope: \"your_secret_scope\",\n# user_config.dbx_smtp_password: \"your_password\",\n}\n# Commented fields are optional or required when notifications are enabled\nuser_conf = {\nuser_config.se_notifications_enable_email: False,\n# user_config.se_notifications_enable_smtp_server_auth: False,\n# user_config.se_notifications_enable_custom_email_body: True,\n# user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n# user_config.se_notifications_email_smtp_port: 25,\n# user_config.se_notifications_smtp_password: \"your_password\",\n# user_config.se_notifications_smtp_creds_dict: smtp_creds_dict,\n# user_config.se_notifications_email_from: \"\",\n# user_config.se_notifications_email_to_other_mail_id: \"\",\n# user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n# user_config.se_notifications_email_custom_body: \"Custom statistics: 'product_id': {}\",\nuser_config.se_notifications_enable_slack: False,\n# user_config.se_notifications_slack_webhook_url: \"\",\n# user_config.se_notifications_on_start: True,\n# user_config.se_notifications_on_completion: True,\n# user_config.se_notifications_on_fail: True,\n# user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n# user_config.se_notifications_on_error_drop_threshold: 15,\n# user_config.se_enable_error_table: True,\n# user_config.enable_query_dq_detailed_result: True,\n# user_config.enable_agg_dq_detailed_result: True,\n# user_config.se_dq_rules_params: { \"env\": \"local\", \"table\": \"product\", },\n}\n@se.with_expectations(\ntarget_table=\"dq_spark_local.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\nreturn _df_order\n</code></pre>"},{"location":"email_notifications/","title":"Spark Expectations Email Notifications/Alerts","text":"<p>Spark Expectations can send three kinds of emails.</p>"},{"location":"email_notifications/#1-data-quality-report-emails","title":"1. Data Quality Report Emails","text":"<p>Please see the Observability Examples doc for more detailed information on emails that contain the DQ report.</p>"},{"location":"email_notifications/#2-basic-email-notificationsalerts","title":"2. Basic Email Notifications/Alerts","text":"<p>In addition to the email alerts described above for the report table, Spark Expectations can also send basic email alerts on a job's start, completion, failure, and/or other conditions depending on user configuration. These alerts have a structure like:</p> <pre><code>Spark expectations job has started\ntable_name: &lt;Name of the Table&gt;\nrun_id: &lt;Run ID&gt;\nrun_date: &lt;Run Date&gt;\n</code></pre> <p>Basic notifications only report the table name, run ID, and run date and this is not configurable at this time. For emails where you can choose specific metrics see the custom metrics email section below.</p> <p>The configurations related to basic email notifications are:</p> <pre><code>    user_config.se_notifications_enable_email: True,\nuser_config.se_notifications_on_start: True,\nuser_config.se_notifications_on_completion: True,\nuser_config.se_notifications_on_fail: True,\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\nuser_config.se_notifications_on_rules_action_if_failed_set_ignore: True,\nuser_config.se_notifications_on_error_drop_threshold: 15,\n# smtp details for sending emails\nuser_config.se_notifications_email_smtp_host: \"smtp.######.com\",\nuser_config.se_notifications_email_smtp_port: 587,\nuser_config.se_notifications_smtp_password: \"************\",\nuser_config.se_notifications_email_from: \"sender@mail.com\",\nuser_config.se_notifications_email_to_other_mail_id: \"receiver@mail.com\"\nuser_config.se_notifications_email_subject: \"Spark Expectations - Notification\"\n</code></pre>"},{"location":"email_notifications/#html-template-options-for-basic-emails","title":"HTML Template Options for Basic Emails","text":"<p>Spark Expectations supports the use of Jinja templates to apply HTML to the basic email notifications. </p> <p>Please note that the template for basic email notifications is different and separate from the template for the DQ report emails.</p> <p>To enable Jinja templates for basic email alerts set these two optional parameters: <pre><code>    user_config.se_notifications_enable_templated_basic_email_body: True\nuser_config.se_notifications_default_basic_email_template: \"\"\n</code></pre></p> <p>If a template is not provided the default template located in <code>spark_expectations/config/templates/basic_email_alert_template.jinja</code> will be used.</p> <p>This feature is somewhat limited currently and the template should be set up to handle <code>rows</code>.</p>"},{"location":"email_notifications/#example-template-config-setup","title":"Example Template Config Setup","text":"<pre><code>basic_html_template = \"\"\"\n&lt;style&gt;\n    table {\n        border-collapse: collapse;\n        width: 60%; /* Reduced width from 100% to 80% */\n        font-family: Arial, sans-serif;\n        border: 2px solid black; /* Added black border for the table */\n    }\n    td {\n        border: 1px solid black; /* Changed to black border */\n        text-align: left;\n        padding: 4px;\n    }\n    tr:nth-child(even) {\n        background-color: #f9f9f9;\n    }\n&lt;/style&gt;\n{% macro render_table(rows) %}\n&lt;table border=1&gt;\n    &lt;tbody&gt;\n        {% for row in rows %}\n            &lt;tr&gt;\n                {% for cell in row %}\n                        &lt;td&gt;{{ cell }}&lt;/td&gt;\n                {% endfor %}\n            &lt;/tr&gt;\n        {% endfor %}\n    &lt;/tbody&gt;\n&lt;/table&gt;\n{% endmacro %}\n&lt;h3&gt;{{ title }}&lt;/h3&gt;\n{{ render_table(rows) }}\n\"\"\"\nuser_config.se_notifications_enable_templated_basic_email_body: True\nuser_config.se_notifications_default_basic_email_template: basic_html_template\n</code></pre>"},{"location":"email_notifications/#3-custom-metrics-emails","title":"3. Custom Metrics Emails","text":"<p>The following two attributes in the user configuration should be set to enable custom metrics emails (plain text by default): <pre><code>user_config.se_notifications_enable_custom_email_body: True,\nuser_config.se_notifications_email_custom_body: \"custom stats: 'product_id': {}\"\n</code></pre> The <code>se_notifications_email_custom_body</code> field needs to comply with a specific syntax, matching the style in the example below. The metrics that can be requested are the names of the columns in the dq_stats table.</p>"},{"location":"email_notifications/#example-custom-metrics-config","title":"Example Custom Metrics Config","text":"<p>The following is a comprehensive configuration for the custom email body, showing how to request more metrics. <pre><code>user_config.se_notifications_email_custom_body: (\n\"'product_id': {},\\n \"\n\"'table_name': {},\\n \"\n\"'input_count': {},\\n \"\n\"'error_count': {},\\n \"\n\"'output_count': {},\\n \"\n\"'output_percentage': {},\\n \"\n\"'success_percentage': {},\\n \"\n\"'error_percentage': {},\\n \"\n\"'source_agg_dq_results': {},\\n \"\n\"'source_query_dq_results': {},\\n \"\n\"'final_agg_dq_results': {},\\n \"\n\"'final_query_dq_results': {},\\n \"\n\"'row_dq_res_summary': {},\\n \"\n\"'row_dq_error_threshold': {},\\n \"\n\"'dq_status': {},\\n \"\n\"'dq_run_time': {},\\n \"\n\"'dq_rules': {},\\n \"\n\"'meta_dq_run_id': {},\\n \"\n\"'meta_dq_run_date': {},\\n \"\n\"'meta_dq_run_datetime': {},\\n \"\n\"'dq_env': {}\\n\"\n)\n</code></pre> By default the custom emails are in plain text, but there is an option to format them using an HTML template.</p>"},{"location":"email_notifications/#using-custom-jinja2-html-templates","title":"Using Custom Jinja2 HTML Templates","text":"<p>Set <code>user_config.se_notifications_enable_templated_custom_email</code> and <code>user_config.se_notifications_email_custom_template</code> to enable using a Jinja2 HTML template to format the custom metrics email. The example below includes an example template definition in addition to the config.</p> <p><pre><code>custom_html_template = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;DQ Results Summary&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            color: #333;\n            margin: 20px;\n        }\n        dl {\n            border: 1px solid #ccc;\n            padding: 12px;\n            width: 400px;\n            background-color: #f9f9f9;\n        }\n        dt {\n            font-weight: bold;\n            margin-top: 8px;\n        }\n        dd {\n            margin: 0 0 8px 16px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;dl&gt;\n    &lt;dt&gt;Product&lt;/dt&gt;\n    &lt;dd&gt;{{ product_id }}&lt;/dd&gt;\n    &lt;dt&gt;Table&lt;/dt&gt;\n    &lt;dd&gt;{{ table_name }}&lt;/dd&gt;\n    &lt;dt&gt;Source AGG DQ Check Name [0]&lt;/dt&gt;\n    &lt;dd&gt;{{ source_agg_dq_results[0].rule }}&lt;/dd&gt;\n    &lt;dt&gt;Source AGG DQ Check Description [0]&lt;/dt&gt;\n    &lt;dd&gt;{{ source_agg_dq_results[0].description }}&lt;/dd&gt;\n    &lt;dt&gt;Source AGG DQ Check Result [0]&lt;/dt&gt;\n    &lt;dd&gt;{{ source_agg_dq_results[0].status }}&lt;/dd&gt;\n    &lt;dt&gt;Source AGG DQ Check Name [1]&lt;/dt&gt;\n    &lt;dd&gt;{{ source_agg_dq_results[1].rule }}&lt;/dd&gt;\n    &lt;dt&gt;Source AGG DQ Check Description [1]&lt;/dt&gt;\n    &lt;dd&gt;{{ source_agg_dq_results[1].description }}&lt;/dd&gt;\n    &lt;dt&gt;Source AGG DQ Check Result [1]&lt;/dt&gt;\n    &lt;dd&gt;{{ source_agg_dq_results[1].status }}&lt;/dd&gt;\n    &lt;dt&gt;Source Query DQ Check Name&lt;/dt&gt;\n    {% for item in source_query_dq_results %}\n        &lt;dd&gt;{{ item.rule }}&lt;/dd&gt;\n    {% endfor %}\n    &lt;dt&gt;Source Query DQ Check Description&lt;/dt&gt;\n    {% for item in source_query_dq_results %}\n        &lt;dd&gt;{{ item.description }}&lt;/dd&gt;\n    {% endfor %}\n    &lt;dt&gt;Source Query DQ Check Result&lt;/dt&gt;\n    {% for item in source_query_dq_results %}\n        &lt;dd&gt;{{ item.status }}&lt;/dd&gt;\n    {% endfor %}\n    &lt;dt&gt;Rule Execution Timestamp&lt;/dt&gt;\n    &lt;dd&gt;{{ meta_dq_run_datetime }}&lt;/dd&gt;\n    &lt;dt&gt;Final AGG DQ Results&lt;/dt&gt;\n    &lt;dd&gt;{{ final_agg_dq_results }}&lt;/dd&gt;\n&lt;/dl&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\nuser_config.se_notifications_enable_templated_custom_email: True\nuser_config.se_notifications_email_custom_template: \"\"\n</code></pre> As seen in the template, some of the results values are nested data structures (often an array of maps/dicts) and their elements can be accessed directly with the right syntax, or using <code>for</code> loops or other options that Jinja2 supports.</p> <p>Note: - If an empty string <code>\"\"</code> is passed in to the template field in the user config, the default template will be used: (spark_expectations/config/templates/custom_email_alert_template.jinja). - A metric name must be specified in <code>user_config.se_notifications_email_custom_body</code> if it is being referenced in the template. For example, if <code>{{ final_agg_dq_results }}</code> is referenced in the template, then <code>\"'final_agg_dq_results': {},\\n \"</code> must be in the user config.</p>"},{"location":"examples/","title":"Understand Args","text":""},{"location":"examples/#configurations","title":"Configurations","text":"<p>In order to establish the global configuration parameter for DQ Spark Expectations, you must define and complete the required fields within a variable. This involves creating a variable and ensuring that all the necessary information is provided in the appropriate fields.</p> <pre><code>from spark_expectations.config.user_config import Constants as user_config\nse_user_conf = {\nuser_config.se_notifications_enable_email: False,  # (1)!\nuser_config.se_notifications_enable_smtp_server_auth: False, # (2)!\nuser_config.se_notifications_enable_custom_email_body: False, # (3)\nuser_config.se_notifications_email_smtp_host: \"mailhost.com\",  # (4)!\nuser_config.se_notifications_email_smtp_port: 25,  # (5)!\nuser_config.se_notifications_smtp_password: \"your_password\",# (6)!\n# user_config.se_notifications_smtp_creds_dict: {\n#     user_config.secret_type: \"cerberus\",\n#     user_config.cbs_url: \"https://cerberus.example.com\",\n#     user_config.cbs_sdb_path: \"your_sdb_path\",\n#     user_config.cbs_smtp_password: \"your_smtp_password\",\n# }, # (7)!\nuser_config.se_notifications_email_from: \"&lt;sender_email_id&gt;\",  # (8)!\nuser_config.se_notifications_email_to_other_mail_id: \"&lt;receiver_email_id's&gt;\",  # (9)!\nuser_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",  # (10)!\nuser_config.se_notifications_email_custom_body: \"custom stats: 'product_id': {}\", # (11)!\nuser_config.se_notifications_enable_slack: True,  # (12)!\nuser_config.se_notifications_slack_webhook_url: \"&lt;slack-webhook-url&gt;\",  # (13)!\nuser_config.se_notifications_on_start: True,  # (14)!\nuser_config.se_notifications_on_completion: True,  # (15)!\nuser_config.se_notifications_on_fail: True,  # (16)!\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,  # (17)!\nuser_config.se_notifications_on_rules_action_if_failed_set_ignore: True,  # (18)! \nuser_config.se_notifications_on_error_drop_threshold: 15,  # (19)!\nuser_config.se_enable_error_table: True,  # (20)!\nuser_config.enable_query_dq_detailed_result: True, # (21)!\nuser_config.enable_agg_dq_detailed_result: True, # (22)!\nuser_config.querydq_output_custom_table_name: \"&lt;catalog.schema.table-name&gt;\", #23\nuser_config.se_dq_rules_params: {\n\"env\": \"local\",\n\"table\": \"product\",\n}, # (24)!\nuser_config.se_notifications_enable_templated_basic_email_body: True, # (25)!\nuser_config.se_notifications_default_basic_email_template: \"\", # (26)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_notifications_enable_email</code> parameter, which controls whether notifications are sent via email, is set to false by default</li> <li>The <code>user_config.se_notifications_enable_smtp_server_auth</code> optional parameter, which controls whether SMTP server authentication is enabled, is set to false by default</li> <li>The <code>user_config.se_notifications_enable_custom_email_body</code> optional parameter, which controls whether custom email body is enabled, is set to false by default</li> <li>The <code>user_config.se_notifications_email_smtp_host</code> parameter is set to \"mailhost.com\" by default and is used to specify the email SMTP domain host</li> <li>The <code>user_config.se_notifications_email_smtp_port</code> parameter, which accepts a port number, is set to \"25\" by default</li> <li>The <code>user_config.se_notifications_smtp_password</code> parameter is used to specify the password for the SMTP server (if smtp_server requires authentication either this parameter or <code>user_config.se_notifications_smtp_creds_dict</code> should be set)</li> <li>The <code>user_config.se_notifications_smtp_creds_dict</code> parameter is used to specify the credentials for the SMTP server (if smtp_server requires authentication either this parameter or <code>user_config.se_notifications_smtp_password</code> should be set)</li> <li>The <code>user_config.se_notifications_email_from</code> parameter is used to specify the email ID that will trigger the email notification</li> <li>The <code>user_config.se_notifications_email_to_other_mail_id</code> parameter accepts a list of recipient email IDs</li> <li>The <code>user_config.se_notifications_email_subject</code> parameter captures the subject line of the email</li> <li>The <code>user_config.se_notifications_email_custom_body</code> optional parameter, captures the custom email body, need to be compliant with certain syntax</li> <li>The <code>user_config.se_notifications_enable_slack</code> parameter, which controls whether notifications are sent via slack, is set to false by default </li> <li>The <code>user_config/se_notifications_slack_webhook_url</code> parameter accepts the webhook URL of a Slack channel for sending notifications </li> <li>When <code>user_config.se_notifications_on_start</code> parameter set to <code>True</code> enables notification on start of the spark-expectations, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_completion</code> parameter set to <code>True</code> enables notification on completion of spark-expectations framework, variable by default set to <code>False</code></li> <li>When <code>user_config.se_notifications_on_fail</code> parameter set to <code>True</code> enables notification on failure of spark-expectations data quality framework, variable by default set to <code>True</code></li> <li>When <code>user_config.se_notifications_on_error_drop_exceeds_threshold_breach</code> parameter set to <code>True</code> enables notification when error threshold reaches above the configured value </li> <li>When <code>user_config.se_notifications_on_rules_action_if_failed_set_ignore</code> parameter set to <code>True</code> enables notification when rules action is set to ignore if failed </li> <li>The <code>user_config.se_notifications_on_error_drop_threshold</code> parameter captures error drop threshold value </li> <li>The <code>user_config.se_enable_error_table</code> parameter, which controls whether error data to load into error table, is set to true by default </li> <li>When <code>user_config.enable_query_dq_detailed_result</code> parameter set to <code>True</code>, enables the option to capture the query_dq detailed stats to detailed_stats table. By default set to <code>False</code></li> <li>When <code>user_config.enable_agg_dq_detailed_result</code> parameter set to <code>True</code>, enables the option to capture the agg_dq detailed stats to detailed_stats table. By default set to <code>False</code></li> <li>The <code>user_config.querydq_output_custom_table_name</code> parameter is used to specify the name of the custom query_dq output table which captures the output of the alias queries passed in the query dq expectation. Default is _custom_output  <li>The <code>user_config.se_dq_rules_params</code> parameter, which are required to dynamically update dq rules</li> <li>The <code>user_config.se_notifications_enable_templated_basic_email_body</code> optional parameter is used to enable using a Jinja template for basic email notifications (notifying on job start, completion, failure, etc.)</li> <li>The <code>user_config.se_notifications_default_basic_email_template</code> optional parameter is used to specify the Jinja template used for basic email notifications. If the provided template is blank or this option is missing (while basic email templates are enabled) a default template will be used.</li> <p>In case of SMTP server authentication, the password can be passed directly with the user config or set in a secure way like Cerberus or Databricks secret. If it is preferred to use Cerberus for secure password storage, the <code>user_config.se_notifications_smtp_creds_dict</code> parameter can be used to specify the credentials for the SMTP server in the following way: <pre><code>from spark_expectations.config.user_config import Constants as user_config\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\", # (1)!\nuser_config.cbs_url: \"https://.example.com\", # (2)!\nuser_config.cbs_sdb_path: \"your_sdb_path\", # (3)!\nuser_config.cbs_smtp_password: \"your_smtp_password\", # (4)!\n}\n</code></pre> 1. The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) 2. The <code>user_config.cbs_url</code> used to pass Cerberus URL  3. The <code>user_config.cbs_sdb_path</code> captures Cerberus secure data store path  4. The <code>user_config.cbs_smtp_password</code> captures key for smtp_password in the Cerberus sdb</p> <p>Similarly, if it is preferred to use Databricks for secure password storage, the <code>user_config.se_notifications_smtp_creds_dict</code> parameter can be used to specify the credentials for the SMTP server in the following way: <pre><code>from spark_expectations.config.user_config import Constants as user_config\nsmtp_creds_dict = {\nuser_config.secret_type: \"databricks\", # (1)!\nuser_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\", # (2)!\nuser_config.dbx_secret_scope: \"your_secret_scope\", # (3)!\nuser_config.dbx_smtp_password: \"your_password\", # (4)!\n}\n</code></pre> 1. The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) 2. The <code>user_config.dbx_workspace_url</code> used to pass Databricks workspace in the format <code>https://&lt;workspace_name&gt;.cloud.databricks.com</code> 3. The <code>user_config.dbx_secret_scope</code> captures name of the secret scope 4. The <code>user_config.dbx_smtp_password</code> captures secret key for smtp password in the Databricks secret scope</p> <pre><code>### Spark Expectations Initialization \nFor all the below examples the below import and SparkExpectations class instantiation is mandatory\nWhen store for sensitive details is Databricks secret scope,construct config dictionary for authentication of Kafka and \navoid duplicate construction every time your project is initialized, you can create a dictionary with the following keys and their appropriate values. \nThis dictionary can be placed in the __init__.py file of your project or declared as a global variable.\n```python\nfrom typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.dbx_workspace_url  : \"https://workspace.cloud.databricks.com\", # (3)!\nuser_config.dbx_secret_scope: \"sole_common_prod\", # (4)!\nuser_config.dbx_kafka_server_url: \"se_streaming_server_url_secret_key\", # (5)!\nuser_config.dbx_secret_token_url: \"se_streaming_auth_secret_token_url_key\", # (6)!\nuser_config.dbx_secret_app_name: \"se_streaming_auth_secret_appid_key\", # (7)!\nuser_config.dbx_secret_token: \"se_streaming_auth_secret_token_key\", # (8)!\nuser_config.dbx_topic_name: \"se_streaming_topic_name\", # (9)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.dbx_workspace_url</code> used to pass Databricks workspace in the format <code>https://&lt;workspace_name&gt;.cloud.databricks.com</code></li> <li>The <code>user_config.dbx_secret_scope</code> captures name of the secret scope</li> <li>The <code>user_config.dbx_kafka_server_url</code> captures secret key for the Kafka URL</li> <li>The <code>user_config.dbx_secret_token_url</code> captures secret key for the Kafka authentication app URL</li> <li>The <code>user_config.dbx_secret_app_name</code> captures secret key for the Kafka authentication app name</li> <li>The <code>user_config.dbx_secret_token</code> captures secret key for the Kafka authentication app secret token</li> <li>The <code>user_config.dbx_topic_name</code> captures secret key for the Kafka topic name</li> </ol> <p>Similarly when sensitive store is Cerberus: </p> <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: True, # (1)!\nuser_config.secret_type: \"databricks\", # (2)!\nuser_config.cbs_url  : \"https://&lt;url&gt;.cerberus.com\", # (3)!\nuser_config.cbs_sdb_path: \"cerberus_sdb_path\", # (4)!\nuser_config.cbs_kafka_server_url: \"se_streaming_server_url_secret_sdb_path\", # (5)!\nuser_config.cbs_secret_token_url: \"se_streaming_auth_secret_token_url_sdb_path\", # (6)!\nuser_config.cbs_secret_app_name: \"se_streaming_auth_secret_appid_sdb_path\", # (7)!\nuser_config.cbs_secret_token: \"se_streaming_auth_secret_token_sdb_path\", # (8)!\nuser_config.cbs_topic_name: \"se_streaming_topic_name_sdb_path\", # (9)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> <li>The <code>user_config.secret_type</code> used to define type of secret store and takes two values (<code>databricks</code>, <code>cerberus</code>) by default will be <code>databricks</code></li> <li>The <code>user_config.cbs_url</code> used to pass Cerberus URL</li> <li>The <code>user_config.cbs_sdb_path</code> captures Cerberus secure data store path</li> <li>The <code>user_config.cbs_kafka_server_url</code> captures path where Kafka URL stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_secret_token_url</code> captures path where Kafka authentication app stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_secret_app_name</code> captures path where Kafka authentication app name stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_secret_token</code> captures path where Kafka authentication app name secret token stored in the Cerberus sdb</li> <li>The <code>user_config.cbs_topic_name</code>  captures path where Kafka topic name stored in the Cerberus sdb</li> </ol> <p>You can disable the streaming functionality by setting the <code>user_config.se_enable_streaming</code> parameter to <code>False</code> </p> <pre><code>from typing import Dict, Union\nfrom spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict: Dict[str, Union[bool, str]] = {\nuser_config.se_enable_streaming: False, # (1)!\n}\n</code></pre> <ol> <li>The <code>user_config.se_enable_streaming</code> parameter is used to control the enabling or disabling of Spark Expectations (SE) streaming functionality. When enabled, SE streaming stores the statistics of every batch run into Kafka.</li> </ol> <pre><code>from spark_expectations.core.expectations import SparkExpectations\n# product_id should match with the \"product_id\" in the rules table\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your-products-id\", \nstats_streaming_options=stats_streaming_config_dict)  # (1)!\n</code></pre> <ol> <li>Instantiate <code>SparkExpectations</code> class which has all the required functions for running data quality rules</li> </ol>"},{"location":"examples/#example-1","title":"Example 1","text":"<pre><code>from spark_expectations.core.expectations import SparkExpectations, WrappedDataFrameWriter\nwriter = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")  # (1)!\nse = SparkExpectations(  # (10)!\nproduct_id=\"your_product\",  # (11)!\nrules_df=spark.table(\"dq_spark_local.dq_rules\"),  # (12)!\nstats_table=\"dq_spark_local.dq_stats\",  # (13)!\nstats_table_writer=writer,  # (14)!\ntarget_and_error_table_writer=writer,  # (15)!\ndebugger=False,  # (16)!\n# stats_streaming_options={user_config.se_enable_streaming: False},  # (17)!\n)\n@se.with_expectations(  # (2)!\nwrite_to_table=True,  # (3)!\nwrite_to_temp_table=True,  # (4)!\nuser_conf=se_user_conf,  # (5)!\ntarget_table_view=\"order\",  # (6)!\ntarget_and_error_table_writer=writer,  # (7)!\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")  # (8)!\nreturn _df_order  # (9)!\n</code></pre> <ol> <li>The <code>WrappedDataFrameWriter</code> class is used to wrap the <code>DataFrameWriter</code> class and add additional functionality to it</li> <li>The <code>@se.with_expectations</code> decorator is used to run the data quality rules</li> <li>The <code>write_to_table</code> parameter is used to write the final data into the table. By default, it is False. This is optional, if you just want to run the data quality checks. A good example will be a staging table or temporary view.</li> <li>The <code>write_to_temp_table</code> parameter is used to write the input dataframe into the temp table, so that it breaks the spark plan and might speed up the job in cases of complex dataframe lineage</li> <li>The <code>user_conf</code> parameter is utilized to gather all the configurations that are associated with notifications. There are four types of notifications: notification_on_start, notification_on_completion, notification_on_fail and notification_on_error_threshold_breach.    Enable notifications for all four stages by setting the values to <code>True</code>. By default, all four stages are set to <code>False</code></li> <li>The <code>target_table_view</code> parameter is used to provide the name of a view that represents the target validated dataset for implementation of <code>query_dq</code> on the clean dataset from <code>row_dq</code></li> <li>The <code>target_and_error_table_writer</code> parameter is used to write the final data into the table. By default, it is False. This is optional, if you just want to run the data quality checks. A good example will be a staging table or temporary view.</li> <li>View registration can be utilized when implementing <code>query_dq</code> expectations.</li> <li>Returning a dataframe is mandatory for the <code>spark_expectations</code> to work, if we do not return a dataframe - then an exception will be raised</li> <li>Instantiate <code>SparkExpectations</code> class which has all the required functions for running data quality rules</li> <li>The <code>product_id</code> parameter is used to specify the product ID of the data quality rules. This has to be a unique value</li> <li>The <code>rules_df</code> parameter is used to specify the dataframe that contains the data quality rules</li> <li>The <code>stats_table</code> parameter is used to specify the table name where the statistics will be written into</li> <li>The <code>stats_table_writer</code> takes in the configuration that need to be used to write the stats table using pyspark</li> <li>The <code>target_and_error_table_writer</code> takes in the configuration that need to be used to write the target and error table using pyspark</li> <li>The <code>debugger</code> parameter is used to enable the debugger mode</li> <li>The <code>stats_streaming_options</code> parameter is used to specify the configurations for streaming statistics into Kafka. To not use Kafka, uncomment this.</li> </ol>"},{"location":"github_copilot_chat_commands/","title":"GitHub Copilot Chat Commands: Detailed Guide","text":"<p>GitHub Copilot Chat provides a conversational interface in Visual Studio Code, enabling you to interact with Copilot using natural language and special commands. Below is a detailed guide to the most useful Copilot Chat commands, their usage, and examples.</p>"},{"location":"github_copilot_chat_commands/#1-explain","title":"1. /explain","text":"<p>Purpose: Explains the selected code, function, or file.</p> <p>Usage: - Select code or place your cursor in a function, then type <code>/explain</code> in the Copilot Chat panel.</p> <p>Example: <pre><code>/explain\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#2-fix","title":"2. /fix","text":"<p>Purpose: Suggests a fix for the selected code or error message.</p> <p>Usage: - Select the problematic code or error, then type <code>/fix</code>.</p> <p>Example: <pre><code>/fix\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#3-tests","title":"3. /tests","text":"<p>Purpose: Generates unit tests for the selected function, class, or file.</p> <p>Usage: - Select the code you want tests for, then type <code>/tests</code>.</p> <p>Example: <pre><code>/tests\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#4-doc","title":"4. /doc","text":"<p>Purpose: Generates documentation for the selected code.</p> <p>Usage: - Select a function, class, or code block, then type <code>/doc</code>.</p> <p>Example: <pre><code>/doc\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#5-generate","title":"5. /generate","text":"<p>Purpose: Generates code based on your prompt or requirements.</p> <p>Usage: - Type <code>/generate</code> followed by your prompt.</p> <p>Example: <pre><code>/generate Create a Python function to calculate factorial\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#6-help","title":"6. /help","text":"<p>Purpose: Lists all available Copilot Chat commands and their descriptions.</p> <p>Usage: <pre><code>/help\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#7-vscode","title":"7. /vscode","text":"<p>Purpose: Interacts with VS Code features, such as running commands or opening files.</p> <p>Usage: - Type <code>/vscode</code> followed by your request.</p> <p>Example: <pre><code>/vscode Open README.md\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#8-feedback","title":"8. /feedback","text":"<p>Purpose: Send feedback about Copilot Chat to GitHub.</p> <p>Usage: <pre><code>/feedback Copilot Chat helped me refactor my code efficiently.\n</code></pre></p>"},{"location":"github_copilot_chat_commands/#tips-for-using-copilot-chat-commands","title":"Tips for Using Copilot Chat Commands","text":"<ul> <li>You can use these commands in the Copilot Chat panel or in code comments (for some commands).</li> <li>Combine commands with natural language for more context-aware results.</li> <li>Use <code>/help</code> to discover new or updated commands.</li> </ul> <p>For more information, visit the GitHub Copilot documentation.</p>"},{"location":"iceberg/","title":"Iceberg","text":""},{"location":"iceberg/#example-write-to-delta","title":"Example - Write to Delta","text":"<p>Setup SparkSession for iceberg to test in your local environment. Configure accordingly for higher environments. Refer to Examples in base_setup.py and iceberg.py</p> spark_session<pre><code>from pyspark.sql import SparkSession\nbuilder = (\nSparkSession.builder.config(\n\"spark.jars.packages\",\n\"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1\",\n)\n.config(\n\"spark.sql.extensions\",\n\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n)\n.config(\n\"spark.sql.catalog.spark_catalog\",\n\"org.apache.iceberg.spark.SparkSessionCatalog\",\n)\n.config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n.config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/tmp/hive/warehouse\")\n.config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n.config(\"spark.sql.catalog.local.type\", \"hadoop\")\n.config(\"spark.sql.catalog.local.warehouse\", \"/tmp/hive/warehouse\")\n)\nspark = builder.getOrCreate()\n</code></pre> <p>Below is the configuration that can be used to run SparkExpectations and write to Delta Lake</p> iceberg_write<pre><code>import os\nfrom pyspark.sql import DataFrame\nfrom spark_expectations.core.expectations import (\nSparkExpectations,\nWrappedDataFrameWriter,\n)\nfrom spark_expectations.config.user_config import Constants as user_config\nwriter = WrappedDataFrameWriter().mode(\"append\").format(\"iceberg\")\nse: SparkExpectations = SparkExpectations(\nproduct_id=\"your_product\",\nrules_df=spark.sql(\"select * from dq_spark_local.dq_rules\"),\nstats_table=\"dq_spark_local.dq_stats\",\nstats_table_writer=writer,\ntarget_and_error_table_writer=writer,\ndebugger=False,\nstats_streaming_options={user_config.se_enable_streaming: False},\n)\n#if smtp server needs to be authenticated, password can be passed directly with user config or set in a secure way like cerberus or databricks secret\nsmtp_creds_dict = {\nuser_config.secret_type: \"cerberus\",\nuser_config.cbs_url: \"htpps://cerberus.example.com\",\nuser_config.cbs_sdb_path: \"\",\nuser_config.cbs_smtp_password: \"\",\n# user_config.secret_type: \"databricks\",\n# user_config.dbx_workspace_url: \"https://workspace.cloud.databricks.com\",\n# user_config.dbx_secret_scope: \"your_secret_scope\",\n# user_config.dbx_smtp_password: \"your_password\",\n}\n# Commented fields are optional or required when notifications are enabled\nuser_conf = {\nuser_config.se_notifications_enable_email: False,\n# user_config.se_notifications_enable_smtp_server_auth: False,\n# user_config.se_notifications_enable_custom_email_body: True,\n# user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n# user_config.se_notifications_email_smtp_port: 25,\n# user_config.se_notifications_smtp_password: \"your_password\",\n# user_config.se_notifications_smtp_creds_dict: smtp_creds_dict,\n# user_config.se_notifications_email_from: \"\",\n# user_config.se_notifications_email_to_other_mail_id: \"\",\n# user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n# user_config.se_notifications_email_custom_body: \"Custom statistics: 'product_id': {}\",\nuser_config.se_notifications_enable_slack: False,\n# user_config.se_notifications_slack_webhook_url: \"\",\n# user_config.se_notifications_on_start: True,\n# user_config.se_notifications_on_completion: True,\n# user_config.se_notifications_on_fail: True,\n# user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n# user_config.se_notifications_on_error_drop_threshold: 15,\n# user_config.se_enable_error_table: True,\n# user_config.enable_query_dq_detailed_result: True,\n# user_config.enable_agg_dq_detailed_result: True,\n# user_config.se_dq_rules_params: { \"env\": \"local\", \"table\": \"product\", },\n}\n@se.with_expectations(\ntarget_table=\"dq_spark_local.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\nreturn _df_order\n</code></pre>"},{"location":"key_prompts_for_repo/","title":"Key Prompts for the spark-expectations Repository","text":"<p>This document provides over 50 important prompts to help you explore, configure, develop, and troubleshoot the spark-expectations project. Use these prompts with GitHub Copilot Chat or as a checklist for your work.</p>"},{"location":"key_prompts_for_repo/#getting-started-setup","title":"Getting Started &amp; Setup","text":"<ol> <li>How do I set up the development environment?</li> <li>How do I install dependencies for this project?</li> <li>How do I run the sample notebooks?</li> <li>How do I start the local SparkExpectations environment with Docker?</li> <li>How do I generate mail server certificates for local testing?</li> <li>How do I build the documentation?</li> <li>How do I remove all hatch environments?</li> <li>How do I upgrade project dependencies?</li> <li>How do I install Spark-Expectations from a specific git branch?</li> <li>How do I install Spark-Expectations from PyPI?</li> </ol>"},{"location":"key_prompts_for_repo/#data-quality-rules-configuration","title":"Data Quality Rules &amp; Configuration","text":"<ol> <li>How do I create the rules table for data quality checks?</li> <li>What columns are required in the rules table?</li> <li>What are the valid values for the <code>rule_type</code> column?</li> <li>How do I add constraints to the rules table?</li> <li>How do I define a row-level data quality rule?</li> <li>How do I define an aggregation data quality rule?</li> <li>How do I define a query data quality rule?</li> <li>What does the <code>action_if_failed</code> column do?</li> <li>How do I set up error drop alerts for rules?</li> <li>How do I set rule priorities (low, medium, high)?</li> <li>How do I enable or disable specific rules?</li> <li>How do I use tags and descriptions for rules?</li> <li>How do I enable source or target DQ validation for a rule?</li> <li>How do I set a custom delimiter for query DQ rules?</li> <li>How do I enable custom output tables for query DQ?</li> </ol>"},{"location":"key_prompts_for_repo/#running-using-spark-expectations","title":"Running &amp; Using Spark Expectations","text":"<ol> <li>How do I run a SparkExpectations job on my data?</li> <li>How do I configure the stats table for logging results?</li> <li>How do I use the <code>WrappedDataFrameWriter</code>?</li> <li>How do I pass user configuration to a SparkExpectations job?</li> <li>How do I run DQ checks and handle failures?</li> <li>How do I use the <code>with_expectations</code> decorator?</li> <li>How do I view the output tables generated by SparkExpectations?</li> <li>How do I use the sample scripts in <code>spark_expectations/examples</code>?</li> <li>How do I use the sample notebooks in the <code>notebooks</code> directory?</li> </ol>"},{"location":"key_prompts_for_repo/#notifications-observability","title":"Notifications &amp; Observability","text":"<ol> <li>How do I enable email notifications for DQ runs?</li> <li>How do I configure SMTP settings for email notifications?</li> <li>How do I enable Slack notifications?</li> <li>How do I set the Slack webhook URL?</li> <li>How do I customize the email alert template?</li> <li>How do I enable notifications on job start, completion, or failure?</li> <li>How do I enable error drop threshold breach notifications?</li> <li>How do I use the observability features and report tables?</li> <li>How do I generate and view DQ reports?</li> </ol>"},{"location":"key_prompts_for_repo/#testing-quality","title":"Testing &amp; Quality","text":"<ol> <li>How do I run all tests?</li> <li>How do I run only integration or unit tests?</li> <li>How do I check code formatting and linting?</li> <li>How do I run type checks with mypy?</li> <li>How do I check test coverage?</li> <li>How do I add a new test for a custom rule or sink?</li> <li>How do I contribute to the project and submit a pull request?</li> <li>How do I follow the code style and linting standards?</li> <li>How do I update the documentation with new features?</li> <li>How do I troubleshoot failing tests or jobs?</li> <li>How do I use the example DQ pipelines for BigQuery, Delta, or Iceberg?</li> <li>How do I override the SparkExpectations version in a notebook?</li> </ol> <p>For answers and details, refer to the <code>docs/</code>, <code>CONTRIBUTING.md</code>, <code>spark_expectations/</code>, and <code>notebooks/</code> directories in this repository.</p>"},{"location":"api/sample_dq_bigquery/","title":"Sample dq bigquery","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery-attributes","title":"Attributes","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.current_dir","title":"<code>spark_expectations.examples.sample_dq_bigquery.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.dic_job_info","title":"<code>spark_expectations.examples.sample_dq_bigquery.dic_job_info = {'job': 'job_name', 'Region': 'NA', 'Snapshot': '2024-04-15'}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.job_info","title":"<code>spark_expectations.examples.sample_dq_bigquery.job_info = str(dic_job_info)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.se","title":"<code>spark_expectations.examples.sample_dq_bigquery.se: SparkExpectations = SparkExpectations(product_id='your_product', rules_df=spark.read.format('bigquery').load('&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;rules_table&gt;'), stats_table='&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;stats_table&gt;', stats_table_writer=writer, target_and_error_table_writer=writer, debugger=False)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.spark","title":"<code>spark_expectations.examples.sample_dq_bigquery.spark = set_up_bigquery('&lt;temp_dataset&gt;')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.user_conf","title":"<code>spark_expectations.examples.sample_dq_bigquery.user_conf: Dict[str, Union[str, int, bool, Dict[str, str]]] = {user_config.se_notifications_enable_email: False, user_config.se_notifications_email_smtp_host: 'mailhost.com', user_config.se_notifications_email_smtp_port: 25, user_config.se_notifications_email_from: '', user_config.se_notifications_email_to_other_mail_id: '', user_config.se_notifications_email_subject: 'spark expectations - data quality - notifications', user_config.se_notifications_enable_slack: False, user_config.se_notifications_slack_webhook_url: '', user_config.se_notifications_on_start: True, user_config.se_notifications_on_completion: True, user_config.se_notifications_on_fail: True, user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True, user_config.se_notifications_on_error_drop_threshold: 15, user_config.se_enable_query_dq_detailed_result: True, user_config.se_enable_agg_dq_detailed_result: True, user_config.se_enable_error_table: True, user_config.se_dq_rules_params: {'env': 'local', 'table': 'product'}, user_config.se_job_metadata: job_info}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.writer","title":"<code>spark_expectations.examples.sample_dq_bigquery.writer = WrappedDataFrameWriter().mode('overwrite').format('bigquery').option('createDisposition', 'CREATE_IF_NEEDED').option('writeMethod', 'direct')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery-classes","title":"Classes","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery-functions","title":"Functions","text":""},{"location":"api/sample_dq_bigquery/#spark_expectations.examples.sample_dq_bigquery.build_new","title":"<code>spark_expectations.examples.sample_dq_bigquery.build_new() -&gt; DataFrame</code>","text":"Source code in <code>.venv/env/virtual/dev.py3.12/lib/python3.12/site-packages/spark_expectations/examples/sample_dq_bigquery.py</code> <pre><code>@se.with_expectations(\ntarget_table=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_name&gt;\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;target_table_view_name&gt;\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order.csv\"))\n)\n_df_order.createOrReplaceTempView(\"order\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer.csv\"))\n)\n_df_customer.createOrReplaceTempView(\"customer\")\nreturn _df_order\n</code></pre>"},{"location":"api/sample_dq_delta/","title":"Sample dq delta","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta-attributes","title":"Attributes","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.current_dir","title":"<code>spark_expectations.examples.sample_dq_delta.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.dic_job_info","title":"<code>spark_expectations.examples.sample_dq_delta.dic_job_info = {'job': 'job_name', 'Region': 'NA', 'env': 'dev', 'Snapshot': '2024-04-15', 'data_object_name ': 'customer_order'}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.job_info","title":"<code>spark_expectations.examples.sample_dq_delta.job_info = str(dic_job_info)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.se","title":"<code>spark_expectations.examples.sample_dq_delta.se: SparkExpectations = SparkExpectations(product_id='your_product', rules_df=spark.table('dq_spark_dev.dq_rules'), stats_table='dq_spark_dev.dq_stats', stats_table_writer=writer, target_and_error_table_writer=writer, debugger=False)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.spark","title":"<code>spark_expectations.examples.sample_dq_delta.spark = set_up_delta()</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.user_conf","title":"<code>spark_expectations.examples.sample_dq_delta.user_conf: Dict[str, Union[str, int, bool, Dict[str, str]]] = {user_config.se_notifications_smtp_password: 'w*******', user_config.se_notifications_smtp_creds_dict: {user_config.secret_type: 'cerberus', user_config.cbs_url: 'https://cerberus.example.com', user_config.cbs_sdb_path: 'your_sdb_path', user_config.cbs_smtp_password: 'your_smtp_password'}, user_config.se_notifications_enable_smtp_server_auth: False, user_config.se_enable_obs_dq_report_result: False, user_config.se_dq_obs_alert_flag: False, user_config.se_dq_obs_default_email_template: '', user_config.se_notifications_enable_email: False, user_config.se_notifications_enable_custom_email_body: False, user_config.se_notifications_email_smtp_host: 'smtp.office365.com', user_config.se_notifications_email_smtp_port: 587, user_config.se_notifications_email_from: 'a.dsm.*****.com', user_config.se_notifications_email_to_other_mail_id: 'abc@mail.com', user_config.se_notifications_email_subject: 'spark expectations - data quality - notifications', user_config.se_notifications_email_custom_body: \"Spark Expectations Statistics for this dq run:\\n    'product_id': {},\\n    'table_name': {},\\n    'source_agg_dq_results': {}',\\n    'dq_status': {}\", user_config.se_notifications_enable_slack: False, user_config.se_notifications_slack_webhook_url: '', user_config.se_notifications_on_start: False, user_config.se_notifications_on_completion: False, user_config.se_notifications_on_fail: False, user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True, user_config.se_notifications_on_error_drop_threshold: 15, user_config.se_enable_query_dq_detailed_result: True, user_config.se_enable_agg_dq_detailed_result: True, user_config.se_enable_error_table: True, user_config.se_dq_rules_params: {'env': 'dev', 'table': 'product', 'data_object_name': 'customer_order', 'data_source': 'customer_source', 'data_layer': 'Integrated'}, user_config.se_job_metadata: job_info}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.writer","title":"<code>spark_expectations.examples.sample_dq_delta.writer = WrappedDataFrameWriter().mode('append').format('delta')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta-classes","title":"Classes","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta-functions","title":"Functions","text":""},{"location":"api/sample_dq_delta/#spark_expectations.examples.sample_dq_delta.build_new","title":"<code>spark_expectations.examples.sample_dq_delta.build_new() -&gt; DataFrame</code>","text":"Source code in <code>.venv/env/virtual/dev.py3.12/lib/python3.12/site-packages/spark_expectations/examples/sample_dq_delta.py</code> <pre><code>@se.with_expectations(\ntarget_table=\"dq_spark_dev.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_s.csv\"))\n)\n_df_order_source.createOrReplaceTempView(\"order_source\")\n_df_order_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_t.csv\"))\n)\n_df_order_target.createOrReplaceTempView(\"order_target\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_source.createOrReplaceTempView(\"customer_source\")\n_df_customer_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_target.createOrReplaceTempView(\"customer_target\")\nreturn _df_order_source\n</code></pre>"},{"location":"api/sample_dq_iceberg/","title":"Sample dq iceberg","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg-attributes","title":"Attributes","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.current_dir","title":"<code>spark_expectations.examples.sample_dq_iceberg.current_dir = os.path.dirname(os.path.abspath(__file__))</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.dic_job_info","title":"<code>spark_expectations.examples.sample_dq_iceberg.dic_job_info = {'job': 'job_name', 'Region': 'NA', 'Snapshot': '2024-04-15'}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.job_info","title":"<code>spark_expectations.examples.sample_dq_iceberg.job_info = str(dic_job_info)</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.se","title":"<code>spark_expectations.examples.sample_dq_iceberg.se: SparkExpectations = SparkExpectations(product_id='your_product', rules_df=spark.sql('select * from dq_spark_local.dq_rules'), stats_table='dq_spark_local.dq_stats', stats_table_writer=writer, target_and_error_table_writer=writer, debugger=False, stats_streaming_options={user_config.se_enable_streaming: False})</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.spark","title":"<code>spark_expectations.examples.sample_dq_iceberg.spark = set_up_iceberg()</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.user_conf","title":"<code>spark_expectations.examples.sample_dq_iceberg.user_conf: Dict[str, Union[str, int, bool, Dict[str, str]]] = {user_config.se_notifications_enable_email: False, user_config.se_notifications_email_smtp_host: 'mailhost.com', user_config.se_notifications_email_smtp_port: 25, user_config.se_notifications_email_from: '', user_config.se_notifications_email_to_other_mail_id: '', user_config.se_notifications_email_subject: 'spark expectations - data quality - notifications', user_config.se_notifications_enable_slack: False, user_config.se_notifications_slack_webhook_url: '', user_config.se_notifications_on_start: True, user_config.se_notifications_on_completion: True, user_config.se_notifications_on_fail: True, user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True, user_config.se_notifications_on_error_drop_threshold: 15, user_config.se_enable_query_dq_detailed_result: True, user_config.se_enable_agg_dq_detailed_result: True, user_config.se_enable_error_table: True, user_config.se_dq_rules_params: {'env': 'local', 'table': 'product'}, user_config.se_job_metadata: job_info}</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.writer","title":"<code>spark_expectations.examples.sample_dq_iceberg.writer = WrappedDataFrameWriter().mode('append').format('iceberg')</code>  <code>module-attribute</code>","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg-classes","title":"Classes","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg-functions","title":"Functions","text":""},{"location":"api/sample_dq_iceberg/#spark_expectations.examples.sample_dq_iceberg.build_new","title":"<code>spark_expectations.examples.sample_dq_iceberg.build_new() -&gt; DataFrame</code>","text":"Source code in <code>.venv/env/virtual/dev.py3.12/lib/python3.12/site-packages/spark_expectations/examples/sample_dq_iceberg.py</code> <pre><code>@se.with_expectations(\ntarget_table=\"dq_spark_local.customer_order\",\nwrite_to_table=True,\nuser_conf=user_conf,\ntarget_table_view=\"order\",\n)\ndef build_new() -&gt; DataFrame:\n_df_order_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_s.csv\"))\n)\n_df_order_source.createOrReplaceTempView(\"order_source\")\n_df_order_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/order_t.csv\"))\n)\n_df_order_target.createOrReplaceTempView(\"order_target\")\n_df_product: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/product.csv\"))\n)\n_df_product.createOrReplaceTempView(\"product\")\n_df_customer_source: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_source.createOrReplaceTempView(\"customer_source\")\n_df_customer_target: DataFrame = (\nspark.read.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(os.path.join(os.path.dirname(__file__), \"resources/customer_source.csv\"))\n)\n_df_customer_target.createOrReplaceTempView(\"customer_target\")\nreturn _df_order_source\n</code></pre>"},{"location":"configurations/configure_rules/","title":"Configure Rules","text":""},{"location":"configurations/configure_rules/#configure-rules-in-the-catalogschemaproduct_rules","title":"Configure Rules in the <code>catalog</code>.<code>schema</code>.<code>{product}_rules</code>","text":"<p>Please find the data set which used for the data quality rules setup order.csv</p>"},{"location":"configurations/configure_rules/#example-of-row-aggregation-and-query-rules-for-data-quality","title":"Example Of Row, Aggregation And Query Rules For Data Quality","text":"<p>To perform row data quality checks for artificially order table, please set up rules using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description, enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active, enable_error_drop_alert, error_drop_threshold ,query_dq_delimiter,enable_querydq_custom_output) values\n--The row data quality has been set on customer_id when customer_id is null, drop respective row into error table \n--as \"action_if_failed\" tagged \"drop\"\n('apla_nd', '`catalog`.`schema`.customer_order',  'row_dq', 'customer_id_is_not_null', 'customer_id', 'customer_id is not null','drop', 'validity', 'customer_id should not be null', false, false, true,false, 0,null, null)\n--The row data quality has been set on sales when sales is less than zero, drop respective row into error table as \n--'action_if_failed' tagged \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'sales_greater_than_zero', 'sales', 'sales &gt; 0', 'drop', 'accuracy', 'sales value should be greater than zero', false, false, true,false, 0,null, null)\n--The row data quality has been set on discount when discount is less than 60, drop respective row into error table\n--and final table  as \"action_if_failed\" tagged 'ignore'\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'discount_threshold', 'discount', 'discount*100 &lt; 60',\n'ignore', 'validity', 'discount should be less than 40', false, false, true,false, 0,null, null)\n--The row data quality has been set on ship_mode when ship_mode not in (\"second class\", \"standard class\", \n--\"standard class\"), drop respective row into error table and fail the framework  as \"action_if_failed\" tagged \"fail\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'ship_mode_in_set', 'ship_mode', 'lower(trim(ship_mode))\nin('second class', 'standard class', 'standard class')', 'fail', 'validity', 'ship_mode mode belongs in the sets',\nfalse, false, true,false, 0,null, null)\n--The row data quality has been set on profit when profit is less than or equals to 0, drop respective row into \n--error table and final table as \"action_if_failed\" tagged \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'profit_threshold', 'profit', 'profit&gt;0', 'ignore', 'validity', 'profit threshold should be greater than 0', false, false, true,false, 0,null, null)\n--The rule has been established to identify and remove completely identical records in which rows repeat with the \n--same value more than once, while keeping one instance of the row. Any additional duplicated rows will be dropped \n--into error table as action_if_failed set to \"drop\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'row_dq', 'complete_duplicate', 'All', 'row_number() \n over(partition by customer_id, order_id order by 1)=1', 'drop', 'uniqueness', 'drop complete duplicate records', false, false, true,false, 0,null, null)\n</code></pre> <p>Please set up rules for checking the quality of the columns in the artificial order table, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description,  enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active, enable_error_drop_alert, error_drop_threshold ,query_dq_delimiter,enable_querydq_custom_output) values\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales values falls below 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales', 'sales', 'sum(sales)&gt;10000', 'ignore', 'validity', 'sum of sales must be greater than 10000',  true, true, true,false, 0,null, null)\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales values falls between 1000 and 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales_range_type1', 'sales', 'sum(sales) between 1000 and 10000', 'ignore', 'validity', 'sum of sales must be between 1000 and 1000',  true, true, true)\n--The aggregation rule is established on the 'sales' column and the metadata of the rule will be captured in the \n--statistics table when the sum of the sales value is greater than 1000 and less than 10000\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'sum_of_sales_range_type2', 'sales', 'sum(sales)&gt;1000 and sum(sales)&lt;10000', 'ignore', 'validity', 'sum of sales must be greater than 1000 and less than 10000',  true, true, true)\n--The aggregation rule is established on the 'ship_mode' column and the metadata of the rule will be captured in \n--the statistics table when distinct ship_mode greater than 3 and enabled for only source data set\n,('apla_nd', '`catalog`.`schema`.customer_order', 'agg_dq', 'distinct_of_ship_mode', 'ship_mode', 'count(distinct ship_mode)&lt;=3', 'ignore', 'validity', 'regex format validation for quantity', true, false, true,false, 0,null, null)\n-- The aggregation rule is established on the table count and the metadata of the rule will be captured in the \n--statistics table when distinct count greater than 10000 and fails the job as \"action_if_failed\" set to \"fail\" \n--and enabled only for validated dataset\n,('apla_nd', '`catalog`.`schema`..customer_order', 'agg_dq', 'row_count', '*', 'count(*)&gt;=10000', 'fail', 'validity',\n'distinct ship_mode must be less or equals to 3', false, true, true,false, 0,null, null)\n</code></pre> <p>Please set up rules for checking the quality of artificially order table by implementing query data quality option, using the specified format</p> <pre><code>insert into `catalog`.`schema`.`{product}_rules` (product_id, table_name, rule_type, rule, column_name, expectation, action_if_failed, tag, description, enable_for_source_dq_validation,  enable_for_target_dq_validation, is_active, enable_error_drop_alert, error_drop_threshold ,query_dq_delimiter,enable_querydq_custom_output) values\n--The query dq rule is established to check product_id difference between two table if difference is more than 20% \n--from source table, the metadata of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\"\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_missing_count_threshold', '*', '((select count(distinct product_id) from {table}) - (select count(distinct product_id) from order))&gt;\n(select count(distinct product_id) from product)*0.2', 'ignore', 'validity', 'row count threshold difference must \nbe less than 20%', true, true, true,false, 0,null, null)\n--The query dq rule is established to check distinct product_id in the product table is less than 5, if not the \n--metadata of the rule will be captured in the statistics table along with fails the job as \"action_if_failed\" is \n--\"fail\" and enabled for source dataset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'product_category', '*', '(select count(distinct category) \nfrom {table}) &lt; 5', 'fail', 'validity', 'distinct product category must be less than 5', true, False, true,false, 0,null, null)\n--The query dq rule is established to check count of the dataset should be less than 10000 other wise the metadata \n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled only for target dataset\n,('apla_nd', '`catalog`.`schema`.customer_order', 'query_dq', 'row_count_in_order', '*', '(select count(*) from order)&lt;10000', 'ignore', 'accuracy', 'count of the row in order dataset must be less then 10000', false, true, true,false, 0,null, null)\n--The query dq rule is established to check count of the unique productid and orderid between order_source and order_target dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"true\". This will capture the source_f1 and target_f1 alias queries in a query_dq custom output table. \n--This custom table can be passed in the user config : user_config.querydq_output_custom_table_name: &lt;catalog.schema.table_name&gt;. \n--If this value is not passed, the query_dq custom output table will be created by appending \"custom_output\" as suffix to \n--the stats_table parameter passed with SparkExpectation class invoke. \"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries.\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"product_missing_count_threshold\", \"*\", \"((select count(*) from ({source_f1}) a) - (select count(*) from ({target_f1}) b) ) &lt; 3$source_f1$select distinct product_id,order_id \nfrom order_source$target_f1$select distinct product_id,order_id from order_target\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0,null, true)\n--The query dq rule is established to check count of the customer_id counts between customer_source and customer_target dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"true\". This will capture the source_f1 and target_f1 alias queries in a query_dq custom output table. \n--This custom table can be passed in the user config : user_config.querydq_output_custom_table_name: &lt;catalog.schema.table_name&gt;.\n-- The alias can be anything but if it is passed in the format source_&lt;key_name&gt; and target_&lt;key_name&gt;, the query_dq custom output table will capture the the source and target in the same row which will help in easy comparision. \n--If user_config.querydq_output_custom_table_name is not passed, the query_dq custom output table will be created by appending \"custom_output\" as suffix to the stats_table \n--parameter passed with SparkExpectation class invoke. \"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries.\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"customer_missing_count_threshold\",\"*\", \"((select count(*) from ({source_f1}) a join ({source_f2}) b on a.customer_id = b.customer_id) - (select count(*) \nfrom ({target_f1}) a join ({target_f2}) b on a.customer_id = b.customer_id)) &gt; ({target_f3})$source_f1$select customer_id, count(*) \nfrom customer_source group by customer_id$source_f2$select customer_id, \ncount(*) from order_source group by customer_id$target_f1$select customer_id, count(*) from customer_target \ngroup by customer_id$target_f2$select customer_id, count(*) from order_target group by customer_id$target_f3$select count(*) \nfrom order_source\", \"ignore\", \"validity\", \"customer count threshold\", true, true, true, false, 0,null, true)\n--The query dq rule is established to check count of the unique productid and orderid between order_source and order_target dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"false\". This will not capture the source_f1 alias query to query_dq custom output table. \n--\"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries. Default value is '$'. Her it is overrided with '@'\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"order_count_validity\", \"*\", \"({source_f1}) &gt; 10@source_f1@select count(*) \nfrom order_source\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0, \"@\", false)\n--The query dq rule is established to check count of the order_source dataset. The output\n--of the rule will be captured in the statistics table as \"action_if_failed\" is \"ignore\" and enabled for both source and target dataset. \n--The enable_querydq_custom_output is set to \"true\". Though it is \"true\" since there is no alias query in the expectaion, \n--there will not be any output to be captured in the query_dq custom output table. \n-- \"query_dq_delimiter\" is the optional param to have the a specific delimiter for alias queries. Default value is '$'.\n,(\"na_nd\", \"`catalog`.`schema`.customer_order\", \"query_dq\", \"order_count_validity_check\", \"*\", \"(select count(*) \nfrom order_source) &gt; 10\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0, null, true)\n</code></pre>"},{"location":"configurations/databricks_setup_guide/","title":"Databricks setup guide","text":""},{"location":"configurations/databricks_setup_guide/#effortlessly-explore-spark-expectations-on-example-dataset-with-automated-setup-in-databricks","title":"Effortlessly Explore Spark Expectations on Example Dataset with Automated Setup in Databricks","text":"<p>This section provides instructions on how to set up a sample notebook in the Databricks environment to investigate, comprehend, and conduct a feasibility study on the Spark Expectations framework.</p>"},{"location":"configurations/databricks_setup_guide/#prerequisite","title":"Prerequisite:","text":"<ol> <li>Recommended Databricks run time environment for better experience - DBS 11.0 and above</li> <li>Please install the Kafka jar using the path <code>dbfs:/kafka-jars/databricks-shaded-strimzi-kafka-oauth-client-1.1.jar</code>, If the jar is not available in the dbfs location, please raise a ticket with Platform team to add the jar to your workspace</li> <li>Please follow the steps provided here to integrate and clone repo from git Databricks</li> <li>Please follow the steps to create the webhook-hook URL for team-specific channel here</li> </ol>"},{"location":"configurations/rules/","title":"Rules","text":""},{"location":"configurations/rules/#different-types-of-expectations","title":"Different Types of Expectations","text":"<p>Please find the different types of possible expectations </p>"},{"location":"configurations/rules/#possible-row-data-quality-expectations","title":"Possible Row Data Quality Expectations","text":"rule_description category tag rule_expectation Expect that the values in the column should not be null/empty null_validation completeness <code>[col_name] is not null</code> Ensure that the primary key values are unique and not duplicated primary_key_validation uniqueness <code>count(*) over(partition by [primary_key_or_combination_of_primary_key] order by 1)=1</code> Perform a thorough check to make sure that there are no duplicate values, if there are duplicates preserve one row into target complete_duplicate_validation uniqueness <code>row_number() over(partition by [all_the_column_in_dataset_b_ comma_separated] order by 1)=1</code> Verify that the date values are in the correct format date_format_validation validity <code>to_date([date_col_name], '[mention_expected_date_format]') is not null</code> Verify that the date values are in the correct format using regex date_format_validation_with_regex validity <code>[date_col_name] rlike '[regex_format_of_date]'</code> Expect column value is date parseable expect_column_values_to_be_date_parseable validity <code>try_cast([date_col_name] as date)</code> Verify values in a column to conform to a specified regular expression pattern expect_column_values_to_match_regex validity <code>[col_name]  rlike '[regex_format]'</code> Verify values in a column to not conform to a specified regular expression pattern expect_column_values_to_not_match_regex validity <code>[col_name] not rlike '[regex_format]'</code> Verify values in a column to match regex in list expect_column_values_to_match_regex_list validity <code>[col_name] not rlike '[regex format1]' or [col_name] not rlike '[regex_format2]' or [col_name] not rlike '[regex_format3]'</code> Expect the values in a column to belong to a specified set expect_column_values_to_be_in_set accuracy <code>[col_name] in ([values_in_comma_separated])</code> Expect the values in a column not to belong to a specified set expect_column_values_to_be_not_in_set accuracy <code>[col_name] not in ([values_in_comma_separated])</code> Expect the values in a column to fall within a defined range expect_column_values_to_be_in_range accuracy <code>[col_name] between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be within a specified range expect_column_value_lengths_to_be_between accuracy <code>length([col_name]) between [min_threshold] and [max_threshold]</code> Expect the lengths of the values in a column to be equal to a certain value expect_column_value_lengths_to_be_equal accuracy <code>length([col_name])=[threshold]</code> Expect values in the column to exceed a certain limit expect_column_value_to_be_greater_than accuracy <code>[col_name] &gt; [threshold_value]</code> Expect values in the column  not to exceed a certain limit expect_column_value_to_be_lesser_than accuracy <code>[col_name] &lt; [threshold_value]</code> Expect values in the column to be equal to or exceed a certain limit expect_column_value_greater_than_equal accuracy <code>[col_name] &gt;= [threshold_value]</code> Expect values in the column to be equal to or not exceed a certain limit expect_column_value_lesser_than_equal accuracy <code>[col_name] &lt;= [threshold_value]</code> Expect values in column A to be greater than values in column B expect_column_pair_values_A_to_be_greater_than_B accuracy <code>[col_A] &gt; [col_B]</code> Expect values in column A to be lesser than values in column B expect_column_pair_values_A_to_be_lesser_than_B accuracy <code>[col_A] &lt; [col_B]</code> Expect values in column A to be greater than or equals to values in column B expect_column_A_to_be_greater_than_B accuracy <code>[col_A] &gt;= [col_B]</code> Expect values in column A to be lesser than or equals to values in column B expect_column_A_to_be_lesser_than_or_equals_B accuracy <code>[col_A] &lt;= [col_B]</code> Expect the sum of values across multiple columns to be equal to a certain value expect_multicolumn_sum_to_equal accuracy <code>[col_1] + [col_2] + [col_3] = [threshold_value]</code> Expect sum of values in each category equals certain value expect_sum_of_value_in_subset_equal accuracy <code>sum([col_name]) over(partition by [category_col] order by 1)</code> Expect count of values in each category equals certain value expect_count_of_value_in_subset_equal accuracy <code>count(*) over(partition by [category_col] order by 1)</code> Expect distinct value in each category exceeds certain range expect_distinct_value_in_subset_exceeds accuracy <code>count(distinct [col_name]) over(partition by [category_col] order by 1)</code>"},{"location":"configurations/rules/#possible-aggregation-data-quality-expectations","title":"Possible Aggregation Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column that are present in a given list expect_column_distinct_values_to_be_in_set accuracy <code>array_intersect(collect_list(distinct [col_name]), Array($compare_values_string)) = Array($compare_values_string)</code> Expect the mean value of a column to fall within a specified range expect_column_mean_to_be_between consistency <code>avg([col_name]) between [lower_bound] and [upper_bound]</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>percentile_approx([column_name], 0.5) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>stddev([col_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>count(distinct [col_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>max([col_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_sum_to_be_between accuracy <code>min([col_name]) between [lower_bound] and [upper_bound]</code> Expect row count of the dataset fall within certain range expect_row_count_to_be_between accuracy <code>count(*) between [lower_bound] and [upper_bound]</code> Expect row count of the dataset fall within certain range expect_row_count_to_be_in_range accuracy <code>count(*) &gt;[lower_bound] and count(*) &lt; [upper_bound]</code>"},{"location":"configurations/rules/#possible-query-data-quality-expectations","title":"Possible Query Data Quality Expectations","text":"rule_description rule_type tag rule_expectation Expect distinct values in a column must be greater than threshold value expect_column_distinct_values_greater_than_threshold_value accuracy <code>(select count(distinct [col_name]) from [table_name]) &gt; [threshold_value]</code> Expect count between two table or view must be same expect_count_between_two_table_same consistency <code>(select count(*) from [table_a]) = (select count(*) from [table_b])</code> Expect the median value of a column to be within a certain range expect_column_median_to_be_between consistency <code>(select percentile_approx([column_name], 0.5) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the standard deviation of a column's values to fall within a specified range expect_column_stdev_to_be_between consistency <code>(select stddev([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the count of unique values in a column to fall within a specified range expect_column_unique_value_count_to_be_between accuracy <code>(select count(distinct [col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the maximum value in a column to fall within a specified range expect_column_max_to_be_between accuracy <code>(select max([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect the minimum value in a column fall within a specified range expect_column_min_to_be_between accuracy <code>(select min([col_name]) from [table_name]) between [lower_bound] and [upper_bound]</code> Expect referential integrity expect_referential_integrity_between_two_table_should_be_less_than_100 accuracy <code>( select * from [table_a] left join [table_b] on [condition] where [table_b.column] is null) select count(*) from refrentail_check) &lt; 100</code> Compare the source table and target table output (by default @ is the delimiter. can be overriden by query_dq_delimiter atttribute in rules table) customer_missing_count_threshold validity The_alias_within_the_curly_bracket_is_added_to_the_expectation_which_gets_resolved_at_compile_time_with_alias_values<code>((select count(*) from ({source_f1}) a join ({source_f2}) b on a.customer_id = b.customer_id) - (select count(*) from ({target_f1}) a join ({target_f2}) b on source_column = target_column)) &gt; ({target_f3})@source_f1@select column, count(*) from source_tbl group by column@source_f2@select column2, count(*) from table2 group by column2@target_f1@select column, count(*) from target_tbl  group by column@target_f2@select column2, count(*) from target_tbl2 group by column2@target_f3@select count(*) from source_tbl</code>"},{"location":"developer_guide/contributing/","title":"Contributing","text":"<ul> <li>Guidelines</li> </ul>"},{"location":"developer_guide/setup/","title":"Setup","text":""},{"location":"developer_guide/setup/#prerequisites","title":"Prerequisites","text":""},{"location":"developer_guide/setup/#python","title":"Python","text":"<ul> <li>Supported versions: 3.9, 3.10, 3.11, 3.12 (recommended: latest 3.12.x)</li> <li>Check version: <pre><code>    python3 --version\n</code></pre></li> <li>Install Python: You can install Python by downloading it from python.org or by using a version manager such as pyenv.</li> </ul>"},{"location":"developer_guide/setup/#hatch","title":"Hatch","text":"<ul> <li>Install Hatch <pre><code>  # MacOS\nbrew install hatch\n</code></pre></li> </ul>"},{"location":"developer_guide/setup/#java","title":"Java","text":"<ul> <li>Supported versions: 8, 11, 17 (recommended: latest 17.x)</li> <li>Recommendation: Use a JDK version manager such as SDKMAN! or OpenJDK(Linux/macOS) or an equivalent tool for your operating system to install and manage Java versions.</li> <li>JAVA_HOME: If your tools or environment require it, set the <code>JAVA_HOME</code> environment variable to point to your installed JDK. Refer to your JDK version manager\u2019s documentation for instructions.</li> </ul>"},{"location":"developer_guide/setup/#ide","title":"IDE","text":"<ul> <li>Recommended: Visual Studio Code</li> <li>Other options: PyCharm</li> </ul>"},{"location":"developer_guide/setup/#docker","title":"Docker","text":"<ul> <li>Requirement: A container engine is required for running integration tests. You can use Docker, Podman, containerd, Rancher, or any equivalent container runtime for your operating system.</li> </ul>"},{"location":"developer_guide/setup/#github-configuration","title":"Github Configuration","text":"<ul> <li>Required: A GitHub account to access the source code and documentation.</li> </ul>"},{"location":"developer_guide/setup/#create-gpg-and-ssh-keys","title":"Create GPG and SSH Keys","text":"<ul> <li>GPG: Use GPG for signing commits and tags.   See GitHub Docs \u2013 Generate a GPG Key.</li> <li>SSH: Use SSH for connecting to and interacting with remote repositories.   See GitHub Docs \u2013 Generate an SSH Key.</li> <li>Clone the repository: <pre><code>git clone git@github.com:Nike-Inc/spark-expectations.git\n</code></pre></li> </ul>"},{"location":"developer_guide/setup/#environment-setup","title":"Environment Setup","text":"<p>We recommend using version manager tool like:</p> <ul> <li>pyenv - Python version manager</li> <li>asdf - Multiple language version manager<ul> <li>supported plugins</li> </ul> </li> </ul> <p>Create a virtual environment: <pre><code>    pyenv install -l | less # List available versions\npyenv install 3.12.11 # Install specific python version\npyenv versions # List installed versions\npyenv prefix # Display full path to the current installed python directory\npyenv global &lt;version&gt;   # sets default for all shells\npyenv local &lt;version&gt;    # sets version for current directory (creates .python-version)\n</code></pre></p> <p>Install Dependencies:</p> <p>This project uses Hatch for Python environment and dependency management.  </p> <p>All required and optional dependencies are managed via <code>pyproject.toml</code>. </p> <p>To initialize dev environment (python virtual environments) and install dependencies run following makefile target:</p> <pre><code>  # Configures  Hatch dev Environment \n# Initializes Python virtual environments ( 3.10,3.11,3.12)\n# Installs Dependencies\nmake dev\n</code></pre> <p>To view Hatch Environments:</p> <p>This shows which environments are available for development, testing, and other workflows, and how they are configured for your project.  <pre><code>hatch env show\n</code></pre></p> <p>Since <code>Hatch</code> is used as dependency management tool output of this command will be creation of multiple python virtual environments stored under project root <code>.venv/env/virtual/&lt;dev.py3.X&gt;</code></p> <p>Troubleshooting Hatch Environments:</p> <p>If you encounter issues, try cleaning and recreating the environment. <pre><code>make env-remove-all\n# Manual option would be to delete &lt;project_root&gt;./venv/ directory to delete hatch python virtual environments\n# Run following to re-create environments\nmake dev\n</code></pre></p>"},{"location":"developer_guide/setup/#running-tests","title":"Running Tests","text":"<p>To execture all spark-expectations tests run following command: </p> <pre><code>make cov\n</code></pre> <p>Warning</p> <p>Previous command will spin up docker container needed to execute some tests so make sure docker is running. Checkout <code>./spark_expectations/examples/docker_scripts</code> and test fixtures to better understand what is being spin up. For Example <pre><code>  sh ./spark_expectations/examples/docker_scripts/docker_kafka_start_script.sh  </code></pre> This script will:</p> <ul> <li>Build the required Docker image</li> <li>Start a Kafka service in a Docker container</li> <li>Make Kafka available for integration tests or local development</li> </ul>"},{"location":"developer_guide/setup/#ide-debugging","title":"IDE Debugging","text":"VSCode Interpreter and Launch Config settings.json <p>Setting default python interpreter be project created virtual environment <pre><code>{\n\"python.testing.pytestEnabled\": true,\n\"python.testing.autoTestDiscoverOnSaveEnabled\": true,\n\"python.defaultInterpreterPath\": \".venv/env/virtual/dev.py3.12/bin/python\",\n\"terminal.integrated.profiles.osx\": {\n\"Hatch Shell\": {\n\"path\": \"hatch\",\n\"args\": [\"shell\"],\n\"icon\": \"terminal\"\n}\n},\n\"terminal.integrated.defaultProfile.osx\": \"zsh\",\n\"python.testing.pytestArgs\": [\n\"--maxfail=1\",\n\"--disable-warnings\"\n]\n}\n</code></pre></p> launch.json <p>debug launch configurations <pre><code>{\n\"version\": \"0.2.0\",\n\"configurations\": [\n{\n\"name\": \"Coverage: Pytest (All files)\",\n\"type\": \"debugpy\",\n\"request\": \"launch\",\n\"module\": \"coverage\",\n\"args\": [\n\"run\",\n\"--source=spark_expectations\",\n\"--omit=spark_expectations/examples/*\",\n\"-m\", \"pytest\",\n\"-v\",\n\"-x\"\n],\n\"console\": \"integratedTerminal\",\n\"justMyCode\": false\n},\n{\n\"name\": \"Coverage: Pytest (Selected File)\",\n\"type\": \"debugpy\",\n\"request\": \"launch\",\n\"module\": \"coverage\",\n\"args\": [\n\"run\",\n\"--source=spark_expectations\",\n\"--omit=spark_expectations/examples/*\",\n\"-m\", \"pytest\",\n\"-v\",\n\"-x\",\n\"${file}\"\n],\n\"console\": \"integratedTerminal\",\n\"justMyCode\": false\n}\n]\n}\n</code></pre></p>"},{"location":"developer_guide/setup/#adding-certificates","title":"Adding Certificates","text":"<p>To enable trusted SSL/TLS communication during Spark-Expectations testing, you may need to provide custom Certificate Authority (CA) certificates.</p> <p>Place any required <code>.crt</code> files in the <code>spark_expectations/examples/docker_scripts/certs</code> directory. </p> <p>During test container startup, all certificates in this folder will be automatically imported into the container\u2019s trusted certificate store, ensuring that your Spark jobs and dependencies can establish secure connections as needed.</p>"},{"location":"developer_guide/setup/#deploying-the-docs-site-locally","title":"Deploying the Docs site locally","text":"<p>When updating the project documnetation, it is good idea to test your changes locally. You could deploy the server locally following these steps:</p> <pre><code># Installs Dependencies\nmake dev\n\n# Deploy the Docs server locally\nmake docs </code></pre>"},{"location":"home/why_spark_expectations/","title":"Concept","text":"<p>Most of the data quality tools do the data quality checks or data validation on a table at rest and provide metrics in  different forms. <code>While the existing tools are good to do profiling and provide metrics, below are the problems that we  commonly see</code> </p> <ul> <li>The existing tools do not perform any action or remove the malformed data in the original table </li> <li>Most existing frameworks do not offer the capability to perform both row and column level data quality checks  within a single tool.</li> <li>User have to manually check the provided metrics, and it becomes cumbersome to find the records which doesn't meet  the data quality standards</li> <li>Downstream users have to consume the same data with error, or they have to do additional computation to remove the  records that doesn't meet the standards</li> <li>Another process is required as a corrective action to rectify the errors in the data and lot of planning is usually  required for this activity</li> </ul> <p><code>Spark-Expectations solves all of the above problems by following the below principles</code></p> <ul> <li>Spark Expectations provides the ability to run both individual row-based and overall aggregated data quality rules  on both the source and validated data sets. In case a rules fails, the row-level error is recorded in the <code>_error</code> table  and a summarized report of all failed aggregated data quality rules is compiled in the <code>_stats</code> table. </li> <li>Spark Expectations optionally provides detailed, rule-level records showing the execution and results of each rule.  These per-rule validation results are captured in the <code>stats_detailed</code> table, allowing for thorough inspection and analysis.</li> <li>All the records which fail one or more data quality rules, are by default quarantined in an <code>_error</code> table along with  the metadata on rules that failed, job information etc. This helps analysts or products to look at the error data easily  and work with the teams required to correct the data and reprocess it easily</li> <li>Aggregated Metrics are provided on the job level along with necessary metadata so that recalculation or compute is  avoided</li> <li>The data that doesn't meet the data quality contract or the standards is not written into the final table unless or otherwise specified. </li> <li>By default, frameworks have the capability to send notifications only upon failure, but they have the ability to  send notifications at the start, as well as upon completion</li> </ul> <p>There is a field in the rules table called action_if_failed, which determines what needs to be done if a rule fails</p> <ul> <li>Let's consider a hypothetical scenario, where we have 100 columns and with 200 row level data quality rules, 10 aggregation data quality rules and 5 query data quality rules  computed against. When the dq job is run, there are 10 rules that failed on a particular row and 4 aggregation rules fails- what determines if that row should end up in  final table or not? Below are the hierarchy of checks that happens?</li> <li>Among the row level 10 rules failed, if there is at least one rule which has an action_if_failed as fail -    then the job will be failed </li> <li>Among the 10 row level rules failed, if there is no rule that has an action_if_failed as fail, but at least    has one rule with action_if_failed as drop - then the record/row will be dropped</li> <li>Among the 10 row level rules failed, if no rule neither has fail nor drop as an action_if_failed - then    the record will be end up in the final table. Note that, this record would also exist in the <code>_error</code> table</li> <li>The aggregation and query dq rules have a setting called <code>action_if_failed</code> with two options: <code>fail</code> or <code>ignore</code>. If any of   the 10 aggregation rules and 5 query dq rules which failed has an action_if_failed as fail, then the metadata summary will be    recorded in the <code>_stats</code> table and the job will be considered a failure. However, if none of the failed rules    has an action_if_failed as fail, then summary of the aggregated rules' metadata will still be collected in the    <code>_stats</code> table for failed aggregated and  query dq rules.</li> </ul>"},{"location":"user_guide/data_quality_metrics/","title":"Data Quality Metrics","text":""},{"location":"user_guide/data_quality_metrics/#dq-stats-table","title":"DQ Stats Table","text":"<p>In order to collect the stats/metrics for each data quality job run, the spark-expectations job will automatically create the stats table if it does not exist. </p> <p>Warning</p> <p>The below SQL statement can be used to create the table if you want to create it manually, but it is not recommended.</p> <pre><code>create table if not exists `catalog`.`schema`.`dq_stats` (\nproduct_id STRING,  -- (1)!\ntable_name STRING,  -- (2)!\ninput_count LONG,  -- (3)!\nerror_count LONG,  -- (4)!\noutput_count LONG,  -- (5)!\noutput_percentage FLOAT,  -- (6)!\nsuccess_percentage FLOAT,  -- (7)!\nerror_percentage FLOAT,  -- (8)!\nsource_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (9)!\nfinal_agg_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (10)!\nsource_query_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (11)!\nfinal_query_dq_results array&lt;map&lt;string, string&gt;&gt;,  -- (12)!\nrow_dq_res_summary array&lt;map&lt;string, string&gt;&gt;,  -- (13)!\nrow_dq_error_threshold array&lt;map&lt;string, string&gt;&gt;,  -- (14)!\ndq_status map&lt;string, string&gt;,  -- (15)!\ndq_run_time map&lt;string, float&gt;,  -- (16)!\ndq_rules map&lt;string, map&lt;string,int&gt;&gt;,  -- (17)!\nmeta_dq_run_id STRING,  -- (18)!\nmeta_dq_run_date DATE,  -- (19)!\nmeta_dq_run_datetime TIMESTAMP,  -- (20)!\ndq_env STRING,  -- (21)!\n);\n</code></pre> <ol> <li><code>product_id</code> A unique name at the level of dq rules execution</li> <li><code>table_name</code> The table for which the rule is being defined for</li> <li><code>input_count</code> total input row count of given dataframe</li> <li><code>error_count</code> total error count for all row_dq rules</li> <li><code>output_count</code> total count of records that passed the row_dq rules or configured to be ignored when they fail</li> <li><code>output_percentage</code> percentage of total count of records that passed the row_dq rules or configured to be ignored when they fail</li> <li><code>success_percentage</code> percentage of total count of records that passed the row_dq rules</li> <li><code>error_percentage</code> percentage of total count of records that failed the row_dq rules</li> <li><code>source_agg_dq_results</code> results for agg dq rules are stored after row_dq rules executed in an array with each entry as a map\\&lt;string, string&gt; containing the following keys: <code>description</code>, <code>tag</code>, <code>column_name</code>, <code>rule</code>, <code>rule_type</code>, <code>status</code>, <code>action_if_failed</code></li> <li><code>final_agg_dq_results</code> results for agg dq rules are stored after row_dq rules executed in an array with each entry as a map\\&lt;string, string&gt; containing the following keys: <code>description</code>, <code>tag</code>, <code>column_name</code>, <code>rule</code>, <code>rule_type</code>, <code>status</code>, <code>action_if_failed</code></li> <li><code>source_query_dq_results</code> results for query dq rules are stored after row_dq rules executed in an array with each entry as a map\\&lt;string, string&gt; containing the following keys: <code>description</code>, <code>tag</code>, <code>column_name</code>, <code>rule</code>, <code>rule_type</code>, <code>status</code>, <code>action_if_failed</code></li> <li><code>final_query_dq_results</code> results for query dq rules are stored after row_dq rules executed in an array with each entry as a map\\&lt;string, string&gt; containing the following keys: <code>description</code>, <code>tag</code>, <code>column_name</code>, <code>rule</code>, <code>rule_type</code>, <code>status</code>, <code>action_if_failed</code></li> <li><code>row_dq_res_summary</code> summary of row dq results are stored in an array with each entry as a map\\&lt;string, string&gt; containing the following keys: <code>description</code>, <code>tag</code>, <code>column_name</code>, <code>rule</code>, <code>rule_type</code>, <code>failed_row_count</code>, <code>action_if_failed</code></li> <li><code>row_dq_error_threshold</code> error threshold results for rules defined in the rules table for row_dq rules are stored in an array with each entry as a map\\&lt;string, string&gt; containing the following keys: <code>description</code>, <code>column_name</code> ,<code>error_drop_threshold</code>, <code>rule_type</code>, <code>action_if_failed</code>, <code>rule_name</code>, <code>error_drop_percentage</code></li> <li><code>dq_status</code>  stores the status of the rule execution.</li> <li><code>dq_run_time</code> time taken by the rules</li> <li><code>dq_rules</code> how many dq rules are executed in this run</li> <li><code>meta_dq_run_id</code> unique id generated for this run</li> <li><code>meta_dq_run_date</code> date on which rule is executed</li> <li><code>meta_dq_run_datetime</code> date and time on which rule is executed</li> <li><code>dq_env</code> environment value passed from the user_config.se_dq_rules_params</li> </ol>"},{"location":"user_guide/data_quality_metrics/#dq-detailed-stats-table","title":"DQ Detailed Stats Table","text":"<p>Library is responsible for auto generating two stats tables that provide per expectation/rule executaion status view. </p> <p>Tables in question are - <code>&lt;stats_table_name&gt;_detailed</code> - <code>&lt;stats_table_name&gt;_querydq_output</code></p> <p>This table provides detailed stats of all the expectations along with the status provided in the stats table in a relational format. This table need not be created. It gets auto created with \"_detailed \" to the dq stats table name. </p> <p>Warning</p> <p>Detailed Stats Tables are optional. It is auto created and named as stats table with suffix <code>_detailed</code>.</p> <p>Default Behaviour: Detailed Stats table is disabled. To enable it pass  <pre><code>user_config.se_enable_agg_dq_detailed_result: True,\n</code></pre></p>"},{"location":"user_guide/data_quality_metrics/#schema","title":"Schema","text":"<pre><code>create table if not exists `catalog`.`schema`.`&lt;stats_table_name&gt;_detailed` (\nrun_id string,  -- (1)!    \nproduct_id string,  -- (2)!  \ntable_name string,  -- (3)!  \nrule_type string,  -- (4)!  \nrule string,  -- (5)!\nsource_expectations string,  -- (6)!\ntag string,  -- (7)!\ndescription string,  -- (8)!\nsource_dq_status string,  -- (9)!\nsource_dq_actual_outcome string,  -- (10)!\nsource_dq_expected_outcome string,  -- (11)!\nsource_dq_actual_row_count string,  -- (12)!\nsource_dq_error_row_count string,  -- (13)!\nsource_dq_row_count string,  -- (14)!\nsource_dq_start_time string,  -- (15)!\nsource_dq_end_time string,  -- (16)!\ntarget_expectations string,  -- (17)!\ntarget_dq_status string,  -- (18)!\ntarget_dq_actual_outcome string,  -- (19)!\ntarget_dq_expected_outcome string,  -- (20)!\ntarget_dq_actual_row_count string,  -- (21)!\ntarget_dq_error_row_count string,  -- (22)!\ntarget_dq_row_count string,  -- (23)!\ntarget_dq_start_time string,  -- (24)!\ntarget_dq_end_time string,  -- (25)!\ndq_date date,  -- (26)!\ndq_time string,  -- (27)!\ndq_job_metadata_info string,  -- (28)!\n);\n</code></pre> <ol> <li><code>run_id</code> Run Id for a specific run </li> <li><code>product_id</code> Unique product identifier </li> <li><code>table_name</code> The target table where the final data gets inserted</li> <li><code>rule_type</code> Either row/query/agg dq</li> <li><code>rule</code>  Rule name</li> <li><code>source_expectations</code> Actual Rule to be executed on the source dq</li> <li><code>tag</code> completeness,uniqueness,validity,accuracy,consistency,</li> <li><code>description</code> Description of the Rule</li> <li><code>source_dq_status</code> Status of the rule execution in the Source dq</li> <li><code>source_dq_actual_outcome</code> Actual outcome of the Source dq check</li> <li><code>source_dq_expected_outcome</code> Expected outcome of the Source dq check</li> <li><code>source_dq_actual_row_count</code> Number of rows of the source dq</li> <li><code>source_dq_error_row_count</code> Number of rows failed in the source dq</li> <li><code>source_dq_row_count</code> Number of rows of the source dq</li> <li><code>source_dq_start_time</code> source dq start timestamp</li> <li><code>source_dq_end_time</code> source dq end timestamp</li> <li><code>target_expectations</code> Actual Rule to be executed on the target dq</li> <li><code>target_dq_status</code> Status of the rule execution in the Target dq</li> <li><code>target_dq_actual_outcome</code> Actual outcome of the Target dq check</li> <li><code>target_dq_expected_outcome</code> Expected outcome of the Target dq check</li> <li><code>target_dq_actual_row_count</code> Number of rows of the target dq</li> <li><code>target_dq_error_row_count</code> Number of rows failed in the target dq</li> <li><code>target_dq_row_count</code> Number of rows of the target dq</li> <li><code>target_dq_start_time</code> target dq start timestamp</li> <li><code>target_dq_end_time</code> target dq end timestamp</li> <li><code>dq_date</code> Dq executed date</li> <li><code>dq_time</code> Dq executed timestamp</li> <li><code>dq_job_metadata_info</code> dq job metadata</li> </ol>"},{"location":"user_guide/data_quality_metrics/#dq-query-output-table","title":"DQ Query Output Table","text":"<p>Warning</p> <p>DQ Query Output Table is optional. It is auto created and named as stats table with suffix <code>_querydq_output</code>.</p> <p>Name can be overriden by passing <code>querydq_output_custom_table_name</code></p> <p>Default Behaviour: Detailed Stats table is disabled.</p> <pre><code>user_config.querydq_output_custom_table_name: &lt;string&gt; \nuser_config.se_enable_query_dq_detailed_result: True &lt;-- Toggle switch if table should be enabled\n</code></pre>"},{"location":"user_guide/data_quality_metrics/#schema_1","title":"Schema","text":"<pre><code>create table if not exists `&lt;catalog&gt;`.`&lt;schema&gt;`.`&lt;stats_table_name&gt;_querydq_output` (\nrun_id string,  -- (1)!    \nproduct_id string,  -- (2)!  \ntable_name string,  -- (3)!  \nrule string,  -- (4)! \ncolumn_name string,  -- (5)!  \nalias string,  -- (6)!  \ndq_type string,  -- (7)!  \nsource_output string,  -- (8)!  \ntarget_output string,  -- (9)!  \ndq_time string,  -- (10)!  \n);\n</code></pre> <ol> <li><code>run_id</code> Run Id for a specific run </li> <li><code>product_id</code> Unique product identifier </li> <li><code>table_name</code> --</li> <li><code>rule</code>  Rule name</li> <li><code>column_name</code> column name</li> <li><code>alias</code> --</li> <li><code>dq_type</code> --</li> <li><code>source_output</code> --</li> <li><code>target_output</code> --</li> <li><code>dq_time</code> Dq executed timestamp</li> </ol>"},{"location":"user_guide/data_quality_rules/","title":"Data Quality Rules","text":"<p>When evaluating characteristic of data and it's quality we usually think of data quality dimensions. Spark-expectations is there to verify data integrity once data has already been ingested or in-flight before data lands in a datastore. Some of the data quality dimensions are </p> Dimension What It Means Business Quality Technical Quality <code>Completeness</code> Proportion of the data against the potential of 100% data Is this field required for business use and does it need to be populated fully to answer business questions? Is the data we received equal to the data we loaded in the target? <code>Validity</code> Measures of the existence, structure, content, and other basic characteristics of data Data is valid when it meets the formal and structural requirements of the business rule that the organization defines Validity is a measure of the correspondence between the formal aspects of the column data and format that the organization requires <code>Uniqueness</code> Extent to which all distinct values of a data element appear only once Is an individual data point recorded more than once? Is an individual data point recorded more than once? <code>Consistency</code> Absence of difference when comparing two or more representations against its definition Are the data within the data attribute the same as it moves across the ecosystem based on expectations? Are the record counts and/or summation of the measure of data attributes the same over time based on the defined threshold? <code>Timeliness</code> Degree to which data is available when it is required Is data relevant for the business use at the point in time? Did my data arrive on time? Are tables refreshing on time? <code>Accuracy</code> Degree to which data correctly describes the real-world object or event being described Does data describe the real-world environment it's trying to represent? Does data describe the real-world environment it's trying to represent? <p>Rule Examples</p>"},{"location":"user_guide/data_quality_rules/#rules-table","title":"Rules Table","text":"<p>For user to be able to run spark-expectation or define rules <code>Rules table</code> needs to exist and.  Format of a table needs to match. </p> <p>The below SQL statements used three namespaces which works with Databricks Unity Catalog, but if you are using hive please update the namespaces accordingly and also provide necessary table metadata.</p> <pre><code>create table if not exists `catalog`.`schema`.`{product}_rules` (\nproduct_id STRING,  -- (1)!\ntable_name STRING,  -- (2)!\nrule_type STRING,  -- (3)!\nrule STRING,  -- (4)!\ncolumn_name STRING,  -- (5)!\nexpectation STRING,  -- (6)!\naction_if_failed STRING,  -- (7)!\ntag STRING,  -- (8)!\ndescription STRING,  -- (9)!\nenable_for_source_dq_validation BOOLEAN,  -- (10)! \nenable_for_target_dq_validation BOOLEAN,  -- (11)!\nis_active BOOLEAN,  -- (12)!\nenable_error_drop_alert BOOLEAN,  -- (13)!\nerror_drop_threshold INT,  -- (14)!\nquery_dq_delimiter STRING,  -- (15)!\nenable_querydq_custom_output BOOLEAN,  -- (16)!\npriority STRING DEFAULT \"medium\", -- (17)!\n);\n</code></pre> <ol> <li><code>product_id</code> A unique name at the level of dq rules execution</li> <li><code>table_name</code> The table for which the rule is being defined for</li> <li><code>rule_type</code> 3 different type of rules. They are 'row_dq', 'agg_dq' and 'query_dq'</li> <li><code>rule</code> Short description of the rule </li> <li><code>column_name</code> The column name for which the rule is defined for. This only applies for row_dq. For agg_dq and query_dq, use blank/empty value. </li> <li><code>expectation</code> Provide the DQ rule condition </li> <li><code>action_if_failed</code> There are 3 different types of actions. These are 'ignore', 'drop', and 'fail'.      Ignore: The rule is run and the output is logged. No action is performed regardless of whether the rule has succeeded or failed. Applies for all 3 rule types.      Drop: The rows that fail the rule get dropped from the dataset. Applies for only row_dq rule type.     Fail: job fails if the rule fails. Applies for all 3 rule types.</li> <li><code>tag</code> provide some tag name to dq rule example:  completeness, validity, uniqueness etc. </li> <li><code>description</code>  Long description for the rule</li> <li><code>enable_for_source_dq_validation</code> flag to run the agg rule</li> <li><code>enable_for_target_dq_validation</code> flag to run the query rule</li> <li><code>is_active</code> true or false to indicate if the rule is active or not. </li> <li><code>enable_error_drop_alert</code> true or false. This determines if an alert notification should be sent out if row(s) is(are) dropped from the data set</li> <li><code>error_drop_threshold</code> Threshold for the alert notification that gets triggered when row(s) is(are) dropped from the data set</li> <li><code>query_dq_delimiter</code> segregate custom queries delimiter ex: $, @ etc. By default it is @. Users can override it with any other delimiter based on the need. The same delimiter mentioned here has to be used in the custom query.</li> <li><code>enable_querydq_custom_output</code> required custom query output in separate table</li> <li><code>priority</code> Priority level for the rule. Supported values are: 'low', 'medium' and 'high'.</li> </ol> <p>The Spark Expectation process consists of three phases:</p> <ol> <li>When enable_for_source_dq_validation is true, execute agg_dq and query_dq on the source Dataframe</li> <li>If the first step is successful, proceed to run row_dq</li> <li>When enable_for_target_dq_validation is true, execute agg_dq and query_dq on the Dataframe resulting from row_dq</li> </ol>"},{"location":"user_guide/data_quality_rules/#action-if-failed-configuration-for-data-quality-rules","title":"Action If Failed Configuration For Data Quality Rules","text":"<p>The rules column has a column called <code>action_if_failed</code>. It is important that this column should only accept one of  these values:</p> <ul> <li><code>[fail, drop or ignore]</code> for <code>'rule_type'='row_dq'</code> and </li> <li><code>[fail, ignore]</code> for <code>'rule_type'='agg_dq' and 'rule_type'='query_dq'</code>. </li> </ul> <p>If other values are provided, the library may cause unforeseen errors. Please run the below command to add constraints to the above created rules table</p> <pre><code>ALTER TABLE apla_nd_dq_rules ADD CONSTRAINT action CHECK ((rule_type = 'row_dq' and action_if_failed IN ('ignore', 'drop', 'fail')) or (rule_type = 'agg_dq' and action_if_failed in ('ignore', 'fail')) or (rule_type = 'query_dq' and action_if_failed in ('ignore', 'fail'))) and priority IN ('low', 'medium', 'high');\n</code></pre>"},{"location":"user_guide/data_quality_rules/#rule-types","title":"Rule Types","text":"Rule Type Valid Expectation Format Aggregate Functions Allowed SQL Queries Allowed <code>row_dq</code> Simple row expressions \u274c No \u274c No <code>agg_dq</code> Aggregate expressions \u2705 Yes \u274c No <code>query_dq</code> SQL queries \u2705 Yes (via SQL) \u2705 Yes <p>Tip: - Match your expectation format to the rule type for correct validation. - Use <code>row_dq</code> for per-row checks, <code>agg_dq</code> for summary statistics, and <code>query_dq</code> for advanced SQL-based checks.</p> <p>Below are the details and examples for each rule type:</p>"},{"location":"user_guide/data_quality_rules/#1-row-level-data-quality-row_dq","title":"1. Row-Level Data Quality (<code>row_dq</code>)","text":"<p>Purpose: Checks conditions on individual rows, without using aggregate functions or SQL queries.</p> <p>Valid Expectation Examples: - <code>col1 &gt; 10</code> - <code>col2 &lt; 25</code> - <code>col1 is null</code> - <code>col1 is not null</code> - <code>(col3 % 2) = 0</code></p> <p>Characteristics: - Operates on each row independently. - Aggregate functions are NOT allowed (e.g., <code>sum</code>, <code>avg</code>, <code>min</code>, <code>max</code>). - SQL queries are NOT allowed.</p>"},{"location":"user_guide/data_quality_rules/#2-aggregate-data-quality-agg_dq","title":"2. Aggregate Data Quality (<code>agg_dq</code>)","text":"<p>Purpose: Checks conditions on aggregated values computed from the DataFrame.</p> <p>Valid Expectation Examples: - <code>sum(col3) &gt; 20</code> - <code>avg(col3) &gt; 25</code> - <code>min(col1) &gt; 10</code> - <code>stddev(col3) &gt; 10</code> - <code>count(distinct col2) &gt; 4</code> - <code>avg(col1) &gt; 4</code> - <code>avg(col3) &gt; 18 and avg(col3) &lt; 25</code> - <code>avg(col3) between 18 and 25</code> - <code>count(*) &gt; 5</code> (Note: support may depend on your Spark version)</p> <p>Characteristics: - Aggregate functions are required (e.g., <code>sum</code>, <code>avg</code>, <code>min</code>, <code>max</code>, <code>count</code>, <code>stddev</code>). - Operates on the entire DataFrame or groups of rows. - SQL queries are NOT allowed.</p>"},{"location":"user_guide/data_quality_rules/#3-query-based-data-quality-query_dq","title":"3. Query-Based Data Quality (<code>query_dq</code>)","text":"<p>Purpose: Checks conditions using full SQL queries, typically for more complex or cross-table checks.</p> <p>Valid Expectation Examples: - <code>(select sum(col1) from test_table) &gt; 10</code> - <code>(select stddev(col3) from test_table) &gt; 0</code> - <code>(select max(col1) from test_final_table_view) &gt; 10</code> - <code>(select min(col3) from test_final_table_view) &gt; 0</code> - <code>(select count(col1) from test_final_table_view) &gt; 3</code> - <code>(select count(case when col3&gt;0 then 1 else 0 end) from test_final_table_view) &gt; 10</code> - <code>(select sum(col1) from {table}) &gt; 10</code> - <code>(select count(*) from test_table) &gt; 10</code></p> <p>Characteristics: - Must be a valid SQL query, typically starting with <code>select ... from ...</code>. - Can reference other tables or views. - Supports complex logic, including subqueries and case statements.</p>"},{"location":"user_guide/data_quality_rules/#examples-of-supported-rule-types","title":"Examples of Supported Rule Types","text":"<ul> <li> <p>Row DQ:  <pre><code>INSERT INTO `catalog`.`schema`.`{product}_rules` VALUES ('your_product', 'your_table', 'row_dq', 'check_nulls', 'column_name', 'is not null', 'drop', 'completeness', 'Check for null values in column_name', true, true, true, false, 0, \"medium\");\n</code></pre></p> </li> <li> <p>Aggregation DQ:  <pre><code>INSERT INTO `catalog`.`schema`.`{product}_rules` VALUES ('your_product', 'your_table', 'agg_dq', 'check_row_count', '', 'COUNT(*) &gt; 0', 'fail', 'completeness', 'Ensure the table has at least one row', true, true, true, false, 0, \"medium\");\n</code></pre></p> </li> <li> <p>Query DQ: <pre><code>INSERT INTO `catalog`.`schema`.`{product}_rules` VALUES ('your_product', 'your_table', 'query_dq', 'check_custom_query', '', 'SELECT COUNT(*) FROM your_table WHERE column_name IS NULL', 'ignore', 'validity', 'Custom query to check for null values in column_name', false, true, true, false, 0, \"medium\");\n</code></pre></p> </li> </ul>"},{"location":"user_guide/getting_started/","title":"Getting Started with Spark-Expectations","text":"<p>This guide will help you set up your environment, install the library, and understand the basic requirements for using Spark-Expectations in your data workflows.</p>"},{"location":"user_guide/getting_started/#prerequisites","title":"Prerequisites","text":""},{"location":"user_guide/getting_started/#python","title":"Python","text":"<ul> <li>Supported versions: 3.9, 3.10, 3.11, 3.12 (recommended: latest 3.12.x)</li> </ul>"},{"location":"user_guide/getting_started/#java","title":"Java","text":"<ul> <li>Supported versions: 8, 11, 17 </li> </ul>"},{"location":"user_guide/getting_started/#installation","title":"Installation","text":"<p>You can install Spark-Expectations directly from PyPI:</p> <pre><code>pip install -U spark-expectations\n</code></pre> <p>Or add it to your <code>requirements.txt</code> or dependency manager (e.g., poetry, hatch, uv).</p> <p>Ready to get started? Head to the Quickstart for setting up hello world SparkExpectation job!</p>"},{"location":"user_guide/quickstart/","title":"Quick Start","text":"<p>To successfully run spark-expectations user needs to create <code>Rules</code> table as a first step. </p>"},{"location":"user_guide/quickstart/#required-tables","title":"Required Tables","text":"<p>Spark expectation expects that Rules table is created for spark-expectations to run seamlessly and integrate with a spark job.</p>"},{"location":"user_guide/quickstart/#rules-table","title":"Rules Table","text":"<p>The below SQL statements used three namespaces which works with Databricks Unity Catalog, but if you are using hive please update the namespaces accordingly and also provide necessary table metadata.</p> <p>We need to create a rules tables which contains all the data quality rules. Please use the below template to create your rules table for your project.</p> <pre><code>create table if not exists `catalog`.`schema`.`{product}_rules` (\nproduct_id STRING,  -- (1)!\ntable_name STRING,  -- (2)!\nrule_type STRING,  -- (3)!\nrule STRING,  -- (4)!\ncolumn_name STRING,  -- (5)!\nexpectation STRING,  -- (6)!\naction_if_failed STRING,  -- (7)!\ntag STRING,  -- (8)!\ndescription STRING,  -- (9)!\nenable_for_source_dq_validation BOOLEAN,  -- (10)! \nenable_for_target_dq_validation BOOLEAN,  -- (11)!\nis_active BOOLEAN,  -- (12)!\nenable_error_drop_alert BOOLEAN,  -- (13)!\nerror_drop_threshold INT,  -- (14)!\nquery_dq_delimiter STRING,  -- (15)!\nenable_querydq_custom_output BOOLEAN,  -- (16)!\npriority STRING DEFAULT \"medium\", -- (17)!\n);\n</code></pre> <ol> <li><code>product_id</code> A unique name at the level of dq rules execution</li> <li><code>table_name</code> The table for which the rule is being defined for</li> <li><code>rule_type</code> 3 different type of rules. They are 'row_dq', 'agg_dq' and 'query_dq'</li> <li><code>rule</code> Short description of the rule </li> <li><code>column_name</code> The column name for which the rule is defined for. This only applies for row_dq. For agg_dq and query_dq, use blank/empty value. </li> <li><code>expectation</code> Provide the DQ rule condition </li> <li><code>action_if_failed</code> There are 3 different types of actions. These are 'ignore', 'drop', and 'fail'.      Ignore: The rule is run and the output is logged. No action is performed regardless of whether the rule has succeeded or failed. Applies for all 3 rule types.      Drop: The rows that fail the rule get dropped from the dataset. Applies for only row_dq rule type.     Fail: job fails if the rule fails. Applies for all 3 rule types.</li> <li><code>tag</code> provide some tag name to dq rule example:  completeness, validity, uniqueness etc. </li> <li><code>description</code>  Long description for the rule</li> <li><code>enable_for_source_dq_validation</code> flag to run the agg rule</li> <li><code>enable_for_target_dq_validation</code> flag to run the query rule</li> <li><code>is_active</code> true or false to indicate if the rule is active or not. </li> <li><code>enable_error_drop_alert</code> true or false. This determines if an alert notification should be sent out if row(s) is(are) dropped from the data set</li> <li><code>error_drop_threshold</code> Threshold for the alert notification that gets triggered when row(s) is(are) dropped from the data set</li> <li><code>query_dq_delimiter</code> segregate custom queries delimiter ex: $, @ etc. By default it is @. Users can override it with any other delimiter based on the need. The same delimiter mentioned here has to be used in the custom query.</li> <li><code>enable_querydq_custom_output</code> required custom query output in separate table</li> <li><code>priority</code> Priority level for the rule. Supported values are: 'low', 'medium' and 'high'.</li> </ol> <p>The Spark Expectation process consists of three phases: 1. When enable_for_source_dq_validation is true, execute agg_dq and query_dq on the source Dataframe 2. If the first step is successful, proceed to run row_dq 3. When enable_for_target_dq_validation is true, execute agg_dq and query_dq on the Dataframe resulting from row_dq</p>"},{"location":"user_guide/quickstart/#rule-type","title":"Rule Type","text":"<p>The rules column has a column called \"rule_type\". It is important that this column should only accept one of  these three values - <code>[row_dq, agg_dq, query_dq]</code>. If other values are provided, the library may cause unforeseen errors. Please run the below command to add constraints to the above created rules table</p> <pre><code>ALTER TABLE `catalog`.`schema`.`{product}_rules` ADD CONSTRAINT rule_type_action CHECK (rule_type in ('row_dq', 'agg_dq', 'query_dq'));\n</code></pre> <p>For further information about creating individual rules please refer to the Rules Guide</p>"},{"location":"user_guide/quickstart/#initiating-spark-expectation","title":"Initiating Spark Expectation","text":""},{"location":"user_guide/quickstart/#sample-input-data","title":"Sample input data","text":"<pre><code>data = [\n{\"id\": 1, \"age\": 25,   \"email\": \"alice@example.com\"},\n{\"id\": 2, \"age\": 17,   \"email\": \"bob@example.com\"},\n{\"id\": 3, \"age\": None, \"email\": \"charlie@example.com\"},\n{\"id\": 4, \"age\": 40,   \"email\": \"bob@example.com\"},\n{\"id\": 5, \"age\": None, \"email\": \"ron@example.com\"},\n{\"id\": 6, \"age\": 41,   \"email\": None},\n]\ninput_df = spark.createDataFrame(pd.DataFrame(data))\ninput_df.show(truncate=False)\n</code></pre>"},{"location":"user_guide/quickstart/#insert-expectations-into-rules-table","title":"Insert expectations into Rules table","text":"<pre><code># Name of the rules table previously created \nrules_table = f\"{catalog}.{schema}.{product}_rules\"\nproduct_identifier = \"test_product\"\nrules_data = [\n{\n\"product_id\": product_identifier,\n\"table_name\": f\"{catalog}.{schema}.{target_table_name}\",\n\"rule_type\": \"row_dq\",\n\"rule\": \"age_not_null\",\n\"column_name\": \"age\",\n\"expectation\": \"age IS NOT NULL\",\n\"action_if_failed\": \"drop\",\n\"tag\": \"completeness\",\n\"description\": \"Age must not be null\",\n\"enable_for_source_dq_validation\": True,\n\"enable_for_target_dq_validation\": True,\n\"is_active\": True,\n\"enable_error_drop_alert\": False,\n\"error_drop_threshold\": 0,\n\"priority\": \"medium\",\n}\n]\nimport pandas as pd\nrules_df = spark.createDataFrame(pd.DataFrame(rules_data))\nrules_df.show(truncate=True)\nrules_df.write.mode(\"overwrite\").saveAsTable(rules_table)\n</code></pre>"},{"location":"user_guide/quickstart/#streaming-and-user-config","title":"Streaming and User config","text":"<p>Following example let's spark-expectation use default configuration. </p> <p>Only configuration we are passing is to disable streaming option</p> <pre><code>from spark_expectations.config.user_config import Constants as user_config\nstats_streaming_config_dict = {\nuser_config.se_enable_streaming: False\n}\nuser_config = {}\n</code></pre>"},{"location":"user_guide/quickstart/#run-sparkexpectations-job","title":"Run SparkExpectations job","text":"<p>Please reference Spark Expectation notebooks for fully functioning examples. </p> <p>Note</p> <p>Spark-Expectation repository itself provides docker compose yaml file for running those notebooks.</p> <pre><code>    # Generate self signed certs for mailpit server\nmake generate-mailserver-certs # running following makefile target will spin up spark, jupyter, mailpit and kafka service\nmake local-se-server-start ARGS=\"--build\"` </code></pre> <pre><code>from pyspark.sql import DataFrame\nfrom spark_expectations.core.expectations import (\nSparkExpectations, WrappedDataFrameWriter\n)\nfrom spark_expectations.core import load_configurations\n# Initialize Default Config: in a future release this will not be required \nload_configurations(spark) \nwriter = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\nse = SparkExpectations(\nproduct_id=f\"{product_identifier}\",                   # (1)!\nrules_df=rules_dataframe,                             # (2)!\nstats_table=f\"{catalog}.{schema}.{stats_table_name}\", # (3)!\nstats_table_writer=writer,                            # (4)!\ntarget_and_error_table_writer=writer,                 # (5)!\nstats_streaming_options=stats_streaming_config_dict   # (6)!\n)\n\"\"\"\nThis decorator helps to wrap a function which returns dataframe and apply dataframe rules on it\nArgs:\n    target_table: Name of the table where the final dataframe need to be written\n    write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n    write_to_temp_table: Mark it as \"True\" if the input dataframe need to be written to the temp table to break\n                        the spark plan\n    user_conf: Provide options to override the defaults, while writing into the stats streaming table\n    target_table_view: This view is created after the _row_dq process to run the target agg_dq and query_dq.\n        If value is not provided, defaulted to {target_table}_view\n    target_and_error_table_writer: Provide the writer to write the target and error table,\n        this will take precedence over the class level writer\nReturns:\n    Any: Returns a function which applied the expectations on dataset\n\"\"\"\n@se.with_expectations(\ntarget_table=f\"{catalog}.{schema}.{target_table_name}\",\nwrite_to_table=True,\nwrite_to_temp_table=True,\nuser_conf=user_config,\n)\ndef get_dataset():\n_df_source: DataFrame = input_df\n_df_source.createOrReplaceTempView(\"in_memory_data_source\")\nreturn _df_source\n# This will run the DQ checks and raise if any \"fail\" rules are violated\nget_dataset()\n</code></pre> <ol> <li>The unique product identifier for DQ execution.</li> <li>DataFrame containing the rules to apply.</li> <li>Name of the stats table for logging results.If it doesn't exist it will be generated</li> <li>Writer object for the stats table.</li> <li>Writer object for target and error tables.</li> <li>Dictionary of streaming options for stats.</li> </ol>"},{"location":"user_guide/user_config/","title":"User Config","text":"<p>Please see this page examples page</p>"},{"location":"user_guide/notifications/email_notifications/","title":"Email","text":"<p>By default email notifications are disabled. To use them we need to pass required user configuration for spark-expectation to properly run.</p>"},{"location":"user_guide/notifications/email_notifications/#notification-config-parameters","title":"Notification Config Parameters","text":"<p>user_config.se_notifications_enable_email</p> <p>Master toggle to Enable Email Notifications</p> Notification triggers <p>These parameters control when email notifications are sent during Spark-Expectations runs. <code>Hover over each parameter to see a short description.</code></p> <ul> <li>user_config.se_notifications_enable_email</li> <li>user_config.se_notifications_on_start</li> <li>user_config.se_notifications_on_completion</li> <li>user_config.se_notifications_on_fail</li> <li>user_config.se_notifications_on_error_drop_exceeds_threshold_breach</li> <li>user_config.se_notifications_on_rules_action_if_failed_set_ignore</li> <li>user_config.se_notifications_on_error_drop_threshold</li> </ul> SMTP Config <p>These parameters control what email server to use for sending the notifications. <code>Hover over each parameter to see a short description.</code></p> <ul> <li>user_config.se_notifications_email_smtp_host</li> <li>user_config.se_notifications_email_smtp_port</li> </ul> Email Content <p>These parameters control how email content is going to look like. <code>Hover over each parameter to see a short description.</code> </p> <ul> <li>user_config.se_notifications_email_from</li> <li>user_config.se_notifications_email_to_other_mail_id</li> <li>user_config.se_notifications_email_subject</li> <li>user_config.se_notifications_email_custom_body</li> </ul> Email Templates <p>These parameters configure usage of jinja2 templates </p> <ul> <li>user_config.se_notifications_enable_templated_basic_email_body</li> <li>user_config.se_notifications_default_basic_email_template</li> </ul>"},{"location":"user_guide/notifications/email_notifications/#configure-smtp-notifications","title":"Configure SMTP Notifications","text":"<p>To enable email notifications (such as alerts for data quality failures) in Spark-Expectations, you need to configure SMTP settings.  You can reference the <code>user_config.py</code> file in the <code>spark_expectations/config</code> directory to access / setup the SMTP configuration parameters. This file should contain the necessary SMTP/email notification settings for Spark-Expectations.</p>"},{"location":"user_guide/notifications/email_notifications/#verifying-smtp-parameters","title":"Verifying SMTP Parameters","text":"<p>Before using SMTP notifications, verify that the following parameters are set correctly in your configuration (see <code>user_config.py</code> for the exact constant names):</p> <ul> <li>SMTP server host</li> <li>SMTP server port</li> <li>SMTP username</li> <li>SMTP password</li> <li>Sender email address (<code>from</code>)</li> <li>Recipient email address(es) (<code>to</code>)</li> <li>Enable/disable SMTP authentication and TLS as needed</li> </ul> Sample Python Script to Send a Test Email <p>You can use the following Python script to test your SMTP configuration. This script will send a test email using the configured SMTP settings. Make sure to replace the placeholders with your actual SMTP configuration values. <pre><code>from email.mime.text import MIMEText\nsmtp_host = \"smtp.example.com\"\nsmtp_port = 587\nsmtp_user = \"your_email@example.com\"\nsmtp_password = \"your_password\"\nsmtp_from = \"your_email@example.com\"\nsmtp_to = \"recipient@example.com\"\nmsg = MIMEText(\"This is a test email from Spark-Expectations SMTP setup.\")\nmsg[\"Subject\"] = \"Spark-Expectations SMTP Test\"\nmsg[\"From\"] = smtp_from\nmsg[\"To\"] = smtp_to\nwith smtplib.SMTP(smtp_host, smtp_port) as server:\nserver.starttls()\nserver.login(smtp_user, smtp_password)\nserver.sendmail(smtp_from, [smtp_to], msg.as_string())\nprint(\"Test email sent successfully.\")\n</code></pre></p> <p>Note: Never commit sensitive credentials (like SMTP passwords) to version control. Use environment variables or a secure secrets manager. Make sure your SMTP server allows connections from your environment (some providers may require app passwords or special settings).</p>"},{"location":"user_guide/notifications/email_notifications/#user-configuration-example","title":"User Configuration Example","text":"Show example user configuration <pre><code>user_conf_dict = {\n# Master Toggle\nuser_config.se_notifications_enable_email: True,\n# Notification triggers\nuser_config.se_notifications_on_start: True,\nuser_config.se_notifications_on_completion: True,\nuser_config.se_notifications_on_fail: True,\nuser_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\nuser_config.se_notifications_on_rules_action_if_failed_set_ignore: True,\nuser_config.se_notifications_on_error_drop_threshold: 15,\n# Email notification server config\nuser_config.se_notifications_email_smtp_host: \"mailpit\",\nuser_config.se_notifications_email_smtp_port: 1025,\n# Email headers (sender, receiver, subject)\nuser_config.se_notifications_email_from: \"sender@example.com\",\nuser_config.se_notifications_email_to_other_mail_id: \"receiver@example.com\",\nuser_config.se_notifications_email_subject: \"Test Subject\",\n# Email content\nuser_config.se_notifications_email_custom_body: \"This is a custom email body from test.\",\n# Email Templates\nuser_config.se_notifications_enable_templated_basic_email_body: True,\n# If jinjia template is not provided it will use default one\nuser_config.se_notifications_default_basic_email_template: custom_html_email_template\n}\n</code></pre>"},{"location":"user_guide/notifications/email_notifications/#template-example","title":"Template Example","text":"Show custom template example <pre><code>custom_html_email_template = \"\"\"\n&lt;style&gt;\n    table {\n        border-collapse: collapse;\n        width: 60%;\n        font-family: Arial, sans-serif;\n        border: 2px solid black;\n    }\n    td {\n        border: 1px solid black;\n        text-align: left;\n        padding: 4px;\n        background-color: #00ff1a;\n    }\n&lt;/style&gt;\n{% macro render_table(rows) %}\n&lt;table border=1&gt;\n    &lt;tbody&gt;\n        {% for row in rows %}\n            &lt;tr&gt;\n                {% for cell in row %}\n                        &lt;td&gt;{{ cell }}&lt;/td&gt;\n                {% endfor %}\n            &lt;/tr&gt;\n        {% endfor %}\n    &lt;/tbody&gt;\n&lt;/table&gt;\n{% endmacro %}\n&lt;h3&gt;{{ title }}&lt;/h3&gt;\n{{ render_table(rows) }}\n\"\"\"\n</code></pre>"},{"location":"user_guide/notifications/pagerduty_notifications/","title":"PagerDuty","text":"<p>spark-expectations relies on the PagerDuty Events API V2 to create new incidents. An existing PagerDuty service needs to be created before incidents can be created from spark-expectations.</p>"},{"location":"user_guide/notifications/pagerduty_notifications/#pre-requisites","title":"Pre-requisites","text":"<p>By default PagerDuty notifications (or the ability to create incidents) are disabled. To use them we need to pass the required user configurations for spark-expectations to properly run. </p>"},{"location":"user_guide/notifications/pagerduty_notifications/#notification-config-parameters","title":"Notification Config Parameters","text":"<p>user_config.se_notifications_enable_pagerduty</p> <p>Master toggle to enable PagerDuty notifications (this will create incidents for your service!)</p> Notification triggers <p>These parameters control when notifications are sent during Spark-Expectations runs. This would create a new incident per enabled trigger. <code>Hover over each parameter to see a short description.</code></p> <ul> <li>user_config.se_notifications_enable_pagerduty</li> <li>user_config.se_notifications_on_start</li> <li>user_config.se_notifications_on_completion</li> <li>user_config.se_notifications_on_fail</li> <li>user_config.se_notifications_on_error_drop_exceeds_threshold_breach</li> <li>user_config.se_notifications_on_rules_action_if_failed_set_ignore</li> <li>user_config.se_notifications_on_error_drop_threshold</li> </ul> PagerDuty Configs <p>Additional configurations that are needed to be able to create incidents with spark-expectations. <code>Hover over each parameter to see a short description.</code></p> <ul> <li>user_config.se_notifications_pagerduty_integration_key</li> <li>user_config.se_notifications_pagerduty_webhook_url</li> </ul>"},{"location":"user_guide/notifications/pagerduty_notifications/#user-configuration-example","title":"User Configuration Example","text":"Show example user configuration <pre><code>user_conf_dict = {\n# Master Toggle\nuser_config.se_notifications_enable_pagerduty: True,\n# PagerDuty Configuration\nuser_config.se_notifications_pagerduty_integration_key: &lt;enter_integration_key_here&gt;,\nuser_config.se_notifications_pagerduty_webhook_url: \"https://events.pagerduty.com/v2/enqueue\",\n}\n</code></pre>"},{"location":"user_guide/notifications/pagerduty_notifications/#links-to-example-notebooks","title":"Links to example notebooks","text":"<p>An example notebook is available to use that sets up PD in a notebook here.</p> <p>This notebook will:</p> <ul> <li>Grab integration key using databricks secret manager (default)<ul> <li>An option to use Cerberus Secrets Manager is present but commented out. Uncomment if you would to use this method instead.</li> </ul> </li> <li>Configure spark-expectations</li> <li>Load sample data and then run some validations rules afterwards.</li> </ul> <p>If everything has been configured correctly, this will create a new incident based on the triggers you have enabled. </p>"},{"location":"user_guide/notifications/slack_notifications/","title":"Slack","text":"<p>TBD</p>"}]}