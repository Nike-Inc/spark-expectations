{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5d8d62-d0cd-415b-81cd-2e16f21977ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark - Expectations - User - Guide - Documentation\n",
    "* Please read through the [Spark Expectation Documentation](https://engineering.nike.com/spark-expectations) before proceeding with this demo\n",
    "\n",
    "#### widgets \n",
    "* `catalog`, `schema` - define where output tables are going to be created \n",
    "  * Tables are going to be prefixed with logged in DBX user\n",
    "* `library_source` combo box defines library url(git branch or pypi) from where to pull library \n",
    "  * `pypy` ( installs latest published version available in PyPy)\n",
    "  * `git` ( installs library from specified git branch)\n",
    "    * Set `git_branch` input field to match git branch (example `main`)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b941ee25-ba9e-42e4-8b77-c436b903fff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialize notebook config\n",
    "This step will read widget values needed to explain from where to install Spark-Expectation library as well as define schema, catalog and table naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd581dad-8a64-47ed-a6cd-1307942283a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "logged_in_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "logged_in_user = logged_in_user.split('@')[0]\n",
    "\n",
    "user = re.sub(r'[^a-zA-Z]', '', logged_in_user).lower()\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "library = dbutils.widgets.get(\"library_source\")\n",
    "org = dbutils.widgets.get(\"git_org\")\n",
    "branch_or_commit = dbutils.widgets.get(\"git_branch_or_commit\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": catalog,\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_product\",\n",
    "    \"in_memory_source\": f\"se_{user}_source\",\n",
    "    \"rules_table\": f\"{catalog}.{schema}.se_{user}_rules\",\n",
    "    \"stats_table\": f\"{catalog}.{schema}.se_{user}_stats\",\n",
    "    \"target_table\": f\"{catalog}.{schema}.se_{user}_target\",\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9a56e1-267d-4bf1-80de-5646a4141bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install - Required - Libraries\n",
    "* Spark Expectations\n",
    "* Jinja2 (required for using custom templates) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4f35e3-40ed-42e1-b21f-def67bcedca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if CONFIG[\"library\"] == \"pypi\":\n",
    "  print(\"-----INSTALLING SPARK-EXPECTATIONS from PyPi\")\n",
    "  %pip install spark-expectations\n",
    "elif CONFIG[\"library\"] == \"git\":\n",
    "  print(f\"-----INSTALLING SPARK-EXPECTATIONS from Git Org/User {CONFIG['org']}, Branch/Commit {CONFIG['branch_or_commit']}\")\n",
    "  giturl = f\"git+https://github.com/{CONFIG['org']}/spark-expectations.git@{CONFIG['branch_or_commit']}\"\n",
    "  %pip install --force-reinstall {giturl}\n",
    "\n",
    "print(\"-----INSTALLING Jinja2 template library\")\n",
    "%pip install jinja2 \n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58abd6b8-28e4-441a-8aca-7c17951609ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cleanup \n",
    "- Removing previosly created user prefixed tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d59e28d-1c5b-460a-b3d2-5bee86e58e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE CATALOG {CONFIG['catalog']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"] ]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"----- No tables to drop\")\n",
    "\n",
    "# Drop global and local temp views matching pattern\n",
    "\n",
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"----- No views to drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db150bc8-9680-456b-84de-940fa786042f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run Spark Expectation\n",
    "\n",
    "Steps:\n",
    "1. Create rules dataframe\n",
    "\n",
    "2. Provide custom SparkExpectation config that overwrites default config\n",
    "- stats_streaming_options, \n",
    "- user_conf \n",
    "\n",
    "3. Initialize sparkExpectation\n",
    "- loading input dataset\n",
    "- running sparkExpectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4053bf7-1147-4f96-940c-ed4406e4d893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting Started with Spark Expectations: Simple Example\n",
    "\n",
    "## 1. Sample Source Dataset\n",
    "# initialize simple Pandas DataFrame and convert it to a Spark DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Default Configuration  (not needed when running in Databricks notebook)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "## 2. Define Simple `row_dq` Rules\n",
    "# Create a rules DataFrame with a few simple data quality rules\n",
    "\n",
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_not_null\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age IS NOT NULL\",\n",
    "        \"action_if_failed\": \"drop\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Age must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_adult\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age < 20\",\n",
    "        \"action_if_failed\": \"ignore\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Age must be at less than 20\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    },\n",
    "        {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"email_not_null\",\n",
    "        \"column_name\": \"email\",\n",
    "        \"expectation\": \"email IS NOT NULL\",\n",
    "        \"action_if_failed\": \"warn\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Email must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    }\n",
    "\n",
    "    \n",
    "]\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "rules_df.show(truncate=True)\n",
    "rules_df.write.mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "\n",
    "display(rules_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8586a449-5968-4935-bff5-195adcc589f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 3. Run Spark Expectations\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from spark_expectations.core import load_configurations\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "\n",
    "\n",
    "writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
    "\n",
    "\n",
    "# Initialize Default Config \n",
    "load_configurations(spark) \n",
    "# Custom config (example enable slack/email notifications)\n",
    "stats_streaming_config_dict = {user_config.se_enable_streaming: False}\n",
    "notification_conf = {}\n",
    "\n",
    "\"\"\"\n",
    "This class implements/supports running the data quality rules on a dataframe returned by a function\n",
    "\n",
    "Args:\n",
    "    product_id: Name of the product\n",
    "    rules_df: DataFrame which contains the rules. User is responsible for reading\n",
    "        the rules_table in which ever system it is\n",
    "    stats_table: Name of the table where the stats/audit-info need to be written\n",
    "    debugger: Mark it as \"True\" if the debugger mode need to be enabled, by default is False\n",
    "    stats_streaming_options: Provide options to override the defaults, while writing into the stats streaming table\n",
    "\"\"\"\n",
    "se = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "#  Initialize input data\n",
    "data = [\n",
    "    {\"id\": 1, \"age\": 19,   \"email\": \"alice@example.com\"},\n",
    "    {\"id\": 2, \"age\": 17,   \"email\": \"bob@example.com\"},\n",
    "    {\"id\": 3, \"age\": None, \"email\": \"charlie@example.com\"},\n",
    "    {\"id\": 4, \"age\": 40,   \"email\": \"mike@example.com\"},\n",
    "    {\"id\": 5, \"age\": None, \"email\": \"ron@example.com\"},\n",
    "    {\"id\": 6, \"age\": 35,   \"email\": None},\n",
    "]\n",
    "input_df = spark.createDataFrame(pd.DataFrame(data))\n",
    "input_df.show()\n",
    "\n",
    "\"\"\"\n",
    "This decorator helps to wrap a function which returns dataframe and apply dataframe rules on it\n",
    "\n",
    "Args:\n",
    "    target_table: Name of the table where the final dataframe need to be written\n",
    "    write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n",
    "    write_to_temp_table: Mark it as \"True\" if the input dataframe need to be written to the temp table to break\n",
    "                        the spark plan\n",
    "    user_conf: Provide options to override the defaults, while writing into the stats streaming table\n",
    "    target_table_view: This view is created after the _row_dq process to run the target agg_dq and query_dq.\n",
    "        If value is not provided, defaulted to {target_table}_view\n",
    "    target_and_error_table_writer: Provide the writer to write the target and error table,\n",
    "        this will take precedence over the class level writer\n",
    "\n",
    "Returns:\n",
    "    Any: Returns a function which applied the expectations on dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@se.with_expectations(\n",
    "    target_table=CONFIG[\"target_table\"],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=True,\n",
    "    user_conf=notification_conf,\n",
    ")\n",
    "def get_dataset():\n",
    "    _df_source: DataFrame = input_df\n",
    "    _df_source.createOrReplaceTempView(CONFIG[\"in_memory_source\"])\n",
    "    return _df_source\n",
    "\n",
    "\n",
    "# This will run the DQ checks and raise if any \"fail\" rules are violated\n",
    "get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1385e995-e308-4105-a93f-bdc3c765f351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "Checkout `Stats` and `Error` table to see spark-expectations execution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af4ced6-5263-4ce4-a452-d6b528980f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_stats_table = f\"SELECT * FROM {CONFIG['stats_table']}\"\n",
    "display(spark.sql(query_stats_table))\n",
    "\n",
    "query_error_table = f\"SELECT * FROM {CONFIG['target_table']}_error\"\n",
    "display(spark.sql(query_error_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be6c9367-24fb-49c8-a4e4-cae472656e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Display Target Table\n",
    "Based on the provided rules if `action_if_failed` is set to `drop` target table will remove any rows not passing provided check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7f0512-1ea3-436d-9dfb-ff3b035fb7f8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754329174097}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_target_table = f\"\"\"\n",
    "SELECT *\n",
    "FROM {CONFIG['target_table']} \n",
    "ORDER BY meta_dq_run_id, id\n",
    "\"\"\"\n",
    "\n",
    "final_data_set_df = spark.sql(query_target_table)\n",
    "\n",
    "if final_data_set_df is not None:\n",
    "    display(final_data_set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53aa1ec9-23b3-474d-b4e3-bf9479958b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Display Removed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4be278-1535-415d-871e-d4c90b69d8ce",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"table\":166},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754330258650}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['in_memory_source']}\").collect()[0]['count']\n",
    "output_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['target_table']}\").collect()[0]['count']\n",
    "\n",
    "# Find missing rows in target_table that are present in in_memory_source\n",
    "removed_rows_df = spark.sql(f\"\"\"\n",
    "SELECT s.*\n",
    "FROM {CONFIG['in_memory_source']} s\n",
    "LEFT ANTI JOIN {CONFIG['target_table']} t\n",
    "ON s.id = t.id\n",
    "\"\"\")\n",
    "\n",
    "removed_rows_count = removed_rows_df.count()\n",
    "\n",
    "comparison_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"input\", input_count),\n",
    "        (\"output\", output_count),\n",
    "        (\"removed_records\", removed_rows_count)\n",
    "    ],\n",
    "    [\"table\", \"record_count\"]\n",
    ")\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "if removed_rows_count > 0:\n",
    "    display(removed_rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae738600-086a-4313-a10b-5d33dc483885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cleanup \n",
    "- Post Execution removal of tables,views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04c3377-4e0d-4c75-a904-1d368289e2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "# pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# # Set the current catalog\n",
    "# spark.sql(f\"USE CATALOG {CONFIG['catalog']}\")\n",
    "\n",
    "# # Drop tables matching pattern\n",
    "# tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "# tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"] ]\n",
    "\n",
    "# if tables_to_drop:\n",
    "#     print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "#     for row in tables_to_drop:\n",
    "#         table_name = row[\"tableName\"]\n",
    "#         spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "#         print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "# else:\n",
    "#     print(\"----- No tables to drop\")\n",
    "\n",
    "# # Drop global and local temp views matching pattern\n",
    "\n",
    "# views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "# views_to_drop = views_df.collect()\n",
    "\n",
    "# if views_to_drop:\n",
    "#     print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "#     for row in views_to_drop:\n",
    "#         view_name = row[\"viewName\"]\n",
    "#         spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "#         print(f\"Dropped view: {view_name}\")\n",
    "# else:\n",
    "#     print(\"----- No views to drop\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "catalog",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "git_org",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "schema",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "library_source",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "git_branch_or_commit",
      "width": 187
     }
    ]
   },
   "notebookName": "spark_expectation_basic",
   "widgets": {
    "catalog": {
     "currentValue": "<placeholder>",
     "nuid": "f7aca915-629e-4219-b4ab-890c9869dac6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo_owner",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo_owner",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "git_branch_or_commit": {
     "currentValue": "main",
     "nuid": "c0584858-53ff-42ef-b1b2-db783c07eeb2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": "",
      "name": "git_branch_or_commit",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main",
      "label": "",
      "name": "git_branch_or_commit",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "git_org": {
     "currentValue": "Nike-Inc",
     "nuid": "d2e9fb9e-ac15-4b6e-a744-815f78777256",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Nike-Inc",
      "label": "git_org",
      "name": "git_org",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Nike-Inc",
      "label": "git_org",
      "name": "git_org",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "library_source": {
     "currentValue": "git",
     "nuid": "a1b79a9b-f51a-4e9c-a3ee-52b68634ec21",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "pypi",
      "label": "library_source",
      "name": "library_source",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "pypi",
        "git"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "pypi",
      "label": "library_source",
      "name": "library_source",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "pypi",
        "git"
       ]
      }
     }
    },
    "schema": {
     "currentValue": "<placeholder>",
     "nuid": "31737f2b-d1fb-4cae-9a3a-bd57f195017a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo_product",
      "label": "",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo_product",
      "label": "",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}