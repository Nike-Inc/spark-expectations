{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac24247",
   "metadata": {},
   "source": [
    "<div style=\"color:#fff; background:#0070c0; font-weight:bold; border:2px solid #0070c0; padding:10px; border-radius:6px;\">\n",
    "**Purpose:** This notebook demonstrates how to use <span style=\"color:yellow;\">Spark Expectations</span> with <span style=\"color:yellow;\"> Slack by sending notifications`</span>.<br>\n",
    "Its main focus is to show how data quality alerts and results can be sent to Slack channels.</span>\n",
    "</div>\n",
    "\n",
    "### Spark - Expectations - User - Guide - Documentation\n",
    "\n",
    "<div style=\"color:red; font-weight:bold; border:2px solid red; padding:8px;\">\n",
    "⚠️ ALERT: Notebook meant to be ran by spinning up local docker compose (containers/compose.yaml) !\n",
    "</div>\n",
    "\n",
    "* Please read through the [Spark Expectation Documentation](https://engineering.nike.com/spark-expectations) before proceeding with this demo\n",
    "\n",
    "#### widgets \n",
    "* `catalog`, `schema` - leave default values \n",
    "  * Tables are going to be prefixed with value provided in user widget text field\n",
    "\n",
    "<div style=\"color:orange; font-weight:regular; border:1px solid orange; padding:8px;\">\n",
    "⚠️ Container comes with SparkExpectation by default. If SE version is overriden Kernel will need to be restarted!\n",
    "</div>\n",
    "\n",
    "* `Override SE version` check box to install different SparkExpectation library version\n",
    "* `library_source` combo box defines library url(git branch or pypi) from where to pull library \n",
    "  * `pypi` ( installs latest published version available in PyPi)\n",
    "  * `git` ( installs library from specified git branch)\n",
    "    * Set `git_branch` input field to match git branch (example `main`)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e8e62",
   "metadata": {},
   "source": [
    "# Method 1: Use Webhook URL for Notebook Testing\n",
    "\n",
    "This is used by default for the example notebook. This will just use the webhook URL throughout the notebook for a quick and easy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b024da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local Spark environment, we'll use webhook URL directly\n",
    "# This section would be used in Databricks to create secret scopes\n",
    "\n",
    "# Note: In a real Databricks environment, you would use:\n",
    "# from databricks.sdk import WorkspaceClient\n",
    "# from pyspark.errors import PySparkException\n",
    "# w = WorkspaceClient()\n",
    "# w.secrets.list_scopes()\n",
    "\n",
    "# For local testing, we'll just use the webhook URL directly in the configuration\n",
    "print(\"Running in local Spark environment - using webhook URL directly\")\n",
    "webhook_url = \"https://hooks.slack.com/services/YOUR_WORKSPACE/YOUR_CHANNEL/YOUR_SECRET_TOKEN\"  # Replace with your actual webhook URL\n",
    "print(f\"Webhook URL configured: {webhook_url[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45040e9",
   "metadata": {},
   "source": [
    "# Widget Setup\n",
    "\n",
    "Widgets used in this notebook will be created and then set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE INPUT WIDGETS \n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "widget_user = widgets.Text(\n",
    "    value='user',\n",
    "    placeholder='Type something',\n",
    "    description='user: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_git_org = widgets.Text(\n",
    "    value='catalog_name',\n",
    "    placeholder='Type something',\n",
    "    description='git_org ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_catalog = widgets.Text(\n",
    "    value='development',\n",
    "    placeholder='Type something',\n",
    "    description='catalog:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_schema = widgets.Text(\n",
    "    value='team_name',\n",
    "    placeholder='Type something',\n",
    "    description='schema:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_library_source = widgets.Combobox(\n",
    "    placeholder='Choose source',\n",
    "    options=['pypi', 'git'],\n",
    "    description='library_source:',\n",
    "    ensure_option=True,\n",
    "    value='git',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_git_branch_or_commit = widgets.Text(\n",
    "    value='main',\n",
    "    placeholder='Type branch name or commit hash',\n",
    "    description='git_branch_or_commit:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "widget_override_version = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Override SE version',\n",
    "    disabled=False,\n",
    "    style={'description_width': '30px'}\n",
    "    \n",
    ")\n",
    "\n",
    "hbox = widgets.HBox([\n",
    "    widget_user,\n",
    "    widget_catalog, \n",
    "    widget_schema,\n",
    "    widget_override_version, \n",
    "    widget_library_source, \n",
    "    widget_git_org,\n",
    "    widget_git_branch_or_commit\n",
    "])\n",
    "\n",
    "# Display widgets\n",
    "display(hbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "user = re.sub(r'[^a-zA-Z]', '', widget_user.value).lower()\n",
    "catalog = widget_catalog.value\n",
    "schema = widget_schema.value\n",
    "override_se_version = widget_override_version.value\n",
    "library = widget_library_source.value\n",
    "org = widget_git_org.value\n",
    "branch_or_commit = widget_git_branch_or_commit.value\n",
    "\n",
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": \"development\",\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_product\",\n",
    "    \"in_memory_source\": f\"se_{user}_source\",\n",
    "    \"rules_table\": f\"development.{schema}.se_{user}_rules\",\n",
    "    \"stats_table\": f\"development.{schema}.se_{user}_stats\",\n",
    "    \"target_table\": f\"development.{schema}.se_{user}_target\",\n",
    "    \"override_se_version\" : override_se_version,\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff7e12",
   "metadata": {},
   "source": [
    "# Install Spark Expectation\n",
    "\n",
    "If Running from local container it will come with latest spark-expectation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override Spark Expectations based on user input\n",
    "if override_se_version:\n",
    "    print(\"-----OVERRIDING SPARK-EXPECTATIONS VERSION\")\n",
    "    if CONFIG[\"library\"] == \"pypi\":\n",
    "      print(\"-----INSTALLING SPARK-EXPECTATIONS from PyPi\")\n",
    "      %pip install spark-expectations\n",
    "    elif CONFIG[\"library\"] == \"git\":\n",
    "      print(f\"-----INSTALLING SPARK-EXPECTATIONS from Git Org/User {CONFIG['org']}, Branch/Commit {CONFIG['branch_or_commit']}\")\n",
    "      giturl = f\"git+https://github.com/{CONFIG['org']}/spark-expectations.git@{CONFIG['branch_or_commit']}\"\n",
    "      %pip install --force-reinstall {giturl}    \n",
    "else:\n",
    "    print(f\"---- Using SparkExpectation from local codebase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff84b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SPARK SESSION AND DATABASE\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Example\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show(truncate=False)\n",
    "\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6771bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE {CONFIG['catalog']}.{CONFIG['schema']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"] ]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"----- No tables to drop\")\n",
    "\n",
    "# Drop global and local temp views matching pattern\n",
    "\n",
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"----- No views to drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd98b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Started with Spark Expectations: Simple Example\n",
    "\n",
    "## 1. Sample Source Dataset\n",
    "# initialize simple Pandas DataFrame and convert it to a Spark DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "## 2. Define Simple `row_dq` Rules\n",
    "# Create a rules DataFrame with a few simple data quality rules\n",
    "\n",
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_not_null\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age IS NOT NULL\",\n",
    "        \"action_if_failed\": \"warn\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Age must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_adult\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age < 20\",\n",
    "        \"action_if_failed\": \"ignore\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Age must be less than 20\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "        {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"email_not_null\",\n",
    "        \"column_name\": \"email\",\n",
    "        \"expectation\": \"email IS NOT NULL\",\n",
    "        \"action_if_failed\": \"drop\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Email must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    }\n",
    "\n",
    "    \n",
    "]\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "rules_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "\n",
    "rules_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2cb182",
   "metadata": {},
   "source": [
    "# Notification Configurations for Slack\n",
    "\n",
    "This is where you can set the webhook URL with the following ways:\n",
    "- setting the webhook URL manually\n",
    "- using databricks secret storage (assuming you have secrets stored within your notebook)\n",
    "- using cerberus secrets storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccabfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure streaming and notification configuration\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "\n",
    "# This is a dictionary that can be used to configure Spark Expectations behavior and override default settings.\n",
    "stats_streaming_config_dict = {\n",
    "    user_config.se_enable_streaming: False,\n",
    "}\n",
    "\n",
    "# For local Spark environment, we don't need Databricks workspace info\n",
    "# dbx_workspace_id = get_context().workspaceId\n",
    "# dbx_workspace_url = get_context().browserHostName\n",
    "\n",
    "user_conf_dict = {\n",
    "    user_config.se_notifications_enable_slack: True,\n",
    "    # Slack Configuration - where you supply the webhook URL for your Slack channel\n",
    "    user_config.se_notifications_slack_webhook_url: \"your_webhook_url\",  # Replace with your actual webhook URL\n",
    "\n",
    "    # Fill in your cbs_url + cbs_sdb_path on where your secret is stored.\n",
    "    # user_config.secret_type: \"cerberus\",\n",
    "    # user_config.cbs_url: \"https://cerberus.com\",\n",
    "    # user_config.cbs_sdb_path: \"app/your/sdb/path\",\n",
    "\n",
    "    # Optionally configure additional Slack settings\n",
    "    # user_config.se_notifications_slack_channel: \"#data-quality-alerts\",\n",
    "    # user_config.se_notifications_slack_username: \"Spark Expectations Bot\",\n",
    "    # user_config.se_notifications_slack_icon_emoji: \":warning:\",\n",
    "\n",
    "    # Enable detailed results in notifications\n",
    "    user_config.se_enable_query_dq_detailed_result: True,\n",
    "    \n",
    "    # Set notification on query failure - corrected attribute name\n",
    "    user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n",
    "    user_config.se_notifications_on_error_drop_threshold: 15\n",
    "     # Notification triggers\n",
    "    user_config.se_notifications_on_start: True,\n",
    "    user_config.se_notifications_on_completion: True,\n",
    "    user_config.se_notifications_on_fail: True,\n",
    "    user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n",
    "    user_config.se_notifications_on_rules_action_if_failed_set_ignore: True,\n",
    "    user_config.se_notifications_on_error_drop_threshold: 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a95a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Run Spark Expectations\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from spark_expectations.core import load_configurations\n",
    "\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "\n",
    "\n",
    "writer = WrappedDataFrameWriter().mode(\"overwrite\").format(\"delta\")\n",
    "\n",
    "\n",
    "# Initialize Default Config \n",
    "load_configurations(spark) \n",
    "\n",
    "\"\"\"\n",
    "This class implements/supports running the data quality rules on a dataframe returned by a function\n",
    "\n",
    "Args:\n",
    "    product_id: Name of the product\n",
    "    rules_df: DataFrame which contains the rules. User is responsible for reading\n",
    "        the rules_table in which ever system it is\n",
    "    stats_table: Name of the table where the stats/audit-info need to be written\n",
    "    debugger: Mark it as \"True\" if the debugger mode need to be enabled, by default is False\n",
    "    stats_streaming_options: Provide options to override the defaults, while writing into the stats streaming table\n",
    "\"\"\"\n",
    "se = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "#  Initialize input data\n",
    "data = [\n",
    "    {\"id\": 1, \"age\": 19,   \"email\": \"alice@example.com\"},\n",
    "    {\"id\": 2, \"age\": 17,   \"email\": \"bob@example.com\"},\n",
    "    {\"id\": 3, \"age\": None, \"email\": \"charlie@example.com\"},\n",
    "    {\"id\": 4, \"age\": 40,   \"email\": \"mike@example.com\"},\n",
    "    {\"id\": 5, \"age\": None, \"email\": \"ron@example.com\"},\n",
    "    {\"id\": 6, \"age\": 35,   \"email\": None},\n",
    "]\n",
    "input_df = spark.createDataFrame(pd.DataFrame(data))\n",
    "input_df.show(truncate=False)\n",
    "\n",
    "\"\"\"\n",
    "This decorator helps to wrap a function which returns dataframe and apply dataframe rules on it\n",
    "\n",
    "Args:\n",
    "    target_table: Name of the table where the final dataframe need to be written\n",
    "    write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n",
    "    write_to_temp_table: Mark it as \"True\" if the input dataframe need to be written to the temp table to break\n",
    "                        the spark plan\n",
    "    user_conf: Provide options to override the defaults, while writing into the stats streaming table\n",
    "    target_table_view: This view is created after the _row_dq process to run the target agg_dq and query_dq.\n",
    "        If value is not provided, defaulted to {target_table}_view\n",
    "    target_and_error_table_writer: Provide the writer to write the target and error table,\n",
    "        this will take precedence over the class level writer\n",
    "\n",
    "Returns:\n",
    "    Any: Returns a function which applied the expectations on dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@se.with_expectations(\n",
    "    target_table=CONFIG[\"target_table\"],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=True,\n",
    "    user_conf=user_conf_dict,\n",
    ")\n",
    "def get_dataset():\n",
    "    _df_source: DataFrame = input_df\n",
    "    _df_source.createOrReplaceTempView(CONFIG[\"in_memory_source\"])\n",
    "    return _df_source\n",
    "\n",
    "\n",
    "# This will run the DQ checks and raise if any \"fail\" rules are violated\n",
    "get_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Project Env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
