{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#fff; background:#0070c0; font-weight:bold; border:2px solid #0070c0; padding:10px; border-radius:6px;\">\n",
    "üìß <span style=\"color:#fff;\">**Purpose:** This notebook demonstrates how to use <span style=\"color:yellow;\">Spark Expectations</span>.<br>\n",
    "Its main focus is to show how to pass ROW_DQ rule type and view different action type `drop,ignore,warn` during pipeline execution.</span>\n",
    "</div>\n",
    "\n",
    "### Spark - Expectations - User - Guide - Documentation\n",
    "\n",
    "<div style=\"color:red; font-weight:bold; border:2px solid red; padding:8px;\">\n",
    "‚ö†Ô∏è ALERT: Notebook meant to be ran by spinning up local docker compose (containers/compose.yaml) !\n",
    "</div>\n",
    "\n",
    "* Please read through the [Spark Expectation Documentation](https://engineering.nike.com/spark-expectations) before proceeding with this demo\n",
    "\n",
    "#### widgets \n",
    "* `catalog`, `schema` - leave default values \n",
    "  * Tables are going to be prefixed with value provided in user widget text field\n",
    "\n",
    "<div style=\"color:yellow; font-weight:regular; border:1px solid yellow; padding:8px;\">\n",
    "‚ö†Ô∏è Container comes with SparkExpectation by default. If SE version is overriden Kernel will need to be restarted!\n",
    "</div>\n",
    "\n",
    "* `Override SE version` check box to install different SparkExpectation library version\n",
    "* `library_source` combo box defines library url(git branch or pypi) from where to pull library \n",
    "  * `pypi` ( installs latest published version available in PyPi)\n",
    "  * `git` ( installs library from specified git branch)\n",
    "    * Set `git_branch` input field to match git branch (example `main`)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE INPUT WIDGETS \n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "widget_user = widgets.Text(\n",
    "    value='testuser',\n",
    "    placeholder='Type something',\n",
    "    description='user: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_git_org = widgets.Text(\n",
    "    value='Nike-Inc',\n",
    "    placeholder='Type something',\n",
    "    description='git_org ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_catalog = widgets.Text(\n",
    "    value='spark_catalog',\n",
    "    placeholder='Type something',\n",
    "    description='catalog:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_schema = widgets.Text(\n",
    "    value='default',\n",
    "    placeholder='Type something',\n",
    "    description='schema:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_library_source = widgets.Combobox(\n",
    "    placeholder='Choose source',\n",
    "    options=['pypi', 'git'],\n",
    "    description='library_source:',\n",
    "    ensure_option=True,\n",
    "    value='git',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_git_branch_or_commit = widgets.Text(\n",
    "    value='main',\n",
    "    placeholder='Type branch name or commit hash',\n",
    "    description='git_branch_or_commit:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "widget_override_version = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Override SE version',\n",
    "    disabled=False,\n",
    "    style={'description_width': '30px'}\n",
    "    \n",
    ")\n",
    "\n",
    "hbox = widgets.HBox([\n",
    "    widget_user,\n",
    "    widget_catalog, \n",
    "    widget_schema,\n",
    "    widget_override_version, \n",
    "    widget_library_source, \n",
    "    widget_git_org,\n",
    "    widget_git_branch_or_commit\n",
    "])\n",
    "\n",
    "# Display widgets\n",
    "display(hbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "user = re.sub(r'[^a-zA-Z]', '', widget_user.value).lower()\n",
    "catalog = widget_catalog.value\n",
    "schema = widget_schema.value\n",
    "override_se_version = widget_override_version.value\n",
    "library = widget_library_source.value\n",
    "org = widget_git_org.value\n",
    "branch_or_commit = widget_git_branch_or_commit.value\n",
    "\n",
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": catalog,\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_product\",\n",
    "    \"in_memory_source\": f\"se_{user}_source\",\n",
    "    \"rules_table\": f\"{catalog}.{schema}.se_{user}_rules\",\n",
    "    \"stats_table\": f\"{catalog}.{schema}.se_{user}_stats\",\n",
    "    \"target_table\": f\"{catalog}.{schema}.se_{user}_target\",\n",
    "    \"override_se_version\" : override_se_version,\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Spark Expectation\n",
    "\n",
    "If Running from local container it will come with latest spark-expectation library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override Spark Expectations based on user input\n",
    "if override_se_version:\n",
    "    print(\"-----OVERRIDING SPARK-EXPECTATIONS VERSION\")\n",
    "    if CONFIG[\"library\"] == \"pypi\":\n",
    "      print(\"-----INSTALLING SPARK-EXPECTATIONS from PyPi\")\n",
    "      %pip install spark-expectations\n",
    "    elif CONFIG[\"library\"] == \"git\":\n",
    "      print(f\"-----INSTALLING SPARK-EXPECTATIONS from Git Org/User {CONFIG['org']}, Branch/Commit {CONFIG['branch_or_commit']}\")\n",
    "      giturl = f\"git+https://github.com/{CONFIG['org']}/spark-expectations.git@{CONFIG['branch_or_commit']}\"\n",
    "      %pip install --force-reinstall {giturl}    \n",
    "else:\n",
    "    print(f\"---- Using SparkExpectation from local codebase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Spark Expectations installed version\n",
    "from importlib.metadata import version\n",
    "print(f\"---- Current SparkExpectation Version: {version('spark-expectations')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SPARK SESSION AND DATABASE\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Example\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show(truncate=False)\n",
    "\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE {CONFIG['catalog']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"] ]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"----- No tables to drop\")\n",
    "\n",
    "# Drop global and local temp views matching pattern\n",
    "\n",
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"----- No views to drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Started with Spark Expectations: Simple Example\n",
    "\n",
    "## 1. Sample Source Dataset\n",
    "# initialize simple Pandas DataFrame and convert it to a Spark DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "## 2. Define Simple `row_dq` Rules\n",
    "# Create a rules DataFrame with a few simple data quality rules\n",
    "\n",
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_not_null\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age IS NOT NULL\",\n",
    "        \"action_if_failed\": \"warn\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Age must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_adult\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age < 20\",\n",
    "        \"action_if_failed\": \"ignore\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Age must be less than 20\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "        {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"email_not_null\",\n",
    "        \"column_name\": \"email\",\n",
    "        \"expectation\": \"email IS NOT NULL\",\n",
    "        \"action_if_failed\": \"drop\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Email must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    }\n",
    "\n",
    "    \n",
    "]\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "rules_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "\n",
    "rules_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Run Spark Expectations\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from spark_expectations.core import load_configurations\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "\n",
    "\n",
    "writer = WrappedDataFrameWriter().mode(\"overwrite\").format(\"delta\")\n",
    "\n",
    "\n",
    "# Initialize Default Config \n",
    "load_configurations(spark) \n",
    "# Custom config (example enable slack/email notifications)\n",
    "stats_streaming_config_dict = {user_config.se_enable_streaming: False}\n",
    "notification_conf = {}\n",
    "\n",
    "\"\"\"\n",
    "This class implements/supports running the data quality rules on a dataframe returned by a function\n",
    "\n",
    "Args:\n",
    "    product_id: Name of the product\n",
    "    rules_df: DataFrame which contains the rules. User is responsible for reading\n",
    "        the rules_table in which ever system it is\n",
    "    stats_table: Name of the table where the stats/audit-info need to be written\n",
    "    debugger: Mark it as \"True\" if the debugger mode need to be enabled, by default is False\n",
    "    stats_streaming_options: Provide options to override the defaults, while writing into the stats streaming table\n",
    "\"\"\"\n",
    "se = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "#  Initialize input data\n",
    "data = [\n",
    "    {\"id\": 1, \"age\": 19,   \"email\": \"alice@example.com\"},\n",
    "    {\"id\": 2, \"age\": 17,   \"email\": \"bob@example.com\"},\n",
    "    {\"id\": 3, \"age\": None, \"email\": \"charlie@example.com\"},\n",
    "    {\"id\": 4, \"age\": 40,   \"email\": \"mike@example.com\"},\n",
    "    {\"id\": 5, \"age\": None, \"email\": \"ron@example.com\"},\n",
    "    {\"id\": 6, \"age\": 35,   \"email\": None},\n",
    "]\n",
    "input_df = spark.createDataFrame(pd.DataFrame(data))\n",
    "input_df.show(truncate=False)\n",
    "\n",
    "\"\"\"\n",
    "This decorator helps to wrap a function which returns dataframe and apply dataframe rules on it\n",
    "\n",
    "Args:\n",
    "    target_table: Name of the table where the final dataframe need to be written\n",
    "    write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n",
    "    write_to_temp_table: Mark it as \"True\" if the input dataframe need to be written to the temp table to break\n",
    "                        the spark plan\n",
    "    user_conf: Provide options to override the defaults, while writing into the stats streaming table\n",
    "    target_table_view: This view is created after the _row_dq process to run the target agg_dq and query_dq.\n",
    "        If value is not provided, defaulted to {target_table}_view\n",
    "    target_and_error_table_writer: Provide the writer to write the target and error table,\n",
    "        this will take precedence over the class level writer\n",
    "\n",
    "Returns:\n",
    "    Any: Returns a function which applied the expectations on dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@se.with_expectations(\n",
    "    target_table=CONFIG[\"target_table\"],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=True,\n",
    "    user_conf=notification_conf,\n",
    ")\n",
    "def get_dataset():\n",
    "    _df_source: DataFrame = input_df\n",
    "    _df_source.createOrReplaceTempView(CONFIG[\"in_memory_source\"])\n",
    "    return _df_source\n",
    "\n",
    "\n",
    "# This will run the DQ checks and raise if any \"fail\" rules are violated\n",
    "get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "# Results\n",
    "\n",
    "Checkout `Stats` and `Error` table to see spark-expectations execution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_stats_table = f\"SELECT * FROM {CONFIG['stats_table']}\"\n",
    "spark.sql(query_stats_table).show(truncate=False)\n",
    "\n",
    "query_error_table = f\"SELECT * FROM {CONFIG['target_table']}_error\"\n",
    "spark.sql(query_error_table).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "# Display Target Table\n",
    "Based on the provided rules if `action_if_failed` is set to `drop` target table will remove any rows not passing provided check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_target_table = f\"\"\"\n",
    "SELECT *\n",
    "FROM {CONFIG['target_table']} \n",
    "ORDER BY meta_dq_run_id, id\n",
    "\"\"\"\n",
    "\n",
    "final_data_set_df = spark.sql(query_target_table)\n",
    "\n",
    "if final_data_set_df is not None:\n",
    "    final_data_set_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "## Display Removed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['in_memory_source']}\").collect()[0]['count']\n",
    "output_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['target_table']}\").collect()[0]['count']\n",
    "\n",
    "# Find missing rows in target_table that are present in in_memory_source\n",
    "removed_rows_df = spark.sql(f\"\"\"\n",
    "SELECT s.*\n",
    "FROM {CONFIG['in_memory_source']} s\n",
    "LEFT ANTI JOIN {CONFIG['target_table']} t\n",
    "ON s.id = t.id\n",
    "\"\"\")\n",
    "\n",
    "removed_rows_count = removed_rows_df.count()\n",
    "\n",
    "comparison_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"input\", input_count),\n",
    "        (\"output\", output_count),\n",
    "        (\"removed_records\", removed_rows_count)\n",
    "    ],\n",
    "    [\"table\", \"record_count\"]\n",
    ")\n",
    "\n",
    "comparison_df.show()\n",
    "\n",
    "if removed_rows_count > 0:\n",
    "    removed_rows_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "catalog",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "git_org",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "schema",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "library_source",
      "width": 146
     },
     {
      "breakBefore": false,
      "name": "git_branch_or_commit",
      "width": 187
     }
    ]
   },
   "notebookName": "spark_expectation_basic",
   "widgets": {
    "catalog": {
     "currentValue": "<placeholder>",
     "nuid": "f7aca915-629e-4219-b4ab-890c9869dac6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo_owner",
      "label": "",
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "demo_owner",
      "label": "",
      "name": "catalog",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "git_branch_or_commit": {
     "currentValue": "main",
     "nuid": "c0584858-53ff-42ef-b1b2-db783c07eeb2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": "",
      "name": "git_branch_or_commit",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "main",
      "label": "",
      "name": "git_branch_or_commit",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "git_org": {
     "currentValue": "Nike-Inc",
     "nuid": "d2e9fb9e-ac15-4b6e-a744-815f78777256",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Nike-Inc",
      "label": "git_org",
      "name": "git_org",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "Nike-Inc",
      "label": "git_org",
      "name": "git_org",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "library_source": {
     "currentValue": "git",
     "nuid": "a1b79a9b-f51a-4e9c-a3ee-52b68634ec21",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "pypi",
      "label": "library_source",
      "name": "library_source",
      "options": {
       "choices": [
        "pypi",
        "git"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "pypi",
      "label": "library_source",
      "name": "library_source",
      "options": {
       "autoCreated": false,
       "choices": [
        "pypi",
        "git"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "schema": {
     "currentValue": "<placeholder>",
     "nuid": "31737f2b-d1fb-4cae-9a3a-bd57f195017a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo_product",
      "label": "",
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "demo_product",
      "label": "",
      "name": "schema",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
