{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d39da2-003e-48b3-b51d-7d12eb9b9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8329a6c-6091-43e9-9c40-5ebb465849e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_user = widgets.Text(\n",
    "    value='testuser',\n",
    "    placeholder='Type something',\n",
    "    description='user: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_git_org = widgets.Text(\n",
    "    value='Nike-Inc',\n",
    "    placeholder='Type something',\n",
    "    description='git_org ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_catalog = widgets.Text(\n",
    "    value='spark_catalog',\n",
    "    placeholder='Type something',\n",
    "    description='catalog:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_schema = widgets.Text(\n",
    "    value='default',\n",
    "    placeholder='Type something',\n",
    "    description='schema:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_library_source = widgets.Combobox(\n",
    "    placeholder='Choose source',\n",
    "    options=['pypi', 'git'],\n",
    "    description='library_source:',\n",
    "    ensure_option=True,\n",
    "    value='git',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_git_branch_or_commit = widgets.Text(\n",
    "    value='main',\n",
    "    placeholder='Type branch name or commit hash',\n",
    "    description='git_branch_or_commit:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "widget_override_version = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Override SE version',\n",
    "    disabled=False,\n",
    "    style={'description_width': '30px'}\n",
    "    \n",
    ")\n",
    "\n",
    "hbox = widgets.HBox([\n",
    "    widget_user,\n",
    "    widget_catalog, \n",
    "    widget_schema,\n",
    "    widget_override_version, \n",
    "    widget_library_source, \n",
    "    widget_git_org,\n",
    "    widget_git_branch_or_commit\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae275a4-0765-4491-98b4-79bf6fcf3e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee12d581d94d47a4bed5a4efdc5d2794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='testuser', description='user: ', placeholder='Type something', style=TextStyle(descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display widgets\n",
    "display(hbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602c00f2-b403-481d-a343-fb2e7af13ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testuser\n",
      "spark_catalog\n",
      "default\n",
      "False\n",
      "git\n",
      "Nike-Inc\n",
      "main\n"
     ]
    }
   ],
   "source": [
    "user = re.sub(r'[^a-zA-Z]', '', widget_user.value).lower()\n",
    "catalog = widget_catalog.value\n",
    "schema = widget_schema.value\n",
    "override_se_version = widget_override_version.value\n",
    "library = widget_library_source.value\n",
    "org = widget_git_org.value\n",
    "branch_or_commit = widget_git_branch_or_commit.value\n",
    "\n",
    "print(user)\n",
    "print(catalog)\n",
    "print(schema)\n",
    "print(override_se_version)\n",
    "print(library)\n",
    "print(org)\n",
    "print(branch_or_commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c588b5e-28ae-4b09-862a-3598a7dfaf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owner</td>\n",
       "      <td>testuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>catalog</td>\n",
       "      <td>spark_catalog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schema</td>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user</td>\n",
       "      <td>testuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product_id</td>\n",
       "      <td>se_testuser_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in_memory_source</td>\n",
       "      <td>se_testuser_source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rules_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stats_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_stats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>target_table</td>\n",
       "      <td>spark_catalog.default.se_testuser_target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>override_se_version</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>library</td>\n",
       "      <td>git</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>org</td>\n",
       "      <td>Nike-Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>branch_or_commit</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Key                                     Value\n",
       "0                 owner                                  testuser\n",
       "1               catalog                             spark_catalog\n",
       "2                schema                                   default\n",
       "3                  user                                  testuser\n",
       "4            product_id                       se_testuser_product\n",
       "5      in_memory_source                        se_testuser_source\n",
       "6           rules_table   spark_catalog.default.se_testuser_rules\n",
       "7           stats_table   spark_catalog.default.se_testuser_stats\n",
       "8          target_table  spark_catalog.default.se_testuser_target\n",
       "9   override_se_version                                     False\n",
       "10              library                                       git\n",
       "11                  org                                  Nike-Inc\n",
       "12     branch_or_commit                                      main"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": catalog,\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_product\",\n",
    "    \"in_memory_source\": f\"se_{user}_source\",\n",
    "    \"rules_table\": f\"{catalog}.{schema}.se_{user}_rules\",\n",
    "    \"stats_table\": f\"{catalog}.{schema}.se_{user}_stats\",\n",
    "    \"target_table\": f\"{catalog}.{schema}.se_{user}_target\",\n",
    "    \"override_se_version\" : override_se_version,\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e7b6fd-8763-4a60-9551-1203f172a948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Current SparkExpectation Version: 2.5.1.dev9+gafe0b380c.d20250909\n"
     ]
    }
   ],
   "source": [
    "# Display Spark Expectations installed version\n",
    "from importlib.metadata import version\n",
    "print(f\"---- Current SparkExpectation Version: {version('spark-expectations')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c787f8-6383-4286-a47f-559a64bdcafa",
   "metadata": {},
   "source": [
    "### Setting up spark expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed934e7a-3fb7-480c-a3ab-19177bdafd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/spark/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-04b74fe2-74aa-4ca3-96c6-9a17db094df6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 63ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-04b74fe2-74aa-4ca3-96c6-9a17db094df6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/1ms)\n",
      "25/09/10 01:04:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# CREATE SPARK SESSION AND DATABASE\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Aggregation Rules\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "691ea44f-590d-4d0e-b985-48bfbe9e0334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show(truncate=False)\n",
    "\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed9a071-1101-49e8-9835-5155f8929ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- No tables to drop\n"
     ]
    }
   ],
   "source": [
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE {CONFIG['catalog']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"] ]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"----- No tables to drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce4f3f9-fda9-40de-ac98-7d8cb9911c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- No views to drop\n"
     ]
    }
   ],
   "source": [
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"----- No views to drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1f3e7-1e74-499b-82c8-122cc474996c",
   "metadata": {},
   "source": [
    "### Spark expectation execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb5993f-e517-48d6-b095-9b8fa2a41068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78199b83-d418-4ef4-8cce-ca468a7f2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"agg_dq\",\n",
    "        \"rule\": \"data_existing\",\n",
    "        \"column_name\": \"sales\",\n",
    "        \"expectation\": \"count(*) > 0\",\n",
    "        \"action_if_failed\": \"fail\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Data should be present\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"agg_dq\",\n",
    "        \"rule\": \"min_sales\",\n",
    "        \"column_name\": \"sales\",\n",
    "        \"expectation\": \"min(sales)>1000\",\n",
    "        \"action_if_failed\": \"warn\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Minimum sales should be greater than 1000\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"agg_dq\",\n",
    "        \"rule\": \"no_duplicates\",\n",
    "        \"column_name\": \"id\",\n",
    "        \"expectation\": \"count(distinct id) == count(*)\",\n",
    "        \"action_if_failed\": \"fail\",\n",
    "        \"tag\": \"uniqueness\",\n",
    "        \"description\": \"Each data point should have a unique id\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4f28932-7aa2-4d81-b1c2-e10d5a11913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f87db002-0b42-446b-a37d-ff689086ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a303eed-563f-4829-abe0-19441f387752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>table_name</th>\n",
       "      <th>rule_type</th>\n",
       "      <th>rule</th>\n",
       "      <th>column_name</th>\n",
       "      <th>expectation</th>\n",
       "      <th>action_if_failed</th>\n",
       "      <th>tag</th>\n",
       "      <th>description</th>\n",
       "      <th>enable_for_source_dq_validation</th>\n",
       "      <th>enable_for_target_dq_validation</th>\n",
       "      <th>is_active</th>\n",
       "      <th>enable_error_drop_alert</th>\n",
       "      <th>error_drop_threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>se_testuser_product</td>\n",
       "      <td>spark_catalog.default.se_testuser_target</td>\n",
       "      <td>agg_dq</td>\n",
       "      <td>data_existing</td>\n",
       "      <td>sales</td>\n",
       "      <td>count(*) &gt; 0</td>\n",
       "      <td>fail</td>\n",
       "      <td>completeness</td>\n",
       "      <td>Data should be present</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>se_testuser_product</td>\n",
       "      <td>spark_catalog.default.se_testuser_target</td>\n",
       "      <td>agg_dq</td>\n",
       "      <td>min_sales</td>\n",
       "      <td>sales</td>\n",
       "      <td>min(sales)&gt;1000</td>\n",
       "      <td>warn</td>\n",
       "      <td>validity</td>\n",
       "      <td>Minimum sales should be greater than 1000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>se_testuser_product</td>\n",
       "      <td>spark_catalog.default.se_testuser_target</td>\n",
       "      <td>agg_dq</td>\n",
       "      <td>no_duplicates</td>\n",
       "      <td>id</td>\n",
       "      <td>count(distinct id) == count(*)</td>\n",
       "      <td>fail</td>\n",
       "      <td>uniqueness</td>\n",
       "      <td>Each data point should have a unique id</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            product_id                                table_name rule_type  \\\n",
       "0  se_testuser_product  spark_catalog.default.se_testuser_target    agg_dq   \n",
       "1  se_testuser_product  spark_catalog.default.se_testuser_target    agg_dq   \n",
       "2  se_testuser_product  spark_catalog.default.se_testuser_target    agg_dq   \n",
       "\n",
       "            rule column_name                     expectation action_if_failed  \\\n",
       "0  data_existing       sales                    count(*) > 0             fail   \n",
       "1      min_sales       sales                 min(sales)>1000             warn   \n",
       "2  no_duplicates          id  count(distinct id) == count(*)             fail   \n",
       "\n",
       "            tag                                description  \\\n",
       "0  completeness                     Data should be present   \n",
       "1      validity  Minimum sales should be greater than 1000   \n",
       "2    uniqueness    Each data point should have a unique id   \n",
       "\n",
       "   enable_for_source_dq_validation  enable_for_target_dq_validation  \\\n",
       "0                             True                             True   \n",
       "1                             True                             True   \n",
       "2                             True                             True   \n",
       "\n",
       "   is_active  enable_error_drop_alert  error_drop_threshold  \n",
       "0       True                    False                     0  \n",
       "1       True                    False                     0  \n",
       "2       True                    False                     0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9da04c-7812-47f3-b532-6f03255f5b82",
   "metadata": {},
   "source": [
    "### Running spark expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1220bbf-18be-4302-b732-8145cede9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from spark_expectations.core import load_configurations\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61620ee2-eca8-494e-862e-194f04b81182",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = WrappedDataFrameWriter().mode(\"overwrite\").format(\"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "750afb2b-d7df-4ee8-8130-cf284fa24882",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_configurations(spark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1968db61-8535-4d27-8afc-76d7addd2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom config (example enable slack/email notifications)\n",
    "stats_streaming_config_dict = {user_config.se_enable_streaming: False}\n",
    "notification_conf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06042484-e615-4ea9-9bb4-f111ffff20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e60a9bdf-a11f-4c8b-ad36-da7581340e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|id |name   |sales |\n",
      "+---+-------+------+\n",
      "|1  |Alice  |3500.0|\n",
      "|2  |Bob    |2800.0|\n",
      "|3  |Charlie|4200.0|\n",
      "|4  |Mike   |2300.0|\n",
      "|5  |Ron    |NaN   |\n",
      "|6  |Zach   |3900.0|\n",
      "|7  |Alex   |4100.0|\n",
      "|8  |Steve  |4200.0|\n",
      "|9  |James  |900.0 |\n",
      "|10 |Dan    |2500.0|\n",
      "|4  |Bryan  |1600.0|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Initialize input data\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\",  \"sales\": 3500},\n",
    "    {\"id\": 2, \"name\": \"Bob\",   \"sales\": 2800},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"sales\": 4200},\n",
    "    {\"id\": 4, \"name\": \"Mike\",   \"sales\": 2300},\n",
    "    {\"id\": 5, \"name\": \"Ron\", \"sales\": None},\n",
    "    {\"id\": 6, \"name\": \"Zach\",   \"sales\": 3900},\n",
    "    {\"id\": 7, \"name\": \"Alex\",   \"sales\": 4100},\n",
    "    {\"id\": 8, \"name\": \"Steve\",   \"sales\": 4200},\n",
    "    {\"id\": 9, \"name\": \"James\",   \"sales\": 900},\n",
    "    {\"id\": 10, \"name\": \"Dan\",   \"sales\": 2500},\n",
    "    {\"id\": 4, \"name\": \"Bryan\",   \"sales\": 1600}\n",
    "]\n",
    "input_df = spark.createDataFrame(pd.DataFrame(data))\n",
    "input_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ea47fa0-c409-4971-99bd-32d677dc6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "@se.with_expectations(\n",
    "    target_table=CONFIG[\"target_table\"],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=True,\n",
    "    user_conf=notification_conf,\n",
    ")\n",
    "def get_dataset():\n",
    "    _df_source: DataFrame = input_df\n",
    "    _df_source.createOrReplaceTempView(CONFIG[\"in_memory_source\"])\n",
    "    return _df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e5a769d-d8dd-442f-a28e-b389900dc7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 02:06:26,930] [INFO] [spark_expectations] {expectations.py:wrapper:325} - The function dataframe is getting created\n",
      "[2025-09-10 02:06:27,139] [INFO] [spark_expectations] {expectations.py:wrapper:341} - Validation for rules completed successfully\n",
      "[2025-09-10 02:06:27,202] [INFO] [spark_expectations] {expectations.py:wrapper:344} - data frame input record count: 11\n",
      "[2025-09-10 02:06:27,203] [INFO] [spark_expectations] {expectations.py:wrapper:356} - initialize variable with default values before next run\n",
      "[2025-09-10 02:06:27,203] [INFO] [spark_expectations] {expectations.py:wrapper:390} - Spark Expectations run id for this run: se_testuser_product_bdb44ee8-8dea-11f0-b465-7a2b7591a4dc\n",
      "[2025-09-10 02:06:27,203] [INFO] [spark_expectations] {expectations.py:wrapper:393} - The function dataframe is created\n",
      "[2025-09-10 02:06:27,203] [INFO] [spark_expectations] {expectations.py:wrapper:396} - Dropping to temp table started\n",
      "[2025-09-10 02:06:27,207] [INFO] [spark_expectations] {expectations.py:wrapper:398} - Dropping to temp table completed\n",
      "[2025-09-10 02:06:27,207] [INFO] [spark_expectations] {expectations.py:wrapper:399} - Writing to temp table started\n",
      "[2025-09-10 02:06:27,209] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-09-10 02:06:27,209] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: spark_catalog.default.se_testuser_target_temp\n",
      "[2025-09-10 02:06:27,788] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: spark_catalog.default.se_testuser_target_temp\n",
      "[2025-09-10 02:06:27,803] [INFO] [spark_expectations] {writer.py:save_df_as_table:99} - product_id is not set for table spark_catalog.default.se_testuser_target_temp in tableproperties, setting it now\n",
      "[2025-09-10 02:06:28,223] [INFO] [spark_expectations] {expectations.py:wrapper:406} - Read from temp table started\n",
      "[2025-09-10 02:06:28,227] [INFO] [spark_expectations] {expectations.py:wrapper:409} - Read from temp table completed\n",
      "[2025-09-10 02:06:28,227] [INFO] [spark_expectations] {expectations.py:wrapper:422} - started processing data quality rules for agg level expectations on soure dataframe\n",
      "[2025-09-10 02:06:28,228] [INFO] [spark_expectations] {regulate_flow.py:func_process:92} - The data quality dataframe is getting created for expectations\n",
      "[2025-09-10 02:06:28,254] [INFO] [spark_expectations] {regulate_flow.py:func_process:103} - The data quality dataframe is created for expectations\n",
      "[2025-09-10 02:06:28,639] [INFO] [spark_expectations] {writer.py:write_error_stats:732} - Writing metrics to the stats table: {self._context.get_dq_stats_table_name}, started\n",
      "[2025-09-10 02:06:28,639] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-09-10 02:06:28,641] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: spark_catalog.default.se_testuser_stats\n",
      "[2025-09-10 02:06:29,534] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: spark_catalog.default.se_testuser_stats\n",
      "[2025-09-10 02:06:29,534] [INFO] [spark_expectations] {writer.py:write_error_stats:743} - Writing metrics to the stats table: {self._context.get_dq_stats_table_name}, ended\n",
      "[2025-09-10 02:06:29,534] [INFO] [spark_expectations] {writer.py:write_error_stats:769} - Streaming stats to kafka is disabled, hence skipping writing to kafka\n"
     ]
    },
    {
     "ename": "SparkExpectationsMiscException",
     "evalue": "error occurred while processing spark expectations error occurred while executing func_process error occurred while taking action on given rules Job failed, as there is a data quality issue at agg_dq expectations and the action_if_failed suggested to fail",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkExpectOrFailException\u001b[0m                Traceback (most recent call last)",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/utils/actions.py:656\u001b[0m, in \u001b[0;36mSparkExpectationsActions.action_on_rules\u001b[0;34m(_context, _df_dq, _input_count, _error_count, _output_count, _rule_type, _row_dq_flag, _source_agg_dq_flag, _final_agg_dq_flag, _source_query_dq_flag, _final_query_dq_flag)\u001b[0m\n\u001b[1;32m    654\u001b[0m         _context\u001b[38;5;241m.\u001b[39mset_final_query_dq_status(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkExpectOrFailException(\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed, as there is a data quality issue at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_rule_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpectations and the action_if_failed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggested to fail\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _df_dq\u001b[38;5;241m.\u001b[39mselect(_df_dq_columns[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mSparkExpectOrFailException\u001b[0m: Job failed, as there is a data quality issue at agg_dq expectations and the action_if_failed suggested to fail",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m            Traceback (most recent call last)",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/utils/regulate_flow.py:139\u001b[0m, in \u001b[0;36mSparkExpectationsRegulateFlow.execute_dq_process.<locals>.func_process\u001b[0;34m(df, _rule_type, row_dq_flag, source_agg_dq_flag, final_agg_dq_flag, source_query_dq_flag, final_query_dq_flag, error_count, output_count)\u001b[0m\n\u001b[1;32m    137\u001b[0m     _context\u001b[38;5;241m.\u001b[39mset_final_query_dq_result(agg_dq_res)\n\u001b[0;32m--> 139\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43m_actions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_on_rules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_error_df\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_dq_flag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_df_dq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_input_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_error_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_error_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_output_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_running_rule_type_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_row_dq_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_dq_flag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_source_agg_dq_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_agg_dq_flag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_final_agg_dq_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_agg_dq_flag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_source_query_dq_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_query_dq_flag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_final_query_dq_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_query_dq_flag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m _context\u001b[38;5;241m.\u001b[39mprint_dataframe_with_debugger(df)\n",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/utils/actions.py:664\u001b[0m, in \u001b[0;36mSparkExpectationsActions.action_on_rules\u001b[0;34m(_context, _df_dq, _input_count, _error_count, _output_count, _rule_type, _row_dq_flag, _source_agg_dq_flag, _final_agg_dq_flag, _source_query_dq_flag, _final_query_dq_flag)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkExpectationsMiscException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror occurred while taking action on given rules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m: error occurred while taking action on given rules Job failed, as there is a data quality issue at agg_dq expectations and the action_if_failed suggested to fail",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m            Traceback (most recent call last)",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/core/expectations.py:439\u001b[0m, in \u001b[0;36mSparkExpectations.with_expectations.<locals>._except.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# In this steps source agg data quality expectations runs on raw_data\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# returns:\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m#        _source_dq_df: applied data quality dataframe,\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m#        _dq_source_agg_results: source aggregation result in dictionary\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m#        _: place holder for error data at row level\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m#        status: status of the execution\u001b[39;00m\n\u001b[1;32m    434\u001b[0m (\n\u001b[1;32m    435\u001b[0m     _source_dq_df,\n\u001b[1;32m    436\u001b[0m     _dq_source_agg_results,\n\u001b[1;32m    437\u001b[0m     _,\n\u001b[1;32m    438\u001b[0m     status,\n\u001b[0;32m--> 439\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_process\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_agg_dq_rule_type_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_agg_dq_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mset_source_agg_dq_status(status)\n",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/utils/regulate_flow.py:157\u001b[0m, in \u001b[0;36mSparkExpectationsRegulateFlow.execute_dq_process.<locals>.func_process\u001b[0;34m(df, _rule_type, row_dq_flag, source_agg_dq_flag, final_agg_dq_flag, source_query_dq_flag, final_query_dq_flag, error_count, output_count)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkExpectationsMiscException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror occurred while executing func_process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m: error occurred while executing func_process error occurred while taking action on given rules Job failed, as there is a data quality issue at agg_dq expectations and the action_if_failed suggested to fail",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m            Traceback (most recent call last)",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/sinks/utils/collect_statistics.py:33\u001b[0m, in \u001b[0;36mSparkExpectationsCollectStatistics.collect_stats_on_success_failure.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mset_dq_start_time()\n\u001b[0;32m---> 33\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mset_dq_run_status(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/core/expectations.py:640\u001b[0m, in \u001b[0;36mSparkExpectations.with_expectations.<locals>._except.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkExpectationsMiscException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror occurred while processing spark expectations \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m: error occurred while processing spark expectations error occurred while executing func_process error occurred while taking action on given rules Job failed, as there is a data quality issue at agg_dq expectations and the action_if_failed suggested to fail",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m            Traceback (most recent call last)",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/notifications/push/spark_expectations_notify.py:57\u001b[0m, in \u001b[0;36mSparkExpectationsNotify.notify_on_start_completion_failure.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# self._context.set_dq_start_time()\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mget_notification_on_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/sinks/utils/collect_statistics.py:45\u001b[0m, in \u001b[0;36mSparkExpectationsCollectStatistics.collect_stats_on_success_failure.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mwrite_error_stats()\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkExpectationsMiscException(e)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m: error occurred while processing spark expectations error occurred while executing func_process error occurred while taking action on given rules Job failed, as there is a data quality issue at agg_dq expectations and the action_if_failed suggested to fail",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/spark_expectations/spark_expectations/notifications/push/spark_expectations_notify.py:68\u001b[0m, in \u001b[0;36mSparkExpectationsNotify.notify_on_start_completion_failure.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         _on_failure(e)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# self._context.set_dq_end_time()\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkExpectationsMiscException(e)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mSparkExpectationsMiscException\u001b[0m: error occurred while processing spark expectations error occurred while executing func_process error occurred while taking action on given rules Job failed, as there is a data quality issue at agg_dq expectations and the action_if_failed suggested to fail"
     ]
    }
   ],
   "source": [
    "get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e81ce77-679c-4323-9b81-a830d0cfd89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 02:06:34 ERROR NonFateSharingFuture: Failed to get result from future\n",
      "scala.runtime.NonLocalReturnControl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>table_name</th>\n",
       "      <th>input_count</th>\n",
       "      <th>error_count</th>\n",
       "      <th>output_count</th>\n",
       "      <th>output_percentage</th>\n",
       "      <th>success_percentage</th>\n",
       "      <th>error_percentage</th>\n",
       "      <th>source_agg_dq_results</th>\n",
       "      <th>final_agg_dq_results</th>\n",
       "      <th>...</th>\n",
       "      <th>final_query_dq_results</th>\n",
       "      <th>row_dq_res_summary</th>\n",
       "      <th>row_dq_error_threshold</th>\n",
       "      <th>dq_status</th>\n",
       "      <th>dq_run_time</th>\n",
       "      <th>dq_rules</th>\n",
       "      <th>meta_dq_run_id</th>\n",
       "      <th>meta_dq_run_date</th>\n",
       "      <th>meta_dq_run_datetime</th>\n",
       "      <th>dq_env</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>se_testuser_product</td>\n",
       "      <td>spark_catalog.default.se_testuser_target</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'rule_type': 'agg_dq', 'column_name': 'sales...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'final_query_dq': 'Skipped', 'source_agg_dq':...</td>\n",
       "      <td>{'row_dq_run_time': 0.0, 'source_query_dq_run_...</td>\n",
       "      <td>{'query_dq_rules': {'num_final_query_dq_rules'...</td>\n",
       "      <td>se_testuser_product_bdb44ee8-8dea-11f0-b465-7a...</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>2025-09-10 02:06:19</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            product_id                                table_name  input_count  \\\n",
       "0  se_testuser_product  spark_catalog.default.se_testuser_target           11   \n",
       "\n",
       "   error_count  output_count  output_percentage  success_percentage  \\\n",
       "0            0             0                0.0               100.0   \n",
       "\n",
       "   error_percentage                              source_agg_dq_results  \\\n",
       "0               0.0  [{'rule_type': 'agg_dq', 'column_name': 'sales...   \n",
       "\n",
       "  final_agg_dq_results  ... final_query_dq_results row_dq_res_summary  \\\n",
       "0                 None  ...                   None               None   \n",
       "\n",
       "  row_dq_error_threshold                                          dq_status  \\\n",
       "0                   None  {'final_query_dq': 'Skipped', 'source_agg_dq':...   \n",
       "\n",
       "                                         dq_run_time  \\\n",
       "0  {'row_dq_run_time': 0.0, 'source_query_dq_run_...   \n",
       "\n",
       "                                            dq_rules  \\\n",
       "0  {'query_dq_rules': {'num_final_query_dq_rules'...   \n",
       "\n",
       "                                      meta_dq_run_id meta_dq_run_date  \\\n",
       "0  se_testuser_product_bdb44ee8-8dea-11f0-b465-7a...       2025-09-10   \n",
       "\n",
       "  meta_dq_run_datetime dq_env  \n",
       "0  2025-09-10 02:06:19         \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_stats_table = f\"SELECT * FROM {CONFIG['stats_table']}\"\n",
    "query_stats_table_df = spark.sql(query_stats_table).toPandas()\n",
    "query_stats_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dd0c2046-67e4-4ce9-9bc8-32dfd3a4b2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rule_type': 'agg_dq',\n",
       "  'column_name': 'sales',\n",
       "  'description': 'Data should be present',\n",
       "  'rule': 'data_existing',\n",
       "  'tag': 'completeness',\n",
       "  'action_if_failed': 'fail',\n",
       "  'status': 'pass'},\n",
       " {'rule_type': 'agg_dq',\n",
       "  'column_name': 'sales',\n",
       "  'description': 'Minimum sales should be greater than 1000',\n",
       "  'rule': 'min_sales',\n",
       "  'tag': 'validity',\n",
       "  'action_if_failed': 'warn',\n",
       "  'status': 'fail'},\n",
       " {'rule_type': 'agg_dq',\n",
       "  'column_name': 'id',\n",
       "  'description': 'Each data point should have a unique id',\n",
       "  'rule': 'no_duplicates',\n",
       "  'tag': 'uniqueness',\n",
       "  'action_if_failed': 'fail',\n",
       "  'status': 'fail'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Failure of rules captured here\n",
    "query_stats_table_df.loc[0,'source_agg_dq_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21531b51-efd0-4a35-a800-5792b477ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n",
      "+---------+-----------------------+-----------+\n",
      "|namespace|tableName              |isTemporary|\n",
      "+---------+-----------------------+-----------+\n",
      "|default  |se_testuser_rules      |false      |\n",
      "|default  |se_testuser_stats      |false      |\n",
      "|default  |se_testuser_target     |false      |\n",
      "|default  |se_testuser_target_temp|false      |\n",
      "|         |se_testuser_source     |true       |\n",
      "+---------+-----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show(truncate=False)\n",
    "\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc037873-c946-480a-9db2-38a3d1bdba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------------------------------------------------------+--------------------+\n",
      "|id |name   |sales |meta_dq_run_id                                          |meta_dq_run_datetime|\n",
      "+---+-------+------+--------------------------------------------------------+--------------------+\n",
      "|1  |Alice  |3500.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|2  |Bob    |2800.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|3  |Charlie|4200.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|4  |Bryan  |1600.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|4  |Mike   |2300.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|5  |Ron    |NaN   |se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|6  |Zach   |3900.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|7  |Alex   |4100.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|8  |Steve  |4200.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|9  |James  |900.0 |se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "|10 |Dan    |2500.0|se_testuser_product_ab27526c-8de9-11f0-b465-7a2b7591a4dc|2025-09-10 01:58:38 |\n",
      "+---+-------+------+--------------------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_target_table = f\"\"\"\n",
    "SELECT *\n",
    "FROM {CONFIG['target_table']} \n",
    "ORDER BY meta_dq_run_id, id\n",
    "\"\"\"\n",
    "\n",
    "final_data_set_df = spark.sql(query_target_table)\n",
    "\n",
    "if final_data_set_df is not None:\n",
    "    final_data_set_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b77e6111-d1d8-48a2-be09-ce5d1c7ebfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|          table|record_count|\n",
      "+---------------+------------+\n",
      "|          input|          11|\n",
      "|         output|          11|\n",
      "|removed_records|           0|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['in_memory_source']}\").collect()[0]['count']\n",
    "output_count = spark.sql(f\"SELECT COUNT(*) AS count FROM {CONFIG['target_table']}\").collect()[0]['count']\n",
    "\n",
    "# Find missing rows in target_table that are present in in_memory_source\n",
    "removed_rows_df = spark.sql(f\"\"\"\n",
    "SELECT s.*\n",
    "FROM {CONFIG['in_memory_source']} s\n",
    "LEFT ANTI JOIN {CONFIG['target_table']} t\n",
    "ON s.id = t.id\n",
    "\"\"\")\n",
    "\n",
    "removed_rows_count = removed_rows_df.count()\n",
    "\n",
    "comparison_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"input\", input_count),\n",
    "        (\"output\", output_count),\n",
    "        (\"removed_records\", removed_rows_count)\n",
    "    ],\n",
    "    [\"table\", \"record_count\"]\n",
    ")\n",
    "\n",
    "comparison_df.show()\n",
    "\n",
    "if removed_rows_count > 0:\n",
    "    removed_rows_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913236e-103d-433d-87f5-8590111dc5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
